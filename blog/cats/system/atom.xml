<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: system | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/system/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2016-03-29T17:17:34+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linux系统启动过程分析]]></title>
    <link href="http://abcdxyzk.github.io/blog/2016/01/21/system-base-init/"/>
    <updated>2016-01-21T16:13:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2016/01/21/system-base-init</id>
    <content type="html"><![CDATA[<p><a href="http://blog.chinaunix.net/uid-23069658-id-3142047.html">http://blog.chinaunix.net/uid-23069658-id-3142047.html</a></p>

<h4>BIOS自检</h4>

<p>  稍有计算机基础的人都应该听过BIOS(Basic Input / Output System)，又称基本输入输出系统，可以视为是一个永久地记录在ROM中的一个软件，是操作系统输入输出管理系统的一部分。早期的BIOS芯片确实是"只读"的，里面的内容是用一种烧录器写入的，一旦写入就不能更改，除非更换芯片。现在的主机板都使用一种叫Flash EPROM的芯片来存储系统BIOS，里面的内容可通过使用主板厂商提供的擦写程序擦除后重新写入，这样就给用户升级BIOS提供了极大的方便。</p>

<p>  BIOS的功能由两部分组成，分别是POST码和Runtime服务。POST阶段完成后它将从存储器中被清除，而Runtime服务会被一直保留，用于目标操作系统的启动。BIOS两个阶段所做的详细工作如下：</p>

<p>  步骤1：上电自检POST(Power-on self test)，主要负责检测系统外围关键设备（如：CPU、内存、显卡、I/O、键盘鼠标等）是否正常。例如，最常见的是内存松动的情况，BIOS自检阶段会报错，系统就无法启动起来；</p>

<p>  步骤2：步骤1成功后，便会执行一段小程序用来枚举本地设备并对其初始化。这一步主要是根据我们在BIOS中设置的系统启动顺序来搜索用于启动系统的驱动器，如硬盘、光盘、U盘、软盘和网络等。我们以硬盘启动为例，BIOS此时去读取硬盘驱动器的第一个扇区(MBR，512字节)，然后执行里面的代码。实际上这里BIOS并不关心启动设备第一个扇区中是什么内容，它只是负责读取该扇区内容、并执行。</p>

<p>至此，BIOS的任务就完成了，此后将系统启动的控制权移交到MBR部分的代码。</p>

<p>PS: 在个人电脑中，Linux的启动是从0xFFFF0地址开始的。</p>

<h4>系统引导</h4>

<p>  我们首先来了解一下MBR，它是Master Boot Record的缩写。硬盘的0柱面、0磁头、1扇区称为主引导扇区。它由三个部分组成，主引导程序(Bootloader)、 硬盘分区表DPT（Disk Partition table）和硬盘有效标志（55AA），其结构图如下所示：</p>

<p><img src="/images/system/2016-01-21-1.png" alt="" /></p>

<p> 磁盘分区表包含以下三部分：</p>

<p>1）、Partition ID  （5：延申  82：Swap   83：Linux   8e：LVM     fd：RAID）</p>

<p>2）、Partition起始磁柱</p>

<p>3）、Partition的磁柱数量</p>

<p>  通常情况下，诸如lilo、grub这些常见的引导程序都直接安装在MBR中。我们以grub为例来分析这个引导过程。</p>

<h5>grub引导也分为两个阶段stage1阶段和stage2阶段(有些较新的grub又定义了stage1.5阶段)。</h5>

<p>1)、stage1：stage1是直接被写入到MBR中去的，这样机器一启动检测完硬件后，就将控制权交给了GRUB的代码。也就是上图所看到的前446个字节空间中存放的是stage1的代码。BIOS将stage1载入内存中0x7c00处并跳转执行。stage1（/stage1/start.S）的任务非常单纯，仅仅是将硬盘0头0道2扇区读入内存。而0头0道2扇区内容是源代码中的/stage2/start.S，编译后512字节，它是stage2或者stage1_5的入口。而此时，stage1是没有识别文件系统的能力的。如果感觉脑子有些晕了，那么下面的过程就直接跳过，去看stage2吧！</p>

<h5>【外传】定位硬盘的0头0道2扇区的过程：</h5>

<p>  BIOS将stage1载入内存0x7c00处并执行，然后调用BIOS INIT13中断，将硬盘0头0道2扇区内容载入内存0x7000处，然后调用copy_buffer将其转移到内存0x8000处。在定位0头0道2扇区时通常有两种寻址方式：LBA和CHS。如果你是刨根问底儿型的爱好者，那么此时去找谷哥打听打听这两种方式的来龙去脉吧。</p>

<p>2)、stage2：严格来说这里还应该再区分个stage1.5的，就一并把stage1.5放在这里一起介绍了，免得大家看得心里乱哄哄的。好的，我们继续说0头0到2扇区的/stage2/start.S文件，当它的内容被读入到内存之后，它的主要作用就是负责将stage2或stage1.5从硬盘读到内存中。如果是stage2，它将被载入到0x820处；如果是stage1.5，它将被载入到0x2200处。这里的stage2或者stage1_5不是/boot分区/boot/grub目录下的文件，因为这个时候grub还没有能力识别任何文件系统。</p>

<p> 如果start.S加载stage1.5：stage1.5它存放在硬盘0头0道3扇区向后的位置，stage1_5作为stage1和stage2中间的桥梁，stage1_5有识别文件系统的能力，此后grub才有能力去访问/boot分区/boot/grub目录下的 stage2文件，将stage2载入内存并执行。</p>

<p> 如果start.S加载stage2：同样，这个stage2也不是/boot分区/boot/grub目录下的stage2，这个时候start.S读取的是存放在/boot分区Boot Sector的stage2。这种情况下就有一个限制：因为start.S通过BIOS中断方式直接对硬盘寻址（而非通过访问具体的文件系统），其寻址范围有限，限制在8GB以内。因此这种情况需要将/boot分区分在硬盘8GB寻址空间之前。</p>

<p>假如是情形2，我们将/boot/grub目录下的内容清空，依然能成功启动grub；假如是情形1，将/boot/grub目录下stage2删除后，则系统启动过程中grub会启动失败。</p>

<h4>启动内核</h4>

<p>  当stage2被载入内存执行时，它首先会去解析grub的配置文件/boot/grub/grub.conf，然后加载内核镜像到内存中，并将控制权转交给内核。而内核会立即初始化系统中各设备并做相关的配置工作，其中包括CPU、I/O、存储设备等。</p>

<p>关于Linux的设备驱动程序的加载，有一部分驱动程序直接被编译进内核镜像中，另一部分驱动程序则是以模块的形式放在initrd(ramdisk)中。</p>

<p>  Linux内核需要适应多种不同的硬件架构，但是将所有的硬件驱动编入内核又是不实际的，而且内核也不可能每新出一种硬件结构，就将该硬件的设备驱动写入内核。实际上Linux的内核镜像仅是包含了基本的硬件驱动，在系统安装过程中会检测系统硬件信息，根据安装信息和系统硬件信息将一部分设备驱动写入 initrd 。这样在以后启动系统时，一部分设备驱动就放在initrd中来加载。这里有必要给大家再多介绍一下initrd这个东东：</p>

<p>  initrd 的英文含义是 bootloader initialized RAM disk，就是由 boot loader 初始化的内存盘。在 linu2.6内核启动前，boot loader 会将存储介质中的 initrd 文件加载到内存，内核启动时会在访问真正的根文件系统前先访问该内存中的 initrd 文件系统。在 boot loader 配置了 initrd 的情况下，内核启动被分成了两个阶段，第一阶段先执行 initrd 文件系统中的init，完成加载驱动模块等任务，第二阶段才会执行真正的根文件系统中的 /sbin/init 进程。</p>

<p>另外一个概念：initramfs</p>

<p>  initramfs 是在 kernel 2.5中引入的技术，实际上它的含义就是：在内核镜像中附加一个cpio包，这个cpio包中包含了一个小型的文件系统，当内核启动时，内核将这个 cpio包解开，并且将其中包含的文件系统释放到rootfs中，内核中的一部分初始化代码会放到这个文件系统中，作为用户层进程来执行。这样带来的明显的好处是精简了内核的初始化代码，而且使得内核的初始化过程更容易定制。
疑惑的是：我的内核是2.6.32-71.el6.i686版本，但在我的/boot分区下面却存在的是/boot/initramfs-2.6.32-71.el6.i686.img类型的文件，没搞明白，还望高人解惑。我只知道在2.6内核中支持两种格式的initrd，一种是2.4内核的文件系统镜像image-initrd，一种是cpio格式。接下来我们就来探究一下initramfs-2.6.32-71.el6.i686.img里到底放了那些东西。</p>

<p><img src="/images/system/2016-01-21-2.png" alt="" /></p>

<p>在tmp文件夹中解压initrd.img里的内容：</p>

<p><img src="/images/system/2016-01-21-3.png" alt="" /></p>

<p>如果initrd.img文件的格式显示为“initrd.img:ISO 9660 CD-ROM filesystem data”，则可直接输入命令“mount -o loop initrd.img /mnt/test”进行挂载。</p>

<p>通过上的分析和我们的验证，我们确实得到了这样的结论：</p>

<p>  grub的stage2将initrd加载到内存里，让后将其中的内容释放到内容中，内核便去执行initrd中的init脚本，这时内核将控制权交给了init文件处理。我们简单浏览一下init脚本的内容，发现它也主要是加载各种存储介质相关的设备驱动程序。当所需的驱动程序加载完后，会创建一个根设备，然后将根文件系统rootfs以只读的方式挂载。这一步结束后，释放未使用的内存，转换到真正的根文件系统上面去，同时运行/sbin/init程序，执行系统的1号进程。此后系统的控制权就全权交给/sbin/init进程了。</p>

<h4>初始化系统</h4>

<p>经过千辛万苦的跋涉，我们终于接近黎明的曙光了。接下来就是最后一步了：初始化系统。/sbin/init进程是系统其他所有进程的父进程，当它接管了系统的控制权先之后，它首先会去读取/etc/inittab文件来执行相应的脚本进行系统初始化，如设置键盘、字体，装载模块，设置网络等。主要包括以下工作：</p>

<p>1)、执行系统初始化脚本(/etc/rc.d/rc.sysinit)，对系统进行基本的配置，以读写方式挂载根文件系统及其它文件系统，到此系统算是基本运行起来了，后面需要进行运行级别的确定及相应服务的启动。rc.sysinit所做的事情(不同的Linux发行版，该文件可能有些差异)如下：</p>

<p>（1）获取网络环境与主机类型。首先会读取网络环境设置文件"/etc/sysconfig/network"，获取主机名称与默认网关等网络环境。</p>

<p>（2）测试与载入内存设备/proc及usb设备/sys。除了/proc外，系统会主动检测是否有usb设备，并主动加载usb驱动，尝试载入usb文件系统。</p>

<p>（3）决定是否启动SELinux。</p>

<p>（4）接口设备的检测与即插即用（pnp）参数的测试。</p>

<p>（5）用户自定义模块的加载。用户可以再"/etc/sysconfig/modules/*.modules"加入自定义的模块，此时会加载到系统中。</p>

<p>（6）加载核心的相关设置。按"/etc/sysctl.conf"这个文件的设置值配置功能。</p>

<p>（7）设置系统时间（clock）。</p>

<p>（8）设置终端的控制台的字形。</p>

<p>（9）设置raid及LVM等硬盘功能。</p>

<p>（10）以方式查看检验磁盘文件系统。</p>

<p>（11）进行磁盘配额quota的转换。</p>

<p>（12）重新以读取模式载入系统磁盘。</p>

<p>（13）启动quota功能。</p>

<p>（14）启动系统随机数设备（产生随机数功能）。</p>

<p>（15）清楚启动过程中的临时文件。</p>

<p>（16）将启动信息加载到"/var/log/dmesg"文件中。</p>

<p> 当/etc/rc.d/rc.sysinit执行完后，系统就可以顺利工作了，只是还需要启动系统所需要的各种服务，这样主机才可以提供相关的网络和主机功能，因此便会执行下面的脚本。</p>

<p>2)、执行/etc/rc.d/rc脚本。该文件定义了服务启动的顺序是先K后S，而具体的每个运行级别的服务状态是放在/etc/rc.d/rc<em>.d（</em>=0~6）目录下，所有的文件均是指向/etc/init.d下相应文件的符号链接。rc.sysinit通过分析/etc/inittab文件来确定系统的启动级别，然后才去执行/etc/rc.d/rc*.d下的文件。</p>

<p>/etc/init.d-> /etc/rc.d/init.d</p>

<p>/etc/rc ->/etc/rc.d/rc</p>

<p>/etc/rc<em>.d ->/etc/rc.d/rc</em>.d</p>

<p>/etc/rc.local-> /etc/rc.d/rc.local</p>

<p>/etc/rc.sysinit-> /etc/rc.d/rc.sysinit</p>

<p>也就是说，/etc目录下的init.d、rc、rc*.d、rc.local和rc.sysinit均是指向/etc/rc.d目录下相应文件和文件夹的符号链接。我们以启动级别3为例来简要说明一下。</p>

<p>/etc/rc.d/rc3.d目录，该目录下的内容全部都是以 S 或 K 开头的链接文件，都链接到"/etc/rc.d/init.d"目录下的各种shell脚本。S表示的是启动时需要start的服务内容，K表示关机时需要关闭的服务内容。/etc/rc.d/rc<em>.d中的系统服务会在系统后台启动，如果要对某个运行级别中的服务进行更具体的定制，通过chkconfig命令来操作，或者通过setup、ntsys、system-config-services来进行定制。如果我们需要自己增加启动的内容，可以在init.d目录中增加相关的shell脚本，然后在rc</em>.d目录中建立链接文件指向该shell脚本。这些shell脚本的启动或结束顺序是由S或K字母后面的数字决定，数字越小的脚本越先执行。例如，/etc/rc.d/rc3.d /S01sysstat就比/etc/rc.d/rc3.d /S99local先执行。</p>

<p>3)、执行用户自定义引导程序/etc/rc.d/rc.local。其实当执行/etc/rc.d/rc3.d/S99local时，它就是在执行/etc/rc.d/rc.local。S99local是指向rc.local的符号链接。就是一般来说，自定义的程序不需要执行上面所说的繁琐的建立shell增加链接文件的步骤，只需要将命令放在rc.local里面就可以了，这个shell脚本就是保留给用户自定义启动内容的。</p>

<p>4)、完成了系统所有的启动任务后，linux会启动终端或X-Window来等待用户登录。tty1,tty2,tty3&hellip;这表示在运行等级1，2，3，4的时候，都会执行"/sbin/mingetty"，而且执行了6个，所以linux会有6个纯文本终端，mingetty就是启动终端的命令。</p>

<p>除了这6个之外还会执行"/etc/X11/prefdm-nodaemon"这个主要启动X-Window</p>

<p>至此，系统就启动完毕了。以上分析不到的地方还请各位大虾不吝指正。</p>

<p>关于Linux的其他分析内容下次再继续写。
最后附上一张非常完整的系统启动流程图，适合各个水平阶段的读者。</p>

<p><img src="/images/system/2016-01-21-4.jpg" alt="" /></p>

<p><a href="http://blog.itpub.net/8111049/viewspace-680043">http://blog.itpub.net/8111049/viewspace-680043</a></p>

<p><a href="http://bbs.chinaunix.net/thread-2046548-1-1.html">http://bbs.chinaunix.net/thread-2046548-1-1.html</a></p>

<p><a href="http://blog.chinaunix.net/uid-26495963-id-3066282.html">http://blog.chinaunix.net/uid-26495963-id-3066282.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux系统时间和硬件时钟问题(date和hwclock)]]></title>
    <link href="http://abcdxyzk.github.io/blog/2016/01/06/system-base-time/"/>
    <updated>2016-01-06T10:48:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2016/01/06/system-base-time</id>
    <content type="html"><![CDATA[<p><a href="http://rpf413.blog.163.com/blog/static/4556376020122831444674/">http://rpf413.blog.163.com/blog/static/4556376020122831444674/</a></p>

<h4>总结一下hwclock，这个容易晕：</h4>

<p>1）/etc/sysconfig/clock 文件，只对 hwclock 命令有效，且只在系统启动和关闭的时候才有用（修改了其中的 UTC=true 到 UTC=false 的前后，执行 hwclock (<code>--utc</code>, 或 <code>--localtime</code>) 都没有变化，要重启系统后才生效）；</p>

<p>2）/etc/rc.d/rc.sysinit 文件，run once at boot time，其中有从硬件时钟同步时间到系统时间的操作；</p>

<p>3）<code>hwclock --localtime</code> 的输出，才是硬件时钟真正的时间。如果输出结果带时区（比如CST），还要看/etc/sysconfig/clock里的UTC参数，如果 UTC=false，那时区有意义；如果 UTC=true，那时区没意义，实际上是UTC时间。</p>

<p>4）在 /etc/sysconfig/clock 中 UTC=false 时，date、hwclock、<code>hwclcok --localtime</code> 输出的时间应该都一致，且此时 <code>hwclock --utc</code>是没有意义的；</p>

<p>5）在 /etc/sysconfig/clock 中 UTC=ture 时，date、hwclock 的输出是一致的，<code>hwclock --localtime</code> 的输出则是UTC时间；</p>

<p>6）如果不想在输出中带时区，则 export LANG=C ，然后再运行 hwclock 就没有什么CST了，免得时区误导你；</p>

<p>7）<code>hwclock --utc</code> 很闹腾，还是别看了，你会晕的。。。</p>

<p>8）系统关闭时会同步系统时间到硬件时钟，系统启动时会从硬件时钟读取时间更新到系统，这2个步骤都要根据 /etc/sysconfig/clock 文件中UTC的参数来设置时区转换。</p>

<h4>实际案例分析</h4>

<p>修改了 /etc/sysconfig/clock 中UTC参数但系统未正常关闭的情况</p>

<p>修改 /etc/sysconfig/clock 文件后，如果系统内核突然崩溃，然后直接按电源重启，则系统没有进行 系统时间到硬件时钟的 同步；但是 系统启动时，又根据 /etc/sysconfig/clock 中UTC的参数，来同步硬件时钟到系统，这时就会出现时间问题：</p>

<p>0）假设系统的时区为CST（UTC+8）；<br/>
1）假设原 /etc/sysconfig/clock 中 UTC=true，修改成 UTC=false；<br/>
2）如果此时系统未正常关机，系统时间未按参数 UTC=false 同步时间到硬件时钟（没有+8小时）；<br/>
3）但系统被按电源重启后，系统读取到 UTC=false，认为硬件时钟为CST时间，直接用于系统时间；<br/>
4）那么此时，系统时间将少了8小时。</p>

<hr />

<p><a href="http://hi.baidu.com/lujunqianglw/blog/item/bc2d9144d24fc48fb3b7dc1d.html">http://hi.baidu.com/lujunqianglw/blog/item/bc2d9144d24fc48fb3b7dc1d.html</a></p>

<h4>一、首先要弄清几个概念：</h4>

<h5>1. “系统时间”与“硬件时间”</h5>

<p>系统时间: 一般说来就是我们执行 date 命令看到的时间，linux系统下所有的时间调用（除了直接访问硬件时间的命令）都是使用的这个时间。</p>

<p>硬件时间: 主板上BIOS中的时间，由主板电池供电来维持运行，系统开机时要读取这个时间，并根据它来设定系统时间（注意：系统启动时根据硬件时间设定系统时间的过程可能存在时区换算，这要视具体的系统及相关设置而定）。</p>

<h5>2. “UTC时间”与“本地时间”</h5>

<p>UTC时间：Coordinated Universal 8 e2 i( H7 t0 ^/ ^Time 世界协调时间（又称世界标准时间、世界统一时间），在一般精度要求下，它与GMT（Greenwich Mean Time，格林威治标准时间）是一样的，其实也就是说 GMT≈UTC，但 UTC 是以原子钟校准的，更精确。</p>

<p>本地时间：由于处在不同的时区，本地时间一般与UTC是不同的，换算方法就是</p>

<p>本地时间 = UTC + 时区 或 UTC = 本地时间 - 时区</p>

<p>时区东为正，西为负，例如在中国，本地时间都使用北京时间，在linux上显示就是 CST（China Standard Time，中国标准时，注意美国的中部标准时Central Standard Time也缩写为CST，与这里的CST不是一回事！），时区为东八区，也就是 +8 区，所以 CST=UTC+(+8小时) 或 UTC=CST-(+8小时)。</p>

<h4>二、时间命令</h4>

<h5>1. 系统时间 date</h5>

<p>直接调用 date，得到的是本地时间。如果想得到UTC时间的话，使用 date -u。
<code>
    [12-01 19:07&gt; ~]$ date
    2009年 12月 07日 星期一 14:22:20 CST
    [12-01 19:07&gt; ~]$ date -u
    2009年 12月 07日 星期一 06:22:22 UTC
</code></p>

<h5>2. 硬件时间 /sbin/hwclock</h5>

<p>直接调用 /sbin/hwclock 显示的时间就是 BIOS 中的时间吗？未必！这要看 /etc/sysconfig/clock 中是否启用了UTC，如果启用了UTC（UTC=true），显示的其实是经过时区换算的时间而不是BIOS中真正的时间，如果加上 &ndash;localtime 选项，则得到的总是 BIOS 中实际的时间.</p>

<pre><code>    [12-01 19:07&gt; ~]# hwclock
    2009年12月07日 星期一 14时28分43秒 -0.611463 seconds
    [12-01 19:07&gt; ~]# hwclock --utc
    2009年12月07日 星期一 14时28分46秒 -0.594189 seconds
    [12-01 19:07&gt; ~]# hwclock --localtime
    2009年12月07日 星期一 06时28分50秒 -0.063875 seconds
</code></pre>

<h5>3. /etc/localtime</h5>

<p>这个文件用来设置系统的时区，将 /usr/share/zoneinfo/ 中相应文件拷贝到/etc下并重命名为 localtime 即可修改时区设置，而且这种修改对 date 命令是及时生效的。不论是 date 还是 hwclock 都会用到这个文件，会根据这个文件的时区设置来进行UTC和本地之间之间的换算。</p>

<h5>4. /etc/sysconfig/clock</h5>

<p>这个文件只对 hwclock 有效，而且似乎是只在系统启动和关闭的时候才有用，比如修改了其中的 UTC=true 到 UTC=false 的前后，执行 hwclock (<code>--utc</code>, 或 <code>--localtime</code>) 都没有变化，要重启系统后才生效。注：如果设置 UTC=false 并重启系统后,执行一些命令结果如下：</p>

<pre><code>    date 2009年 12月 07日 星期一 19:26:29 CST
    date -u 2009年 12月 07日 星期一 11:26:29 UTC
    hwclock 2009年12月07日 星期一 19时26分30秒 -0.442668 seconds
    hwclock --utc 2009年12月08日 星期二 03时26分31秒 -0.999091 seconds
    hwclock --localtime 2009年12月07日 星期一 19时26分32秒 -0.999217 seconds
</code></pre>

<p>可见，如果不使用UTC，BIOS时间（红色部分）就是系统本地时间，而且注意这时执行 <code>hwclock --utc</code> 得到的结果没有任何意义，因为这里我们已经禁用了UTC，而且也明显不符合“本地时间=UTC+时区”的关系。</p>

<h4>三、linux与windows双系统间的时间同步</h4>

<p>系统启动和关闭时，硬件时间与系统时间之间的同步有两种方式(假设在中国，用CST代表本地时间)：</p>

<p>方式A: 使用UTC（对linux就是 /etc/sysconfig/clock 中 UTC=true）</p>

<p>开机: BIOS&mdash;&mdash;->UTC（将BIOS中的时间看成是UTC）&mdash;&mdash;(时区变化)&mdash;&ndash;>CST<br/>
关机: CST &mdash;&mdash;-(时区变化)&mdash;&ndash;>UTC&mdash;&mdash;-存储到&mdash;&mdash;>BIOS</p>

<p>方式B: 不使用UTC（对linux就是 /etc/sysconfig/clock 中 UTC=false）</p>

<p>开机: BIOS&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;>CST（将BIOS中的时间看成是CST）<br/>
关机: CST &mdash;&mdash;&mdash;存储到&mdash;&mdash;>BIOS</p>

<hr />

<p>FIX:</p>

<p>方式A: 使用UTC（对linux就是 /etc/sysconfig/clock 中 UTC=true）</p>

<p>关机: CST &mdash;&mdash;-操作系统根据时区算出UTC时间&mdash;&mdash;-存储到&mdash;&mdash;>BIOS<br/>
开机: BIOS&mdash;&mdash;->BIOS中的时间是UTC&mdash;&mdash;&mdash;&ndash;操作系统根据时区计算出localtime&mdash;&mdash;&mdash;-CST</p>

<p>方式B: 不使用UTC（对linux就是 /etc/sysconfig/clock 中 UTC=false）</p>

<p>关机: CST &mdash;&mdash;&ndash;操作系统中UTC=false，直接将localtime存储到&mdash;&mdash;>BIOS<br/>
开机: BIOS&mdash;&mdash;&ndash;BIOS中的时间是localtime&mdash;&ndash;操作系统中UTC=false，BIOS时间当成localtime&mdash;&mdash;&ndash;>CST（将BIOS中的时间看成是CST）</p>

<hr />

<p>通过设定 /etc/sysconfig/clock，linux可以支持这两种方式，然而windows只支持方式B（至少是默认支持B，而我不知道怎么能让它支 持A），那么在双系统情况下，如果linux设成A方式，那么在linux与windows系统切换时一定会造成时间混乱的，解决办法就是将linux中 的UTC禁用，也设成B方式就可以了。</p>

<p>注：可以通过 <code>hwclock --hctosys</code> 来利用硬件时间来设置系统时间（注意不是简单的复制BIOS中的时间为系统时间，要看是否使用UTC，如果使用的话则要做时区换算），通过 <code>hwclock --systohc</code> 来根据系统时间设置硬件时间（也要看是否启用UTC来决定是否做时区换算）。</p>

<p>总之，不论使用 <code>--systohc</code> 还是 <code>--hctosys</code>，同步后直接运行不带参数的 hwclock 得到的时间与直接运行 date 得到的时间应该一致，这个时间是否就是BIOS中的时间（<code>hwclock --localtime</code>)那就不一定了，如果启用了UTC就不是，没启用UTC就是。</p>

<p>而且还要注意：在系统中手动使用 <code>hwclock hwclock --set --date='yyyy-mm-dd'</code> 来设置BIOS时间只在系统运行时有效，因为当系统关闭时，还会按设定好的方式根据系统时间来重设BIOS时间的，于是手动的设置便被覆盖掉了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[cpuset子系统]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/08/07/cgroup-9/"/>
    <updated>2015-08-07T17:26:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/08/07/cgroup-9</id>
    <content type="html"><![CDATA[<p><a href="http://www.cnblogs.com/lisperl/archive/2012/05/02/2478817.html">http://www.cnblogs.com/lisperl/archive/2012/05/02/2478817.html</a></p>

<p>cpuset子系统为cgroup 中的任务分配独立 CPU（在多核系统）和内存节点。Cpuset子系统为定义了一个叫cpuset的数据结构来管理cgroup中的任务能够使用的cpu和内存节点。Cpuset定义如下：
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
<span class='line-number'>193</span>
<span class='line-number'>194</span>
<span class='line-number'>195</span>
<span class='line-number'>196</span>
<span class='line-number'>197</span>
<span class='line-number'>198</span>
<span class='line-number'>199</span>
<span class='line-number'>200</span>
<span class='line-number'>201</span>
<span class='line-number'>202</span>
<span class='line-number'>203</span>
<span class='line-number'>204</span>
<span class='line-number'>205</span>
<span class='line-number'>206</span>
<span class='line-number'>207</span>
<span class='line-number'>208</span>
<span class='line-number'>209</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>struct cpuset {
</span><span class='line'>    struct cgroup_subsys_state css;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    unsigned long flags; /* "unsigned long" so bitops work */
</span><span class='line'>cpumask_var_t cpus_allowed; /* CPUs allowed to tasks in cpuset */
</span><span class='line'>nodemask_t mems_allowed; /* Memory Nodes allowed to tasks */
</span><span class='line'>
</span><span class='line'>struct cpuset *parent; /* my parent */
</span><span class='line'>
</span><span class='line'>struct fmeter fmeter; /* memory_pressure filter */
</span><span class='line'>
</span><span class='line'>/* partition number for rebuild_sched_domains() */
</span><span class='line'>int pn;
</span><span class='line'>
</span><span class='line'>/* for custom sched domain */
</span><span class='line'>int relax_domain_level;
</span><span class='line'>
</span><span class='line'>/* used for walking a cpuset heirarchy */
</span><span class='line'>struct list_head stack_list;
</span><span class='line'>};
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;其中css字段用于task或cgroup获取cpuset结构。
</span><span class='line'>
</span><span class='line'>cpus_allowed和mems_allowed定义了该cpuset包含的cpu和内存节点。
</span><span class='line'>
</span><span class='line'>Parent字段用于维持cpuset的树状结构，stack_list则用于遍历cpuset的层次结构。
</span><span class='line'>
</span><span class='line'>Pn和relax_domain_level是跟Linux 调度域相关的字段，pn指定了cpuset的调度域的分区号，而relax_domain_level表示进行cpu负载均衡寻找空闲cpu的策略。
</span><span class='line'>
</span><span class='line'>除此之外，进程的task_struct结构体里面还有一个cpumask_t cpus_allowed成员，用以存储进程的cpus_allowed信息;一个nodemask_t mems_allowed成员，用于存储进程的mems_allowed信息。
</span><span class='line'>
</span><span class='line'>Cpuset子系统的实现是通过在内核代码加入一些hook代码。由于代码比较散，我们逐条分析。
</span><span class='line'>
</span><span class='line'>在内核初始化代码（即start_kernel函数）中插入了对cpuset_init调用的代码，这个函数用于cpuset的初始化。
</span><span class='line'>
</span><span class='line'>下面我们来看这个函数：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;int __init cpuset_init(void)
</span><span class='line'>{
</span><span class='line'>int err = 0;
</span><span class='line'>
</span><span class='line'>if (!alloc_cpumask_var(&amp;top_cpuset.cpus_allowed, GFP_KERNEL))
</span><span class='line'>    BUG();
</span><span class='line'>
</span><span class='line'>cpumask_setall(top_cpuset.cpus_allowed);
</span><span class='line'>nodes_setall(top_cpuset.mems_allowed);
</span><span class='line'>
</span><span class='line'>fmeter_init(&amp;top_cpuset.fmeter);
</span><span class='line'>set_bit(CS_SCHED_LOAD_BALANCE, &amp;top_cpuset.flags);
</span><span class='line'>top_cpuset.relax_domain_level = -1;
</span><span class='line'>
</span><span class='line'>err = register_filesystem(&amp;cpuset_fs_type);
</span><span class='line'>if (err &lt; 0)
</span><span class='line'>    return err;
</span><span class='line'>
</span><span class='line'>if (!alloc_cpumask_var(&amp;cpus_attach, GFP_KERNEL))
</span><span class='line'>    BUG();
</span><span class='line'>
</span><span class='line'>number_of_cpusets = 1;
</span><span class='line'>return 0;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;cpumask_setall和nodes_setall将top_cpuset能使用的cpu和内存节点设置成所有节点。紧接着，初始化fmeter，设置top_cpuset的load balance标志。最后注册cpuset文件系统，这个是为了兼容性，因为在cgroups之前就有cpuset了，不过在具体实现时，对cpuset文件系统的操作都被重定向了cgroup文件系统。
</span><span class='line'>
</span><span class='line'>除了这些初始化工作，cpuset子系统还在do_basic_setup函数（此函数在kernel_init中被调用）中插入了对cpuset_init_smp的调用代码，用于smp相关的初始化工作。
</span><span class='line'>
</span><span class='line'>下面我们看这个函数：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;void __init cpuset_init_smp(void)
</span><span class='line'>{
</span><span class='line'>cpumask_copy(top_cpuset.cpus_allowed, cpu_active_mask);
</span><span class='line'>top_cpuset.mems_allowed = node_states[N_HIGH_MEMORY];
</span><span class='line'>
</span><span class='line'>hotcpu_notifier(cpuset_track_online_cpus, 0);
</span><span class='line'>hotplug_memory_notifier(cpuset_track_online_nodes, 10);
</span><span class='line'>
</span><span class='line'>cpuset_wq = create_singlethread_workqueue("cpuset");
</span><span class='line'>BUG_ON(!cpuset_wq);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;首先，将top_cpuset的cpu和memory节点设置成所有online的节点，之前初始化时还不知道有哪些online节点所以只是简单设成所有，在smp初始化后就可以将其设成所有online节点了。然后加入了两个hook函数，cpuset_track_online_cpus和cpuset_track_online_nodes，这个两个函数将在cpu和memory热插拔时被调用。
</span><span class='line'>
</span><span class='line'>cpuset_track_online_cpus函数中调用scan_for_empty_cpusets函数扫描空的cpuset，并将其下的进程移到其非空的parent下，同时更新cpuset的cpus_allowed信息。cpuset_track_online_nodes的处理类似。
</span><span class='line'>
</span><span class='line'>那cpuset又是怎么对进程的调度起作用的呢？
</span><span class='line'>
</span><span class='line'>这个就跟task_struct中cpu_allowed字段有关了。首先，这个cpu_allowed和进程所属的cpuset的cpus_allowed保持一致；其次，在进程被fork出来的时候，进程继承了父进程的cpuset和cpus_allowed字段；最后，进程被fork出来后，除非指定CLONE_STOPPED标记，都会被调用wake_up_new_task唤醒，在wake_up_new_task中有：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;cpu = select_task_rq(rq, p, SD_BALANCE_FORK, 0);
</span><span class='line'>set_task_cpu(p, cpu);
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;即为新fork出来的进程选择运行的cpu，而select_task_rq会调用进程所属的调度器的函数，对于普通进程，其调度器是CFS，CFS对应的函数是select_task_rq_fair。在select_task_rq_fair返回选到的cpu后，select_task_rq会对结果和cpu_allowed比较：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;if (unlikely(!cpumask_test_cpu(cpu, &amp;p-&gt;cpus_allowed) ||
</span><span class='line'> !cpu_online(cpu)))
</span><span class='line'>cpu = select_fallback_rq(task_cpu(p), p);
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;这就保证了新fork出来的进程只能在cpu_allowed中的cpu上运行。
</span><span class='line'>
</span><span class='line'>对于被wake up的进程来说，在被调度之前，也会调用select_task_rq选择可运行的cpu。
</span><span class='line'>
</span><span class='line'>这就保证了进程任何时候都只会在cpu_allowed中的cpu上运行。
</span><span class='line'>
</span><span class='line'>最后说一下，如何保证task_struct中的cpus_allowd和进程所属的cpuset中的cpus_allowed一致。首先，在cpu热插拔时，scan_for_empty_cpusets会更新task_struct中的cpus_allowed信息，其次对cpuset下的控制文件写入操作时也会更新task_struct中的cpus_allowed信息,最后当一个进程被attach到其他cpuset时，同样会更新task_struct中的cpus_allowed信息。
</span><span class='line'>
</span><span class='line'>在cpuset之前，Linux内核就提供了指定进程可以运行的cpu的方法。通过调用sched_setaffinity可以指定进程可以运行的cpu。Cpuset对其进行了扩展，保证此调用设定的cpu仍然在cpu_allowed的范围内。在sched_setaffinity中，插入了这样两行代码：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;cpuset_cpus_allowed(p, cpus_allowed);
</span><span class='line'>cpumask_and(new_mask, in_mask, cpus_allowed);
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;其中cpuset_cpus_allowed返回进程对应的cpuset中的cpus_allowed，cpumask_and则将cpus_allowed和调用sched_setaffinity时的参数in_mask相与得出进程新的cpus_allowed。
</span><span class='line'>
</span><span class='line'>通过以上代码的嵌入，Linux内核实现了对进程可调度的cpu的控制。下面我们来分析一下cpuset对memory节点的控制。
</span><span class='line'>
</span><span class='line'>Linux中内核分配物理页框的函数有6个:alloc_pages,alloc_page,__get_free_pages,__get_free_page,get_zeroed_page,__get_dma_pages,这些函数最终都通过alloc_pages实现，而alloc_pages又通过__alloc_pages_nodemask实现，在__alloc_pages_nodemask中，调用get_page_from_freelist从zone list中分配一个page，在get_page_from_freelist中调用cpuset_zone_allowed_softwall判断当前节点是否属于mems_allowed。通过附加这样一个判断，保证进程从mems_allowed中的节点分配内存。
</span><span class='line'>
</span><span class='line'>Linux在cpuset出现之前，也提供了mbind, set_mempolicy来限定进程可用的内存节点。Cpuset子系统对其做了扩展，扩展的方法跟扩展sched_setaffinity类似，通过导出cpuset_mems_allowed，返回进程所属的cupset允许的内存节点，对mbind，set_mempolicy的参数进行过滤。
</span><span class='line'>
</span><span class='line'>最后让我们来看一下，cpuset子系统最重要的两个控制文件：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;{
</span><span class='line'>.name = "cpus",
</span><span class='line'>.read = cpuset_common_file_read,
</span><span class='line'>.write_string = cpuset_write_resmask,
</span><span class='line'>.max_write_len = (100U + 6 * NR_CPUS),
</span><span class='line'>.private = FILE_CPULIST,
</span><span class='line'>},
</span><span class='line'>
</span><span class='line'>{
</span><span class='line'>.name = "mems",
</span><span class='line'>.read = cpuset_common_file_read,
</span><span class='line'>.write_string = cpuset_write_resmask,
</span><span class='line'>.max_write_len = (100U + 6 * MAX_NUMNODES),
</span><span class='line'>.private = FILE_MEMLIST,
</span><span class='line'>},
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;```
</span><span class='line'>通过cpus文件，我们可以指定进程可以使用的cpu节点，通过mems文件，我们可以指定进程可以使用的memory节点。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这两个文件的读写都是通过cpuset_common_file_read和cpuset_write_resmask实现的，通过private属性区分。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;在cpuset_common_file_read中读出可用的cpu或memory节点；在cpuset_write_resmask中则根据文件类型分别调用update_cpumask和update_nodemask更新cpu或memory节点信息。&lt;/p&gt;
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[memory子系统]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2015/08/07/cgroup-8/"/&gt;
</span><span class='line'>&lt;updated&gt;2015-08-07T17:22:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2015/08/07/cgroup-8&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;p&gt;&lt;a href="http://www.cnblogs.com/lisperl/archive/2012/04/28/2474872.html"&gt;http://www.cnblogs.com/lisperl/archive/2012/04/28/2474872.html&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;memory 子系统可以设定 cgroup 中任务使用的内存限制，并自动生成由那些任务使用的内存资源报告。memory子系统是通过linux的resource counter机制实现的。下面我们就先来看一下resource counter机制。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;resource counter是内核为子系统提供的一种资源管理机制。这个机制的实现包括了用于记录资源的数据结构和相关函数。Resource counter定义了一个res_counter的结构体来管理特定资源，定义如下：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>struct res_counter {
</span><span class='line'>    unsigned long long usage;
</span><span class='line'>    unsigned long long max_usage;
</span><span class='line'>    unsigned long long limit;
</span><span class='line'>    unsigned long long soft_limit;
</span><span class='line'>    unsigned long long failcnt; /*
</span><span class='line'>    spinlock_t lock;
</span><span class='line'>    struct res_counter *parent;
</span><span class='line'>};
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Usage用于记录当前已使用的资源，max_usage用于记录使用过的最大资源量，limit用于设置资源的使用上限，进程组不能使用超过这个限制的资源，soft_limit用于设定一个软上限，进程组使用的资源可以超过这个限制，failcnt用于记录资源分配失败的次数，管理可以根据这个记录，调整上限值。Parent指向父节点，这个变量用于处理层次性的资源管理。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;除了这个关键的数据结构，resource counter还定义了一系列相关的函数。下面我们来看几个关键的函数。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    void res_counter_init(struct res_counter *counter, struct res_counter *parent)
</span><span class='line'>{
</span><span class='line'>    spin_lock_init(&amp;counter-&gt;lock);
</span><span class='line'>    counter-&gt;limit = RESOURCE_MAX;
</span><span class='line'>    counter-&gt;soft_limit = RESOURCE_MAX;
</span><span class='line'>    counter-&gt;parent = parent;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这个函数用于初始化一个res_counter。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;第二个关键的函数是int res_counter_charge(struct res_counter *counter, unsigned long val, struct res_counter **limit_fail_at)。当资源将要被分配的时候，资源就要被记录到相应的res_counter里。这个函数作用就是记录进程组使用的资源。在这个函数中有：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    for (c = counter; c != NULL; c = c-&gt;parent) {
</span><span class='line'>    spin_lock(&amp;c-&gt;lock);
</span><span class='line'>    ret = res_counter_charge_locked(c, val);
</span><span class='line'>    spin_unlock(&amp;c-&gt;lock);
</span><span class='line'>    if (ret &lt; 0) {
</span><span class='line'>        *limit_fail_at = c;
</span><span class='line'>        goto undo;
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;在这个循环里，从当前res_counter开始，从下往上逐层增加资源的使用量。我们来看一下res_counter_charge_locked这个函数，这个函数顾名思义就是在加锁的情况下增加使用量。实现如下：</span></code></pre></td></tr></table></div></figure>
    {
        if (counter->usage + val > counter->limit) {
            counter->failcnt++;
            return -ENOMEM;
        }</p>

<pre><code>    counter-&gt;usage += val;
    if (counter-&gt;usage &gt; counter-&gt;max_usage)
        counter-&gt;max_usage = counter-&gt;usage;
    return 0;
}
</code></pre>

<pre><code>首先判断是否已经超过使用上限，如果是的话就增加失败次数，返回相关代码；否则就增加使用量的值，如果这个值已经超过历史最大值，则更新最大值。

第三个关键的函数是void res_counter_uncharge(struct res_counter *counter, unsigned long val)。当资源被归还到系统的时候，要在相应的res_counter减轻相应的使用量。这个函数作用就在于在于此。实现如下：
</code></pre>

<pre><code>for (c = counter; c != NULL; c = c-&gt;parent) {
    spin_lock(&amp;c-&gt;lock);
    res_counter_uncharge_locked(c, val);
    spin_unlock(&amp;c-&gt;lock);
}
</code></pre>

<pre><code>从当前counter开始，从下往上逐层减少使用量，其中调用了res_counter_uncharge_locked，这个函数的作用就是在加锁的情况下减少相应的counter的使用量。

有这些数据结构和函数，只需要在内核分配资源的时候，植入相应的charge函数，释放资源时，植入相应的uncharge函数，就能实现对资源的控制了。

介绍完resource counter，我们再来看memory子系统是利用resource counter实现对内存资源的管理的。

memory子系统定义了一个叫mem_cgroup的结构体来管理cgroup相关的内存使用信息，定义如下：
</code></pre>

<pre><code>struct mem_cgroup {
    struct cgroup_subsys_state css;
    struct res_counter res;
    struct res_counter memsw;
    struct mem_cgroup_lru_info info;
    spinlock_t reclaim_param_lock;
    int prev_priority;
    int last_scanned_child;
    bool use_hierarchy;
    atomic_t oom_lock;
    atomic_t refcnt;
    unsigned int swappiness;
    int oom_kill_disable;
    bool memsw_is_minimum;
    struct mutex thresholds_lock;
    struct mem_cgroup_thresholds thresholds;
    struct mem_cgroup_thresholds memsw_thresholds;
    struct list_head oom_notify;
    unsigned long  move_charge_at_immigrate;
    struct mem_cgroup_stat_cpu *stat;
};
</code></pre>

<pre><code>跟其他子系统一样，mem_cgroup也包含了一个cgroup_subsys_state成员，便于task或cgroup获取mem_cgroup。

mem_cgroup中包含了两个res_counter成员，分别用于管理memory资源和memory+swap资源，如果memsw_is_minimum为true，则res.limit=memsw.limit，即当进程组使用的内存超过memory的限制时，不能通过swap来缓解。

use_hierarchy则用来标记资源控制和记录时是否是层次性的。

oom_kill_disable则表示是否使用oom-killer。

oom_notify指向一个oom notifier event fd链表。

另外memory子系统还定义了一个叫page_cgroup的结构体：
</code></pre>

<pre><code>struct page_cgroup {
    unsigned long flags;
    struct mem_cgroup *mem_cgroup;
    struct page *page;
    struct list_head lru; /* per cgroup LRU list */
};
</code></pre>

<pre><code>此结构体可以看作是mem_map的一个扩展，每个page_cgroup都和所有的page关联，而其中的mem_cgroup成员，则将page与特定的mem_cgroup关联起来。

我们知道在linux系统中，page结构体是用来管理物理页框的，一个物理页框对应一个page结构体，而每个进程中的task_struct中都有一个mm_struct来管理进程的内存信息。每个mm_struct知道它属于的进程，进而知道所属的mem_cgroup，而每个page都知道它属于的page_cgroup，进而也知道所属的mem_cgroup，而内存使用量的计算是按cgroup为单位的，这样以来，内存资源的管理就可以实现了。

memory子系统既然是通过resource counter实现的，那肯定会在内存分配给进程时进行charge操作的。下面我们就来看一下这些charge操作：

1.page fault发生时，有两种情况内核需要给进程分配新的页框。一种是进程请求调页（demand paging），另一种是copy on write。内核在handle_pte_fault中进行处理。其中，do_linear_fault处理pte不存在且页面线性映射了文件的情况，do_anonymous_page处理pte不存在且页面没有映射文件的情况，do_nonlinear_fault处理pte存在且页面非线性映射文件的情况，do_wp_page则处理copy on write的情况。其中do_linear_fault和do_nonlinear_fault都会调用__do_fault来处理。Memory子系统则__do_fault、do_anonymous_page、do_wp_page植入mem_cgroup_newpage_charge来进行charge操作。

2.内核在handle_pte_fault中进行处理时，还有一种情况是pte存在且页又没有映射文件。这种情况说明页面之前在内存中，但是后面被换出到swap空间了。内核用do_swap_page函数处理这种情况，memory子系统在do_swap_page加入了mem_cgroup_try_charge_swapin函数进行charge。mem_cgroup_try_charge_swapin是处理页面换入时的charge的，当执行swapoff系统调用（关掉swap空间），内核也会执行页面换入操作，因此mem_cgroup_try_charge_swapin也被植入到了相应的函数中。

3.当内核将page加入到page cache中时，也需要进行charge操作，mem_cgroup_cache_charge函数正是处理这种情况，它被植入到系统处理page cache的add_to_page_cache_locked函数中。

4.最后mem_cgroup_prepare_migration是用于处理内存迁移中的charge操作。

除了charge操作，memory子系统还需要处理相应的uncharge操作。下面我们来看一下uncharge操作：

1.mem_cgroup_uncharge_page用于当匿名页完全unmaped的时候。但是如果该page是swap cache的话，uncharge操作延迟到mem_cgroup_uncharge_swapcache被调用时执行。

2.mem_cgroup_uncharge_cache_page用于page cache从radix-tree删除的时候。但是如果该page是swap cache的话，uncharge操作延迟到mem_cgroup_uncharge_swapcache被调用时执行。

3.mem_cgroup_uncharge_swapcache用于swap cache从radix-tree删除的时候。Charge的资源会被算到swap_cgroup，如果mem+swap controller被禁用了，就不需要这样做了。

4.mem_cgroup_uncharge_swap用于swap_entry的引用数减到0的时候。这个函数主要在mem+swap controller可用的情况下使用的。

5.mem_cgroup_end_migration用于内存迁移结束时相关的uncharge操作。

Charge函数最终都是通过调用__mem_cgroup_try_charge来实现的。在__mem_cgroup_try_charge函数中，调用res_counter_charge(&amp;mem-&gt;res, csize, &amp;fail_res)对memory进行charge，调用res_counter_charge(&amp;mem-&gt;memsw, csize, &amp;fail_res)对memory+swap进行charge。

Uncharge函数最终都是通过调用__do_uncharge来实现的。在__do_uncharge中，分别调用res_counter_uncharge(&amp;mem-&gt;res,PAGE_SIZE)和res_counter_uncharge(&amp;mem-&gt;memsw, PAGE_SIZE)来uncharge memory和memory+swap。

跟其他子系统一样，memory子系统也实现了一个cgroup_subsys。
</code></pre>

<pre><code>struct cgroup_subsys mem_cgroup_subsys = {
    .name = "memory",
    .subsys_id = mem_cgroup_subsys_id,
    .create = mem_cgroup_create,
    .pre_destroy = mem_cgroup_pre_destroy,
    .destroy = mem_cgroup_destroy,
    .populate = mem_cgroup_populate,
    .can_attach = mem_cgroup_can_attach,
    .cancel_attach = mem_cgroup_cancel_attach,
    .attach = mem_cgroup_move_task,
    .early_init = 0,
    .use_id = 1,
};
</code></pre>

<pre><code>Memory子系统中重要的文件有
</code></pre>

<pre><code>memsw.limit_in_bytes
{
    .name = "memsw.limit_in_bytes",
    .private = MEMFILE_PRIVATE(_MEMSWAP, RES_LIMIT),
    .write_string = mem_cgroup_write,
    .read_u64 = mem_cgroup_read,
},
</code></pre>

<pre><code>这个文件用于设定memory+swap上限值。

Limit_in_bytes
</code></pre>

<pre><code>{
    .name = "limit_in_bytes",
    .private = MEMFILE_PRIVATE(_MEM, RES_LIMIT),
    .write_string = mem_cgroup_write,
    .read_u64 = mem_cgroup_read,
},
</code></pre>

<p>```
这个文件用于设定memory上限值。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ns子系统]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/08/07/cgroup-7/"/>
    <updated>2015-08-07T17:20:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/08/07/cgroup-7</id>
    <content type="html"><![CDATA[<p><a href="http://www.cnblogs.com/lisperl/archive/2012/04/26/2471776.html">http://www.cnblogs.com/lisperl/archive/2012/04/26/2471776.html</a></p>

<p>ns子系统是一个比较特殊的子系统。特殊在哪儿呢，首先ns子系统没有自己的控制文件，其次ns子系统没有属于自己的状态信息，这点从ns子系统的ns_cgroup的定义可以看出：
<code>
    struct ns_cgroup {
        struct cgroup_subsys_state css;
    };
</code>
它只有一个cgroup_subsys_state成员。</p>

<p>最后ns子系统的实现也比较简单，只是提供了一个ns_cgroup_clone函数，在copy_process和unshare_nsproxy_namespaces被调用。而ns_cgroup_clone函数本身的实现也很简单，只是在当前的cgroup下创建了一个子cgroup，该子cgroup完全clone了当前cgroup的信息，然后将当前的进程移到新建立的cgroup中。</p>

<p>这样看来，好像ns子系统没什么意义，其实不然。要想了解ns子系统的意义，就要分析一下ns_cgroup_clone被调用的时机了。我们来看copy_process中的代码：
<code>
    if (current-&gt;nsproxy != p-&gt;nsproxy) {
        retval = ns_cgroup_clone(p, pid);
        if (retval)
            goto bad_fork_free_pid;
    }
</code>
copy_process是在do_fork中被调用的，作用在于为子进程复制父进程的相关信息。这段意思就是当前进程（即父进程）和子进程的命名空间不同时，调用ns_cgroup_clone。这样以来，ns子系统的作用就清楚了，ns子系统实际上是提供了一种同命名空间的进程聚类的机制。具有相同命名空间的进程会在相同cgroup中。</p>

<p>那什么时候，父进程fork出的子进程会拥有不同的命名空间呢，这就设计到了Linux的命名空间的机制了，在这里就不详细讲了。简单说来就是，在调用fork时，加入了特殊flag（比如NEWPID,NEWNS）时，内核会为子进程创建不同的命令空间。</p>

<p>除了这种情况外，ns_cgroup_clone在unshare_nsproxy_namespaces用到了。unshare_nsproxy_namespaces函数被sys_unshare调用，实际上是对unshare系统调用的实现。当指定相应标记时，unshare系统调用会为调用的进程创建不同的命名空间，因此调用ns_cgroup_clone为其创建新的cgroup。</p>
]]></content>
  </entry>
  
</feed>
