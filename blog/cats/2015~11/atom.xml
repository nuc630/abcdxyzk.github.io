<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 2015~11 | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/2015~11/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-11-26T12:16:21+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[c 文件锁flock]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/26/lang-c-flock/"/>
    <updated>2015-11-26T11:35:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/26/lang-c-flock</id>
    <content type="html"><![CDATA[<p><a href="http://blog.csdn.net/lin_fs/article/details/7804494">http://blog.csdn.net/lin_fs/article/details/7804494</a></p>

<p>头文件  #include&lt;sys/file.h></p>

<p>定义函数  int flock(int fd, int operation);</p>

<p>函数说明  flock()会依参数operation所指定的方式对参数fd所指的文件做各种锁定或解除锁定的动作。此函数只能锁定整个文件，无法锁定文件的某一区域。</p>

<p>参数  operation有下列四种情况:<br/>
  LOCK_SH 建立共享锁定。多个进程可同时对同一个文件作共享锁定。<br/>
  LOCK_EX 建立互斥锁定。一个文件同时只有一个互斥锁定。<br/>
  LOCK_UN 解除文件锁定状态。<br/>
  LOCK_NB 无法建立锁定时，此操作可不被阻断，马上返回进程。通常与LOCK_SH或LOCK_EX 做OR(|)组合。<br/>
  单一文件无法同时建立共享锁定和互斥锁定，而当使用dup()或fork()时文件描述词不会继承此种锁定。</p>

<p>返回值  返回0表示成功，若有错误则返回-1，错误代码存于errno。</p>

<p>flock只要在打开文件后，需要对文件读写之前flock一下就可以了，用完之后再flock一下，前面加锁，后面解锁。其实确实是这么简单，但是前段时间用的时候发现点问题，问题描述如下：</p>

<p>  一个进程去打开文件，输入一个整数，然后上一把写锁（LOCK＿EX），再输入一个整数将解锁（LOCK＿UN），另一个进程打开同样一个文件，直接向文件中写数据，发现锁不起作用，能正常写入（我此时用的是超级用户）。google了一大圈发现flock不提供锁检查，也就是说在用flock之前需要用户自己去检查一下是否已经上了锁，说明白点就是读写文件之前用一下flock检查一下文件有没有上锁，如果上锁了flock将会阻塞在那里(An attempt to lock the file using one of these file descriptors may be denied by a lock that the calling process has already placed via another descriptor )，除非用了LOCK_NB。一个完整的用于测试的事例代码如下所示：</p>

<pre><code>    //lockfile.c

    #include &lt;stdio.h&gt;
    #include &lt;unistd.h&gt;
    #include &lt;sys/types.h&gt;
    #include &lt;sys/stat.h&gt;
    #include &lt;fcntl.h&gt;
    #include &lt;errno.h&gt;

    int main()
    {
        int fd,i;
        char path[] = "/home/taoyong/test.txt";
        extern int errno;
        fd = open(path,O_WRONLY|O_CREAT);
        if(fd != -1)
        {
            printf("open file %s ./n", path);
            printf("please input a number to lock the file./n");
            scanf("%d", &amp;i);
            if (flock(fd, LOCK_EX) == 0)
            {
                printf("the file was locked./n");
            }
            else
            {
                printf("the file was not locked./n");
            }
            printf("please input a number to unlock the file./n");
            scanf("%d", &amp;i);
            if (flock(fd, LOCK_UN)==0)
            {
                printf("the file was unlocked./n");
            }
            else
            {
                printf("the file was not unlocked./n");
            }
            close(fd);
        }
        else
        {
            printf("cannot open file %s/n", path);
            printf("errno:%d/n", errno);
            printf("errMsg:%s", strerror(errno));
        }
        return 0;
    }
</code></pre>

<pre><code>    //testprocess.c

    #include &lt;stdio.h&gt;
    #include &lt;unistd.h&gt;
    #include &lt;sys/types.h&gt;
    #include &lt;sys/stat.h&gt;
    #include &lt;fcntl.h&gt;
    #include &lt;errno.h&gt;
    #include &lt;sys/file.h&gt;

    int main()
    {
        int fd,i;
        char path[] = "/home/taoyong/test.txt";
        char s[] = "writing.../nwriting....../n";
        extern int errno;
        fd = open(path, O_WRONLY|O_CREAT|O_APPEND);
        if(fd!=-1)
        {
            printf("open file %s ./n",path);

            if (flock(fd,LOCK_EX|LOCK_NB) == 0)
            {
                printf("the file was locked by the process./n");   
                if (-1 != write(fd,s,sizeof(s)))
                {
                    printf("write %s to the file %s/n", s, path);
                }
                else
                {
                    printf("cannot write the file %s/n", path);
                    printf("errno:%d/n", errno);
                    printf("errMsg:%s/n", strerror(errno));
                }       

            }
            else
            {
                printf("the file was locked by other process.Can't write.../n");
                printf("errno:%d:", errno);
            }
            close(fd);
        }
        else
        {
            printf("cannot open file %s/n", path);
            printf("errno:%d/n", errno);
            printf("errMsg:%s", strerror(errno));
        }
        return 0;
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[debuginfo 编译速度]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/26/debug-spec/"/>
    <updated>2015-11-26T11:30:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/26/debug-spec</id>
    <content type="html"><![CDATA[<p>Have a look into /usr/lib/rpm/macros:
<code>
    #       Compression type and level for source/binary package payloads.
    #               "w9.gzdio"      gzip level 9 (default).
    #               "w9.bzdio"      bzip2 level 9.
    #               "w7.xzdio"      xz level 7, xz's default.
    #               "w7.lzdio"      lzma-alone level 7, lzma's default
    #
    #%_source_payload       w9.gzdio
    #%_binary_payload       w9.gzdio
</code></p>

<p>binkernel.spec中加入
<code>
    %_source_payload       w5.gzdio
    %_binary_payload       w5.gzdio
</code>
略微降低压缩率，大大提高打包速度。kernel增加600K，debuginfo增加3M，时间从14分钟降至2分钟内</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[busybox]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/26/android-base-busybox/"/>
    <updated>2015-11-26T11:11:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/26/android-base-busybox</id>
    <content type="html"><![CDATA[<p>下载 <a href="http://www.busybox.net/downloads/binaries/latest/">http://www.busybox.net/downloads/binaries/latest/</a></p>

<p>或 <a href="/download/android/busybox-armv7l.tar.gz">busybox_armv7l</a></p>

<pre><code>    adb push ~/Download/busybox-armv7l /sdcard/busybox

    adb shell
    su
    mount -o remount,rw /system

    echo $PATH

    cp /sdcard/busybox /system/sbin
    chmod 755 busybox

    # 但是每次前面都加上个busybox太麻烦了，所以我们还要继续完成安装。
    # 在 /system/sbin 下输入
    busybox --install .
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CC_STACKPROTECTOR防内核堆栈溢出补丁分析]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/debug-CC_STACKPROTECTOR/"/>
    <updated>2015-11-17T16:01:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/debug-CC_STACKPROTECTOR</id>
    <content type="html"><![CDATA[<p><a href="http://blog.aliyun.com/1126">http://blog.aliyun.com/1126</a></p>

<p>内核堆栈溢出通常有两种情况。一种是函数调用栈超出了内核栈THREAD_SIZE的大小， 这是栈底越界，另一种是栈上缓冲越界访问，这是栈顶越界。</p>

<h4>检测栈底越界</h4>

<p>以arm平台为例，内核栈THREAD_SIZE为8K,当调用栈层次过多或某调用栈上分配过大的 空间，就会导致它越界。越界后struct thread_info结构可能被破坏，轻则内核 panic，重则内核数据被覆盖仍继续运行。</p>

<h4>检测栈顶越界</h4>

<p>对于栈顶越界，gcc提供了支持。打开内核配置CONFIG_CC_STACKPROTECTOR后，会打 开编译选项-fstack-protector.</p>

<hr />

<p>  CC_STACKPROTECT补丁是Tejun Heo在09年给主线kernel提交的一个用来防止内核堆栈溢出的补丁。默认的config是将这个选项关闭的，可以在编译内核的时候， 修改.config文件为CONFIG_CC_STACKPROTECTOR=y来启用。未来飞天内核可以将这个选项开启来防止利用内核stack溢出的0day攻击。这个补丁的防溢出原理是： 在进程启动的时候， 在每个buffer的后面放置一个预先设置好的stack canary，你可以把它理解成一个哨兵， 当buffer发生缓冲区溢出的时候， 肯定会破坏stack canary的值， 当stack canary的值被破坏的时候， 内核就会直接当机。那么是怎么判断stack canary被覆盖了呢？ 其实这个事情是gcc来做的，内核在编译的时候给gcc加了个-fstack-protector参数， 我们先来研究下这个参数是做什么用的。</p>

<p>先写个简单的有溢出的程序：</p>

<pre><code>    [wzt@localhost csaw]$ cat test.c

    #include &lt;stdio.h&gt;
    #include &lt;stdlib.h&gt;

    void test(void)
    {
        char buff[64];

        memset(buff, 0x41, 128);     //向64大小的buffer拷贝128字节， 肯定会发生缓冲区溢出。
    }

    int main(void)
    {
        test();

        return 0;
    }
</code></pre>

<pre><code>    [wzt@localhost csaw]$ gcc -o test test.c
    [wzt@localhost csaw]$ ./test
    段错误
</code></pre>

<p>反汇编看看：</p>

<pre><code>    [wzt@localhost csaw]$ objdump -d test &gt; hex

    08048384 &lt;test&gt;:
     8048384:       55                      push   %ebp
     8048385:       89 e5                   mov    %esp,%ebp
     8048387:       83 ec 58                sub    $0x58,%esp
     804838a:       c7 44 24 08 80 00 00    movl   $0x80,0x8(%esp)
     8048391:       00
     8048392:       c7 44 24 04 41 00 00    movl   $0x41,0x4(%esp)
     8048399:       00
     804839a:       8d 45 c0                lea    0xffffffc0(%ebp),%eax
     804839d:       89 04 24                mov    %eax,(%esp)
     80483a0:       e8 e3 fe ff ff          call   8048288 &lt;memset@plt&gt;
     80483a5:       c9                      leave
     80483a6:       c3                      ret
</code></pre>

<p>没什么特别的，我们在加上-fstack-protector参数看看：</p>

<pre><code>    [wzt@localhost csaw]$ gcc -o test test.c -fstack-protector
    [wzt@localhost csaw]$ ./test
    *** stack smashing detected ***: ./test terminated
    已放弃
</code></pre>

<p>这次程序打印了一条堆栈被溢出的信息，然后就自动退出了。</p>

<p>在反汇编看下：</p>

<pre><code>    [wzt@localhost csaw]$ objdump -d test &gt; hex1

    080483d4 &lt;test&gt;:
     80483d4:       55                      push   %ebp
     80483d5:       89 e5                   mov    %esp,%ebp
     80483d7:       83 ec 68                sub    $0x68,%esp
     80483da:       65 a1 14 00 00 00       mov    %gs:0x14,%eax
     80483e0:       89 45 fc                mov    %eax,0xfffffffc(%ebp)
     80483e3:       31 c0                   xor    %eax,%eax
     80483e5:       c7 44 24 08 80 00 00    movl   $0x80,0x8(%esp)
     80483ec:       00
     80483ed:       c7 44 24 04 41 00 00    movl   $0x41,0x4(%esp)
     80483f4:       00
     80483f5:       8d 45 bc                lea    0xffffffbc(%ebp),%eax
     80483f8:       89 04 24                mov    %eax,(%esp)
     80483fb:       e8 cc fe ff ff          call   80482cc &lt;memset@plt&gt;
     8048400:       8b 45 fc                mov    0xfffffffc(%ebp),%eax
     8048403:       65 33 05 14 00 00 00    xor    %gs:0x14,%eax
     804840a:       74 05                   je     8048411 &lt;test+0x3d&gt;
     804840c:       e8 db fe ff ff          call   80482ec &lt;__stack_chk_fail@plt&gt;
     8048411:       c9                      leave
     8048412:       c3                      ret
</code></pre>

<p>使用-fstack-protector参数后， gcc在函数的开头放置了几条汇编代码：</p>

<pre><code>     80483d7:       83 ec 68                sub    $0x68,%esp
     80483da:       65 a1 14 00 00 00       mov    %gs:0x14,%eax
     80483e0:       89 45 fc                mov    %eax,0xfffffffc(%ebp)
</code></pre>

<p>将代码段gs偏移0×14内存处的值赋值给了ebp-4， 也就是第一个变量值的后面。</p>

<p>在call完memeset后，有如下汇编代码：</p>

<pre><code>     80483fb:       e8 cc fe ff ff          call   80482cc &lt;memset@plt&gt;
     8048400:       8b 45 fc                mov    0xfffffffc(%ebp),%eax
     8048403:       65 33 05 14 00 00 00    xor    %gs:0x14,%eax
     804840a:       74 05                   je     8048411 &lt;test+0x3d&gt;
     804840c:       e8 db fe ff ff          call   80482ec &lt;__stack_chk_fail@plt&gt;
</code></pre>

<p>在memset后，gcc要检查这个操作是否发生了堆栈溢出, 将保存在ebp-4的这个值与原来的值对比一下，如果不相同， 说明堆栈发生了溢出，那么就会执行stack_chk_fail这个函数， 这个函数是glibc实现的，打印出上面看到的信息， 然后进程退出。</p>

<p>从这个例子中我们可以看出gcc使用了-fstack-protector参数后，会自动检查堆栈是否发生了溢出， 但是有一个前提就是内核要给每个进程提前设置好一个检测值放置在%gs:0×14位置处，这个值称之为stack canary。所以我们可以看到防止堆栈溢出是由内核和gcc共同来完成的。</p>

<p>gcc的任务就是放置几条汇编代码， 然后和%gs:0×14位置处的值进行对比即可。 主要任务还是内核如何来设置stack canary， 也是CC_STACKPROTECTOR补丁要实现的目的， 下面我们仔细来看下这个补丁是如何实现的。</p>

<p>既然gcc硬性规定了stack canary必须在%gs的某个偏移位置处， 那么内核也必须按着这个规定来设置。</p>

<p>对于32位和64位内核， gs寄存器有着不同的功能。</p>

<p>64位内核gcc要求stack canary是放置在gs段的40偏移处， 并且gs寄存器在每cpu变量中是共享的，每cpu变量irq_stack_union的结构如下：</p>

<pre><code>    arch/x86/include/asm/processor.h

    union irq_stack_union {
        char irq_stack[IRQ_STACK_SIZE];
        /*
         * GCC hardcodes the stack canary as %gs:40.  Since the
         * irq_stack is the object at %gs:0, we reserve the bottom
         * 48 bytes of the irq stack for the canary. 
         */
        struct {
            char gs_base[40];
            unsigned long stack_canary;
        };
    };

    DECLARE_PER_CPU_FIRST(union irq_stack_union, irq_stack_union);
</code></pre>

<p>gs_base只是一个40字节的站位空间， stack_canary就紧挨其后。并且在应用程序进出内核的时候，内核会使用swapgs指令自动更换gs寄存器的内容。</p>

<p>32位下就稍微有点复杂了。由于某些处理器在加载不同的段寄存器时很慢， 所以内核使用fs段寄存器替换了gs寄存器。 但是gcc在使用-fstack-protector的时候， 还要用到gs段寄存器， 所以内核还要管理gs寄存器，我们要把CONFIG_X86_32_LAZY_GS选项关闭， gs也只在进程切换的时候才改变。 32位用每cpu变量stack_canary保存stack canary。</p>

<pre><code>    struct stack_canary {
        char __pad[20];         /* canary at %gs:20 */
        unsigned long canary;
    };      
    DECLARE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
</code></pre>

<p>内核是处于保护模式的， 因此gs寄存器就变成了保护模式下的段选子，在GDT表中也要有相应的设置：</p>

<pre><code>    diff --git a/arch/x86/include/asm/segment.h b/arch/x86/include/asm/segment.h
    index 1dc1b51..14e0ed8 100644 (file)
    --- a/arch/x86/include/asm/segment.h
    +++ b/arch/x86/include/asm/segment.h
    @@ -61,7 +61,7 @@
      *
      *  26 - ESPFIX small SS
      *  27 - per-cpu                       [ offset to per-cpu data area ]
    - *  28 - unused
    + *  28 - stack_canary-20               [ for stack protector ]
      *  29 - unused
      *  30 - unused
      *  31 - TSS for double fault handler
    @@ -95,6 +95,13 @@
     #define __KERNEL_PERCPU 0
     #endif

    +#define GDT_ENTRY_STACK_CANARY         (GDT_ENTRY_KERNEL_BASE + 16)
    +#ifdef CONFIG_CC_STACKPROTECTOR
    +#define __KERNEL_STACK_CANARY          (GDT_ENTRY_STACK_CANARY * 8)
    +#else
    +#define __KERNEL_STACK_CANARY          0
    +#endif
    +
     #define GDT_ENTRY_DOUBLEFAULT_TSS      31
</code></pre>

<p>GDT表中的第28个表项用来定为stack canary所在的段。</p>

<pre><code>    #define GDT_STACK_CANARY_INIT                                           \
            [GDT_ENTRY_STACK_CANARY] = GDT_ENTRY_INIT(0x4090, 0, 0x18),
</code></pre>

<p>GDT_STACK_CANARY_INIT在刚进入保护模式的时候被调用， 这个段描述符项被设置为基地址为0， 段大小设为24，因为只在基地址为0， 偏移为0×14处放置一个4bytes的stack canary， 所以24字节正好。不理解的同学可以看看intel保护模式的手册， 对着段描述符结构一个个看就行了。</p>

<p>在进入保护模式后， start_kernel()会调用boot_init_stack_canary()来初始话一个stack canary。</p>

<pre><code>    /*      
     * Initialize the stackprotector canary value.
     *
     * NOTE: this must only be called from functions that never return,
     * and it must always be inlined.
     */
    static __always_inline void boot_init_stack_canary(void)
    {
        u64 canary;
        u64 tsc;

    #ifdef CONFIG_X86_64
        BUILD_BUG_ON(offsetof(union irq_stack_union, stack_canary) != 40);
    #endif
        /*
         * We both use the random pool and the current TSC as a source
         * of randomness. The TSC only matters for very early init,
         * there it already has some randomness on most systems. Later
         * on during the bootup the random pool has true entropy too.
         */
        get_random_bytes(&amp;canary, sizeof(canary));
        tsc = __native_read_tsc();
        canary += tsc + (tsc &lt;&lt; 32UL);

        current-&gt;stack_canary = canary;
    #ifdef CONFIG_X86_64
        percpu_write(irq_stack_union.stack_canary, canary);
    #else
        percpu_write(stack_canary.canary, canary);
    #endif
    }
</code></pre>

<p>随机出了一个值赋值给每cpu变量， 32位是stack_canary, 64位是irq_stack_union。</p>

<p>内核在进一步初始化cpu的时候，会调用setup_stack_canary_segment()来设置每个cpu的GDT的stack canary描述符项：</p>

<p>start_kernel()->setup_per_cpu_areas()->setup_stack_canary_segment：</p>

<pre><code>    static inline void setup_stack_canary_segment(int cpu)
    {
    #ifdef CONFIG_X86_32
        unsigned long canary = (unsigned long)&amp;per_cpu(stack_canary, cpu);
        struct desc_struct *gdt_table = get_cpu_gdt_table(cpu);
        struct desc_struct desc;

        desc = gdt_table[GDT_ENTRY_STACK_CANARY];
        set_desc_base(&amp;desc, canary);
        write_gdt_entry(gdt_table, GDT_ENTRY_STACK_CANARY, &amp;desc, DESCTYPE_S);
    #endif
    }
</code></pre>

<p>在内核刚进入保护模式的时候, stack canary描述符的基地址被初始化为0， 现在在cpu初始化的时候要重新设置为每cpu变量stack_canary的地址， 而不是变量保存的值。通过这些设置当内核代码在访问%gs:0×14的时候， 就会访问stack canry保存的值。注意：setup_stack_canary_segment是针对32位内核做设置， 因为64位内核中的irq_stack_union是每cpu共享的， 不用针对每个cpu单独设置。 然后就可以调用switch_to_new_gdt(cpu);来加载GDT表和加载gs寄存器。</p>

<p>经过上述初始化过程，在内核代码里访问%gs:0×14就可以定位stack canary的值了， 那么每个进程的stack canary是什么时候设置的呢？</p>

<p>在内核启动一个进程的时候， 会把gs寄存器的值设为KERNEL_STACK_CANARY</p>

<pre><code>    --- a/arch/x86/kernel/process_32.c
    +++ b/arch/x86/kernel/process_32.c
    @@ -212,6 +212,7 @@ int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
            regs.ds = __USER_DS;
            regs.es = __USER_DS;
            regs.fs = __KERNEL_PERCPU;
    +       regs.gs = __KERNEL_STACK_CANARY;
            regs.orig_ax = -1;
            regs.ip = (unsigned long) kernel_thread_helper;
            regs.cs = __KERNEL_CS | get_kernel_rpl();
</code></pre>

<p>内核在fork一个进程的时候， 有如下操作：</p>

<pre><code>    static struct task_struct *dup_task_struct(struct task_struct *orig)
    {
    #ifdef CONFIG_CC_STACKPROTECTOR
        tsk-&gt;stack_canary = get_random_int();
    #endif
    }
</code></pre>

<p>随机初始化了一个stack_canary保存在task_struct结构中的stack_canary变量中。当进程在切换的时候， 通过switch宏把新进程的stack canary保存在每cpu变量stack_canary中， 当前进程的stack_canary也保存在一个每cpu变量中，完成stack canary的切换。</p>

<pre><code>    diff --git a/arch/x86/include/asm/system.h b/arch/x86/include/asm/system.h
    index 79b98e5..2692ee8 100644 (file)
    --- a/arch/x86/include/asm/system.h
    +++ b/arch/x86/include/asm/system.h
    @@ -23,6 +23,22 @@ struct task_struct *__switch_to(struct task_struct *prev,

     #ifdef CONFIG_X86_32

    +#ifdef CONFIG_CC_STACKPROTECTOR
    +#define __switch_canary                                                \
    +       "movl "__percpu_arg([current_task])",%%ebx\n\t"                 \
    +       "movl %P[task_canary](%%ebx),%%ebx\n\t"                         \
    +       "movl %%ebx,"__percpu_arg([stack_canary])"\n\t"
    +#define __switch_canary_oparam                                         \
    +       , [stack_canary] "=m" (per_cpu_var(stack_canary))
    +#define __switch_canary_iparam                                         \
    +       , [current_task] "m" (per_cpu_var(current_task))                \
    +       , [task_canary] "i" (offsetof(struct task_struct, stack_canary))
    +#else  /* CC_STACKPROTECTOR */
    +#define __switch_canary
    +#define __switch_canary_oparam
    +#define __switch_canary_iparam
    +#endif /* CC_STACKPROTECTOR */
    +
     /*
      * Saving eflags is important. It switches not only IOPL between tasks,
      * it also protects other tasks from NT leaking through sysenter etc.
    @@ -46,6 +62,7 @@ do {                                                  \
                         "pushl %[next_ip]\n\t"     /* restore EIP   */     \
                         "jmp __switch_to\n"        /* regparm call  */     \
                         "1:\t"                                             \
    +                    __switch_canary                                    \
                         "popl %%ebp\n\t"           /* restore EBP   */     \
                         "popfl\n"                  /* restore flags */     \
                                                                            \
    @@ -58,6 +75,8 @@ do {                                                  \
                           "=b" (ebx), "=c" (ecx), "=d" (edx),              \
                           "=S" (esi), "=D" (edi)                           \
                                                                            \
    +                      __switch_canary_oparam                           \
    +                                                                       \
                           /* input parameters: */                          \
                         : [next_sp]  "m" (next-&gt;thread.sp),                \
                           [next_ip]  "m" (next-&gt;thread.ip),                \
    @@ -66,6 +85,8 @@ do {                                                  \
                           [prev]     "a" (prev),                           \
                           [next]     "d" (next)                            \
                                                                            \
    +                      __switch_canary_iparam                           \
    +                                                                       \
                         : /* reloaded segment registers */                 \
                            "memory");                                      \
     } while (0)
</code></pre>

<p>前面讲过当gcc检测到堆栈溢出的时候， 会调用glibc的stack_chk_fail函数， 但是当内核堆栈发生溢出的时候，不能调用glibc的函数，所以内核自己实现了一个stack_chk_fail函数：</p>

<p>kernel/panic.c</p>

<pre><code>    #ifdef CONFIG_CC_STACKPROTECTOR

    /*
     * Called when gcc's -fstack-protector feature is used, and
     * gcc detects corruption of the on-stack canary value
     */
    void __stack_chk_fail(void)
    {
        panic("stack-protector: Kernel stack is corrupted in: %p\n",
             __builtin_return_address(0));
    }
    EXPORT_SYMBOL(__stack_chk_fail);

    #endif
</code></pre>

<p>当内核堆栈发生溢出的时候，就会执行stack_chk_fail函数， 内核当机。</p>

<p>这就是这个补丁的原理，不懂的同学请参考：</p>

<p><a href="http://git.kernel.org/?p=linux/kernel/git/next/linux-next.git;a=commitdiff;h=60a5317ff0f42dd313094b88f809f63041568b08">http://git.kernel.org/?p=linux/kernel/git/next/linux-next.git;a=commitdiff;h=60a5317ff0f42dd313094b88f809f63041568b08</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ixgbe]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-ixgbe/"/>
    <updated>2015-11-17T15:16:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-ixgbe</id>
    <content type="html"><![CDATA[<p><a href="http://www.pagefault.info/?p=403">http://www.pagefault.info/?p=403</a></p>

<p>这里分析的驱动代码是给予linux kernel 3.4.4</p>

<p>对应的文件在drivers/net/ethernet/intel 目录下，这个分析不涉及到很细节的地方，主要目的是理解下数据在协议栈和驱动之间是如何交互的。</p>

<p>首先我们知道网卡都是pci设备，因此这里每个网卡驱动其实就是一个pci驱动。并且intel这里是把好几个万兆网卡(82599/82598/x540)的驱动做在一起的。</p>

<p>首先我们来看对应的pci_driver的结构体，这里每个pci驱动都是一个pci_driver的结构体，而这里是多个万兆网卡共用这个结构体ixgbe_driver.</p>

<pre><code>    static struct pci_driver ixgbe_driver = {
        .name     = ixgbe_driver_name,
        .id_table = ixgbe_pci_tbl,
        .probe    = ixgbe_probe,
        .remove   = __devexit_p(ixgbe_remove),
    #ifdef CONFIG_PM
        .suspend  = ixgbe_suspend,
        .resume   = ixgbe_resume,
    #endif
        .shutdown = ixgbe_shutdown,
        .err_handler = &amp;ixgbe_err_handler
    };
</code></pre>

<p>然后是模块初始化方法,这里其实很简单，就是调用pci的驱动注册方法，把ixgbe挂载到pci设备链中。 这里不对pci设备的初始化做太多介绍，我以前的blog有这方面的介绍，想了解的可以去看看。这里我们只需要知道最终内核会调用probe回调来初始化ixgbe。</p>

<pre><code>    char ixgbe_driver_name[] = "ixgbe";
    static const char ixgbe_driver_string[] =
                    "Intel(R) 10 Gigabit PCI Express Network Driver";

    static int __init ixgbe_init_module(void)
    {
        int ret;
        pr_info("%s - version %s\n", ixgbe_driver_string, ixgbe_driver_version);
        pr_info("%s\n", ixgbe_copyright);

    #ifdef CONFIG_IXGBE_DCA
        dca_register_notify(&amp;dca_notifier);
    #endif

        ret = pci_register_driver(&amp;ixgbe_driver);
        return ret;
    }
</code></pre>

<p>这里不去追究具体如何调用probe的细节，我们直接来看probe函数，这个函数中通过硬件的信息来确定需要初始化那个驱动(82598/82599/x540),然后核心的驱动结构就放在下面的这个数组中。</p>

<pre><code>    static const struct ixgbe_info *ixgbe_info_tbl[] = {
        [board_82598] = &amp;ixgbe_82598_info,
        [board_82599] = &amp;ixgbe_82599_info,
        [board_X540] = &amp;ixgbe_X540_info,
    };
</code></pre>

<p>ixgbe_probe函数很长，我们这里就不详细分析了，因为这部分就是对网卡进行初始化。不过我们关注下面几个代码片段。</p>

<p>首先是根据硬件的参数来取得对应的驱动值:</p>

<pre><code>    const struct ixgbe_info *ii = ixgbe_info_tbl[ent-&gt;driver_data];
</code></pre>

<p>然后就是如何将不同的网卡驱动挂载到对应的回调中，这里做的很简单，就是通过对应的netdev的结构取得adapter，然后所有的核心操作都是保存在adapter中的，最后将ii的所有回调拷贝给adapter就可以了。我们来看代码：</p>

<pre><code>        struct net_device *netdev;
        struct ixgbe_adapter *adapter = NULL;
        struct ixgbe_hw *hw;
        .....................................

        adapter = netdev_priv(netdev);
        pci_set_drvdata(pdev, adapter);

        adapter-&gt;netdev = netdev;
        adapter-&gt;pdev = pdev;
        hw = &amp;adapter-&gt;hw;
        hw-&gt;back = adapter;
        .......................................
        memcpy(&amp;hw-&gt;mac.ops, ii-&gt;mac_ops, sizeof(hw-&gt;mac.ops));
        hw-&gt;mac.type  = ii-&gt;mac;

        /* EEPROM */
        memcpy(&amp;hw-&gt;eeprom.ops, ii-&gt;eeprom_ops, sizeof(hw-&gt;eeprom.ops));
        .....................................
</code></pre>

<p>最后需要关注的就是设置网卡属性，这些属性一般来说都是通过ethtool 可以设置的属性(比如tso/checksum等),这里我们就截取一部分:</p>

<pre><code>        netdev-&gt;features = NETIF_F_SG |
                   NETIF_F_IP_CSUM |
                   NETIF_F_IPV6_CSUM |
                   NETIF_F_HW_VLAN_TX |
                   NETIF_F_HW_VLAN_RX |
                   NETIF_F_HW_VLAN_FILTER |
                   NETIF_F_TSO |
                   NETIF_F_TSO6 |
                   NETIF_F_RXHASH |
                   NETIF_F_RXCSUM;

        netdev-&gt;hw_features = netdev-&gt;features;

        switch (adapter-&gt;hw.mac.type) {
        case ixgbe_mac_82599EB:
        case ixgbe_mac_X540:
            netdev-&gt;features |= NETIF_F_SCTP_CSUM;
            netdev-&gt;hw_features |= NETIF_F_SCTP_CSUM |
                           NETIF_F_NTUPLE;
            break;
        default:
            break;
        }

        netdev-&gt;hw_features |= NETIF_F_RXALL;
        ..................................................

        netdev-&gt;priv_flags |= IFF_UNICAST_FLT;
        netdev-&gt;priv_flags |= IFF_SUPP_NOFCS;

        if (adapter-&gt;flags &amp; IXGBE_FLAG_SRIOV_ENABLED)
            adapter-&gt;flags &amp;= ~(IXGBE_FLAG_RSS_ENABLED |
                        IXGBE_FLAG_DCB_ENABLED);
        ...................................................................
        if (pci_using_dac) {
            netdev-&gt;features |= NETIF_F_HIGHDMA;
            netdev-&gt;vlan_features |= NETIF_F_HIGHDMA;
        }

        if (adapter-&gt;flags2 &amp; IXGBE_FLAG2_RSC_CAPABLE)
            netdev-&gt;hw_features |= NETIF_F_LRO;
        if (adapter-&gt;flags2 &amp; IXGBE_FLAG2_RSC_ENABLED)
            netdev-&gt;features |= NETIF_F_LRO;
</code></pre>

<p>然后我们来看下中断的注册，因为万兆网卡大部分都是多对列网卡(配合msix)，因此对于上层软件来说，就好像有多个网卡一样，它们之间的数据是相互独立的，这里读的话主要是napi驱动的poll方法，后面我们会分析这个.</p>

<p>到了这里或许要问那么网卡是如何挂载回调给上层，从而上层来发送数据呢，这里是这样子的，每个网络设备都有一个回调函数表(比如ndo_start_xmit)来供上层调用，而在ixgbe中的话，就是ixgbe_netdev_ops，下面就是这个结构，不过只是截取了我们很感兴趣的几个地方.</p>

<p>不过这里注意，读回调并不在里面，这是因为写是软件主动的，而读则是硬件主动的。现在ixgbe是NAPI的，因此它的poll回调是ixgbe_poll，是中断注册时候通过netif_napi_add添加进去的。</p>

<pre><code>    static const struct net_device_ops ixgbe_netdev_ops = {
        .ndo_open       = ixgbe_open,
        .ndo_stop       = ixgbe_close,
        .ndo_start_xmit     = ixgbe_xmit_frame,
        .ndo_select_queue   = ixgbe_select_queue,
        .ndo_set_rx_mode    = ixgbe_set_rx_mode,
        .ndo_validate_addr  = eth_validate_addr,
        .ndo_set_mac_address    = ixgbe_set_mac,
        .ndo_change_mtu     = ixgbe_change_mtu,
        .ndo_tx_timeout     = ixgbe_tx_timeout,
        .................................................
        .ndo_set_features = ixgbe_set_features,
        .ndo_fix_features = ixgbe_fix_features,
    };
</code></pre>

<p>这里我们最关注的其实就是ndo_start_xmit回调，这个回调就是驱动提供给协议栈的发送回调接口。我们来看这个函数.</p>

<p>它的实现很简单，就是选取对应的队列，然后调用ixgbe_xmit_frame_ring来发送数据。</p>

<pre><code>    static netdev_tx_t ixgbe_xmit_frame(struct sk_buff *skb,
                        struct net_device *netdev)
    {
        struct ixgbe_adapter *adapter = netdev_priv(netdev);
        struct ixgbe_ring *tx_ring;

        if (skb-&gt;len &lt;= 0) {
            dev_kfree_skb_any(skb);
            return NETDEV_TX_OK;
        }

        /*
         * The minimum packet size for olinfo paylen is 17 so pad the skb
         * in order to meet this minimum size requirement.
         */
        if (skb-&gt;len &lt; 17) {
            if (skb_padto(skb, 17))
                return NETDEV_TX_OK;
            skb-&gt;len = 17;
        }
        //取得对应的队列
        tx_ring = adapter-&gt;tx_ring[skb-&gt;queue_mapping];
        //发送数据
        return ixgbe_xmit_frame_ring(skb, adapter, tx_ring);
    }
</code></pre>

<p>而在ixgbe_xmit_frame_ring中，我们就关注两个地方，一个是tso(什么是TSO，请自行google)，一个是如何发送.</p>

<pre><code>        tso = ixgbe_tso(tx_ring, first, &amp;hdr_len);
        if (tso &lt; 0)
            goto out_drop;
        else if (!tso)
            ixgbe_tx_csum(tx_ring, first);

        /* add the ATR filter if ATR is on */
        if (test_bit(__IXGBE_TX_FDIR_INIT_DONE, &amp;tx_ring-&gt;state))
            ixgbe_atr(tx_ring, first);

    #ifdef IXGBE_FCOE
    xmit_fcoe:
    #endif /* IXGBE_FCOE */
        ixgbe_tx_map(tx_ring, first, hdr_len);
</code></pre>

<p>调用ixgbe_tso处理完tso之后，就会调用ixgbe_tx_map来发送数据。而ixgbe_tx_map所做的最主要是两步，第一步请求DMA，第二步写寄存器，通知网卡发送数据.</p>

<pre><code>        dma = dma_map_single(tx_ring-&gt;dev, skb-&gt;data, size, DMA_TO_DEVICE);
        if (dma_mapping_error(tx_ring-&gt;dev, dma))
            goto dma_error;

        /* record length, and DMA address */
        dma_unmap_len_set(first, len, size);
        dma_unmap_addr_set(first, dma, dma);

        tx_desc-&gt;read.buffer_addr = cpu_to_le64(dma);

        for (;;) {
            while (unlikely(size &gt; IXGBE_MAX_DATA_PER_TXD)) {
                tx_desc-&gt;read.cmd_type_len =
                    cmd_type | cpu_to_le32(IXGBE_MAX_DATA_PER_TXD);

                i++;
                tx_desc++;
                if (i == tx_ring-&gt;count) {
                    tx_desc = IXGBE_TX_DESC(tx_ring, 0);
                    i = 0;
                }

                dma += IXGBE_MAX_DATA_PER_TXD;
                size -= IXGBE_MAX_DATA_PER_TXD;

                tx_desc-&gt;read.buffer_addr = cpu_to_le64(dma);
                tx_desc-&gt;read.olinfo_status = 0;
            }

            ...................................................
            data_len -= size;

            dma = skb_frag_dma_map(tx_ring-&gt;dev, frag, 0, size,
                           DMA_TO_DEVICE);
            ..........................................................

            frag++;
        }
        .................................
        tx_ring-&gt;next_to_use = i;

        /* notify HW of packet */
        writel(i, tx_ring-&gt;tail);
        .................
</code></pre>

<p>上面的操作是异步的，也就是说此时内核还不能释放SKB，而是网卡硬件发送完数据之后，会再次产生中断通知内核，然后内核才能释放内存.接下来我们来看这部分代码。</p>

<p>首先来看的是中断注册的代码，这里我们假设启用了MSIX,那么网卡的中断注册回调就是ixgbe_request_msix_irqs函数，这里我们可以看到调用request_irq函数来注册回调，并且每个队列都有自己的中断号。</p>

<pre><code>    static int ixgbe_request_msix_irqs(struct ixgbe_adapter *adapter)
    {
        struct net_device *netdev = adapter-&gt;netdev;
        int q_vectors = adapter-&gt;num_msix_vectors - NON_Q_VECTORS;
        int vector, err;
        int ri = 0, ti = 0;

        for (vector = 0; vector &lt; q_vectors; vector++) {
            struct ixgbe_q_vector *q_vector = adapter-&gt;q_vector[vector];
            struct msix_entry *entry = &amp;adapter-&gt;msix_entries[vector];
            .......................................................................
            err = request_irq(entry-&gt;vector, &amp;ixgbe_msix_clean_rings, 0,
                      q_vector-&gt;name, q_vector);
            if (err) {
                e_err(probe, "request_irq failed for MSIX interrupt "
                      "Error: %d\n", err);
                goto free_queue_irqs;
            }
            /* If Flow Director is enabled, set interrupt affinity */
            if (adapter-&gt;flags &amp; IXGBE_FLAG_FDIR_HASH_CAPABLE) {
                /* assign the mask for this irq */
                irq_set_affinity_hint(entry-&gt;vector,
                              &amp;q_vector-&gt;affinity_mask);
            }
        }

        ..............................................

        return 0;

    free_queue_irqs:
        ...............................
        return err;
    }
</code></pre>

<p>而对应的中断回调是ixgbe_msix_clean_rings,而这个函数呢，做的事情很简单(需要熟悉NAPI的原理，我以前的blog有介绍),就是调用napi_schedule来重新加入软中断处理.</p>

<pre><code>    static irqreturn_t ixgbe_msix_clean_rings(int irq, void *data)
    {
        struct ixgbe_q_vector *q_vector = data;

        /* EIAM disabled interrupts (on this vector) for us */

        if (q_vector-&gt;rx.ring || q_vector-&gt;tx.ring)
            napi_schedule(&amp;q_vector-&gt;napi);

        return IRQ_HANDLED;
    }
</code></pre>

<p>而NAPI驱动我们知道，最终是会调用网卡驱动挂载的poll回调，在ixgbe中，对应的回调就是ixgbe_poll，那么也就是说这个函数要做两个工作，一个是处理读，一个是处理写完之后的清理.</p>

<pre><code>    int ixgbe_poll(struct napi_struct *napi, int budget)
    {
        struct ixgbe_q_vector *q_vector =
                    container_of(napi, struct ixgbe_q_vector, napi);
        struct ixgbe_adapter *adapter = q_vector-&gt;adapter;
        struct ixgbe_ring *ring;
        int per_ring_budget;
        bool clean_complete = true;

    #ifdef CONFIG_IXGBE_DCA
        if (adapter-&gt;flags &amp; IXGBE_FLAG_DCA_ENABLED)
            ixgbe_update_dca(q_vector);
    #endif
        //清理写
        ixgbe_for_each_ring(ring, q_vector-&gt;tx)
            clean_complete &amp;= !!ixgbe_clean_tx_irq(q_vector, ring);

        /* attempt to distribute budget to each queue fairly, but don't allow
         * the budget to go below 1 because we'll exit polling */
        if (q_vector-&gt;rx.count &gt; 1)
            per_ring_budget = max(budget/q_vector-&gt;rx.count, 1);
        else
            per_ring_budget = budget;
        //读数据，并清理已完成的
        ixgbe_for_each_ring(ring, q_vector-&gt;rx)
            clean_complete &amp;= ixgbe_clean_rx_irq(q_vector, ring,
                                 per_ring_budget);

        /* If all work not completed, return budget and keep polling */
        if (!clean_complete)
            return budget;

        /* all work done, exit the polling mode */
        napi_complete(napi);
        if (adapter-&gt;rx_itr_setting &amp; 1)
            ixgbe_set_itr(q_vector);
        if (!test_bit(__IXGBE_DOWN, &amp;adapter-&gt;state))
            ixgbe_irq_enable_queues(adapter, ((u64)1 &lt;&lt; q_vector-&gt;v_idx));

        return 0;
    }
</code></pre>
]]></content>
  </entry>
  
</feed>
