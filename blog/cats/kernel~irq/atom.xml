<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel~irq | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel~irq/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2016-05-21T10:54:19+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[linux 中断下半部]]></title>
    <link href="http://abcdxyzk.github.io/blog/2016/05/21/kernel-irq-bh/"/>
    <updated>2016-05-21T10:40:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2016/05/21/kernel-irq-bh</id>
    <content type="html"><![CDATA[<p><a href="http://blog.chinaunix.net/uid-24203478-id-3111803.html">http://blog.chinaunix.net/uid-24203478-id-3111803.html</a></p>

<p>与Linux中断息息相关的一个重要概念是Linux中断分为两个半部：上半部（tophalf）和下半部(bottom half)。上半部的功能是"登记中断"，当一个中断发生时，它进行相应地硬件读写后就把中断例程的下半部挂到该设备的下半部执行队列中去。因此，上半部执行的速度就会很快，可以服务更多的中断请求。但是，仅有"登记中断"是远远不够的，因为中断的事件可能很复杂。因此，Linux引入了一个下半部，来完成中断事件的绝大多数使命。下半部和上半部最大的不同是下半部是可中断的，而上半部是不可中断的，下半部几乎做了中断处理程序所有的事情，而且可以被新的中断打断！下半部则相对来说并不是非常紧急的，通常还是比较耗时的，因此由系统自行安排运行时机，不在中断服务上下文中执行。</p>

<h4>在Linux2.6的内核中存在三种不同形式的下半部实现机制：软中断，tasklet和工作队列。</h4>

<p>Tasklet基于Linux softirq，其使用相当简单，我们只需要定义tasklet及其处理函数并将二者关联：</p>

<pre><code>    void my_tasklet_func(unsigned long); //定义一个处理函数：
    DECLARE_TASKLET(my_tasklet,my_tasklet_func,data); //定义一个tasklet结构my_tasklet，与my_tasklet_func(data)函数相关联
</code></pre>

<p>然后，在需要调度tasklet的时候引用一个简单的API就能使系统在适当的时候进行调度运行：
<code>
    tasklet_schedule(&amp;my_tasklet);
</code></p>

<p>此外，Linux还提供了另外一些其它的控制tasklet调度与运行的API：
<code>
    DECLARE_TASKLET_DISABLED(name,function,data); //与DECLARE_TASKLET类似，但等待tasklet被使能
    tasklet_enable(struct tasklet_struct *); //使能tasklet
    tasklet_disble(struct tasklet_struct *); //禁用tasklet
    tasklet_init(struct tasklet_struct *,void (*func)(unsigned long),unsigned long); //类似DECLARE_TASKLET()
    tasklet_kill(struct tasklet_struct *); // 清除指定tasklet的可调度位，即不允许调度该tasklet
</code></p>

<p>我们先来看一个tasklet的运行实例，这个实例没有任何实际意义，仅仅为了演示。它的功能是：在globalvar被写入一次后，就调度一个tasklet，函数中输出"tasklet is executing"：</p>

<pre><code>    //定义与绑定tasklet函数
    void test_tasklet_action(unsigned long t);
    DECLARE_TASKLET(test_tasklet, test_tasklet_action, 0);

    void test_tasklet_action(unsigned long t)
    {
        printk("tasklet is executing\n");
    }

    ...

    ssize_t globalvar_write(struct file *filp, const char *buf, size_t len, loff_t *off)
    {
        ...
        if (copy_from_user(&amp;global_var, buf, sizeof(int)))
        {
            return - EFAULT;
        }

        //调度tasklet执行
        tasklet_schedule(&amp;test_tasklet);
        return sizeof(int);
    }
</code></pre>

<p>下半部分的任务就是执行与中断处理密切相关但中断处理程序本身不执行的工作。</p>

<h4>在Linux2.6的内核中存在三种不同形式的下半部实现机制：软中断，tasklet和工作队列。</h4>

<p>下面将比较三种机制的差别与联系。
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>软中断:    1、软中断是在编译期间静态分配的。
</span><span class='line'>           2、最多可以有32个软中断。
</span><span class='line'>           3、软中断不会抢占另外一个软中断，唯一可以抢占软中断的是中断处理程序。
</span><span class='line'>           4、可以并发运行在多个CPU上（即使同一类型的也可以）。所以软中断必须设计为可重入的函数（允许多个CPU同时操作），
</span><span class='line'>              因此也需要使用自旋锁来保护其数据结构。
</span><span class='line'>           5、目前只有两个子系直接使用软中断：网络和SCSI。
</span><span class='line'>           6、执行时间有：从硬件中断代码返回时、在ksoftirqd内核线程中和某些显示检查并执行软中断的代码中。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;tasklet:   1、tasklet是使用两类软中断实现的：HI_SOFTIRQ和TASKLET_SOFTIRQ。
</span><span class='line'>       2、可以动态增加减少，没有数量限制。
</span><span class='line'>       3、同一类tasklet不能并发执行。
</span><span class='line'>       4、不同类型可以并发执行。
</span><span class='line'>       5、大部分情况使用tasklet。
</span><span class='line'>
</span><span class='line'>工作队列:  1、由内核线程去执行，换句话说总在进程上下文执行。
</span><span class='line'>       2、可以睡眠，阻塞。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;```&lt;/p&gt;
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[中断子系统之（八）：softirq]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2015/05/07/kernel-irq-softirq/"/&gt;
</span><span class='line'>&lt;updated&gt;2015-05-07T16:04:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2015/05/07/kernel-irq-softirq&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;p&gt;&lt;a href="http://www.wowotech.net/linux_kenrel/soft-irq.html"&gt;http://www.wowotech.net/linux_kenrel/soft-irq.html&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;一、前言&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;对于中断处理而言，linux将其分成了两个部分，一个叫做中断handler（top half），是全程关闭中断的，另外一部分是deferable task（bottom half），属于不那么紧急需要处理的事情。在执行bottom half的时候，是开中断的。有多种bottom half的机制，例如：softirq、tasklet、workqueue或是直接创建一个kernel thread来执行bottom half（这在旧的kernel驱动中常见，现在，一个理智的driver厂商是不会这么做的）。本文主要讨论softirq机制。由于tasklet是基于softirq的，因此本文也会提及tasklet，但主要是从需求层面考虑，不会涉及其具体的代码实现。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;在普通的驱动中一般是不会用到softirq，但是由于驱动经常使用的tasklet是基于softirq的，因此，了解softirq机制有助于撰写更优雅的driver。softirq不能动态分配，都是静态定义的。内核已经定义了若干种softirq number，例如网络数据的收发、block设备的数据访问（数据量大，通信带宽高），timer的deferable task（时间方面要求高）。本文的第二章讨论了softirq和tasklet这两种机制有何不同，分别适用于什么样的场景。第三章描述了一些context的概念，这是要理解后续内容的基础。第四章是进入softirq的实现，对比hard irq来解析soft irq的注册、触发，调度的过程。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;ul&gt;
</span><span class='line'>&lt;li&gt;注：本文中的linux kernel的版本是3.14&lt;/li&gt;
</span><span class='line'>&lt;/ul&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;二、为何有softirq和tasklet&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;1、为何有top half和bottom half&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;中断处理模块是任何OS中最重要的一个模块，对系统的性能会有直接的影响。想像一下：如果在通过U盘进行大量数据拷贝的时候，你按下一个key，需要半秒的时间才显示出来，这个场景是否让你崩溃？因此，对于那些复杂的、需要大量数据处理的硬件中断，我们不能让handler中处理完一切再恢复现场（handler是全程关闭中断的），而是仅仅在handler中处理一部分，具体包括：&lt;br/&gt;
</span><span class='line'>（1）有实时性要求的&lt;br/&gt;
</span><span class='line'>（2）和硬件相关的。例如ack中断，read HW FIFO to ram等&lt;br/&gt;
</span><span class='line'>（3）如果是共享中断，那么获取硬件中断状态以便判断是否是本中断发生&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;除此之外，其他的内容都是放到bottom half中处理。在把中断处理过程划分成top half和bottom half之后，关中断的top half被瘦身，可以非常快速的执行完毕，大大减少了系统关中断的时间，提高了系统的性能。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;我们可以基于下面的系统进一步的进行讨论：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-05-07-11.gif" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;当网卡控制器的FIFO收到的来自以太网的数据的时候（例如半满的时候，可以软件设定），可以将该事件通过irq signal送达Interrupt Controller。Interrupt Controller可以把中断分发给系统中的Processor A or B。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;NIC的中断处理过程大概包括：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>mask and ack interrupt controller---&gt;ack NIC---&gt;copy FIFO to ram---&gt;handle Data in the ram---&gt;unmask interrupt controller
</span><span class='line'>&lt;/code&gt;
</span><span class='line'>我们先假设Processor A处理了这个网卡中断事件，于是NIC的中断handler在Processor A上欢快的执行，这时候，Processor A的本地中断是disable的。NIC的中断handler在执行的过程中，网络数据仍然源源不断的到来，但是，如果NIC的中断handler不操作NIC的寄存器来ack这个中断的话，NIC是不会触发下一次中断的。还好，我们的NIC interrupt handler总是在最开始就会ack，因此，这不会导致性能问题。ack之后，NIC已经具体再次trigger中断的能力。当Processor A上的handler 在处理接收来自网络的数据的时候，NIC的FIFO很可能又收到新的数据，并trigger了中断，这时候，Interrupt controller还没有umask，因此，即便还有Processor B（也就是说有处理器资源），中断控制器也无法把这个中断送达处理器系统。因此，只能眼睁睁的看着NIC FIFO填满数据，数据溢出，或者向对端发出拥塞信号，无论如何，整体的系统性能是受到严重的影响。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;注意：对于新的interrupt controller，可能没有mask和umask操作，但是原理是一样的，只不过NIC的handler执行完毕要发生EOI而已。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;要解决上面的问题，最重要的是尽快的执行完中断handler，打开中断，unmask IRQ（或者发送EOI），方法就是把耗时的handle Data in the ram这个步骤踢出handler，让其在bottom half中执行。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;2、为何有softirq和tasklet&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;OK，linux kernel已经把中断处理分成了top half和bottom half，看起来已经不错了，那为何还要提供softirq、tasklet和workqueue这些bottom half机制，linux kernel本来就够复杂了，bottom half还来添乱。实际上，在早期的linux kernel还真是只有一个bottom half机制，简称BH，简单好用，但是性能不佳。后来，linux kernel的开发者开发了task queue机制，试图来替代BH，当然，最后task queue也消失在内核代码中了。现在的linux kernel提供了三种bottom half的机制，来应对不同的需求。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;workqueue和softirq、tasklet有本质的区别：workqueue运行在process context，而softirq和tasklet运行在interrupt context。因此，出现workqueue是不奇怪的，在有sleep需求的场景中，defering task必须延迟到kernel thread中执行，也就是说必须使用workqueue机制。softirq和tasklet是怎么回事呢？从本质上将，bottom half机制的设计有两方面的需求，一个是性能，一个是易用性。设计一个通用的bottom half机制来满足这两个需求非常的困难，因此，内核提供了softirq和tasklet两种机制。softirq更倾向于性能，而tasklet更倾向于易用性。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;我们还是进入实际的例子吧，还是使用上一节的系统图。在引入softirq之后，网络数据的处理如下：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;关中断：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>mask and ack interrupt controller---&gt;ack NIC---&gt;copy FIFO to ram---&gt;raise softirq---&gt;unmask interrupt controller
</span><span class='line'>&lt;/code&gt;
</span><span class='line'>开中断：在softirq上下文中进行handle Data in the ram的动作&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;同样的，我们先假设Processor A处理了这个网卡中断事件，很快的完成了基本的HW操作后，raise softirq。在返回中断现场前，会检查softirq的触发情况，因此，后续网络数据处理的softirq在processor A上执行。在执行过程中，NIC硬件再次触发中断，Interrupt controller将该中断分发给processor B，执行动作和Processor A是类似的，因此，最后，网络数据处理的softirq在processor B上执行。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;为了性能，同一类型的softirq有可能在不同的CPU上并发执行，这给使用者带来了极大的痛苦，因为驱动工程师在撰写softirq的回调函数的时候要考虑重入，考虑并发，要引入同步机制。但是，为了性能，我们必须如此。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;当网络数据处理的softirq同时在Processor A和B上运行的时候，网卡中断又来了（可能是10G的网卡吧）。这时候，中断分发给processor A，这时候，processor A上的handler仍然会raise softirq，但是并不会调度该softirq。也就是说，softirq在一个CPU上是串行执行的。这种情况下，系统性能瓶颈是CPU资源，需要增加更多的CPU来解决该问题。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;如果是tasklet的情况会如何呢？为何tasklet性能不如softirq呢？如果一个tasklet在processor A上被调度执行，那么它永远也不会同时在processor B上执行，也就是说，tasklet是串行执行的（注：不同的tasklet还是会并发的），不需要考虑重入的问题。我们还是用网卡这个例子吧（注意：这个例子仅仅是用来对比，实际上，网络数据是使用softirq机制的），同样是上面的系统结构图。假设使用tasklet，网络数据的处理如下：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;关中断：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>mask and ack interrupt controller---&gt;ack NIC---&gt;copy FIFO to ram---&gt;schedule tasklet---&gt;unmask interrupt controller
</span><span class='line'>&lt;/code&gt;
</span><span class='line'>开中断：在softirq上下文中（一般使用TASKLET_SOFTIRQ这个softirq）进行handle Data in the ram的动作&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;同样的，我们先假设Processor A处理了这个网卡中断事件，很快的完成了基本的HW操作后，schedule tasklet（同时也就raise TASKLET_SOFTIRQ softirq）。在返回中断现场前，会检查softirq的触发情况，因此，在TASKLET_SOFTIRQ softirq的handler中，获取tasklet相关信息并在processor A上执行该tasklet的handler。在执行过程中，NIC硬件再次触发中断，Interrupt controller将该中断分发给processor B，执行动作和Processor A是类似的，虽然TASKLET_SOFTIRQ softirq在processor B上可以执行，但是，在检查tasklet的状态的时候，如果发现该tasklet在其他processor上已经正在运行，那么该tasklet不会被处理，一直等到在processor A上的tasklet处理完，在processor B上的这个tasklet才能被执行。这样的串行化操作虽然对驱动工程师是一个福利，但是对性能而言是极大的损伤。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;三、理解softirq需要的基础知识（各种context）&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;1、preempt_count&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;为了更好的理解下面的内容，我们需要先看看一些基础知识：一个task的thread info数据结构定义如下（只保留和本场景相关的内容）：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>struct thread_info {
</span><span class='line'>    ......
</span><span class='line'>    int preempt_count;    /* 0 =&gt; preemptable, &lt;0 =&gt; bug */
</span><span class='line'>    ......
</span><span class='line'>};
</span><span class='line'>&lt;/code&gt;
</span><span class='line'>preempt_count这个成员被用来判断当前进程是否可以被抢占。如果preempt_count不等于0（可能是代码调用preempt_disable显式的禁止了抢占，也可能是处于中断上下文等），说明当前不能进行抢占，如果preempt_count等于0，说明已经具备了抢占的条件（当然具体是否要抢占当前进程还是要看看thread info中的flag成员是否设定了_TIF_NEED_RESCHED这个标记，可能是当前的进程的时间片用完了，也可能是由于中断唤醒了优先级更高的进程）。 具体preempt_count的数据格式可以参考下图：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-05-07-12.gif" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;preemption count用来记录当前被显式的禁止抢占的次数，也就是说，每调用一次preempt_disable，preemption count就会加一，调用preempt_enable，该区域的数值会减去一。preempt_disable和preempt_enable必须成对出现，可以嵌套，最大嵌套的深度是255。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;hardirq count描述当前中断handler嵌套的深度。对于ARM平台的linux kernel，其中断部分的代码如下：</span></code></pre></td></tr></table></div></figure>
    void handle_IRQ(unsigned int irq, struct pt_regs <em>regs)
    {
        struct pt_regs </em>old_regs = set_irq_regs(regs);</p>

<pre><code>    irq_enter(); 
    generic_handle_irq(irq);

    irq_exit();
    set_irq_regs(old_regs);
}
</code></pre>

<pre><code>通用的IRQ handler被irq_enter和irq_exit这两个函数包围。irq_enter说明进入到IRQ context，而irq_exit则说明退出IRQ context。在irq_enter函数中会调用preempt_count_add(HARDIRQ_OFFSET)，为hardirq count的bit field增加1。在irq_exit函数中，会调用preempt_count_sub(HARDIRQ_OFFSET)，为hardirq count的bit field减去1。hardirq count占用了4个bit，说明硬件中断handler最大可以嵌套15层。在旧的内核中，hardirq count占用了12个bit，支持4096个嵌套。当然，在旧的kernel中还区分fast interrupt handler和slow interrupt handler，中断handler最大可以嵌套的次数理论上等于系统IRQ的个数。在实际中，这个数目不可能那么大（内核栈就受不了），因此，即使系统支持了非常大的中断个数，也不可能各个中断依次嵌套，达到理论的上限。基于这样的考虑，后来内核减少了hardirq count占用bit数目，改成了10个bit（在general arch的代码中修改为10，实际上，各个arch可以redefine自己的hardirq count的bit数）。但是，当内核大佬们决定废弃slow interrupt handler的时候，实际上，中断的嵌套已经不会发生了。因此，理论上，hardirq count要么是0，要么是1。不过呢，不能总拿理论说事，实际上，万一有写奇葩或者老古董driver在handler中打开中断，那么这时候中断嵌套还是会发生的，但是，应该不会太多（一个系统中怎么可能有那么多奇葩呢？呵呵），因此，目前hardirq count占用了4个bit，应付15个奇葩driver是妥妥的。

对softirq count进行操作有两个场景：

（1）也是在进入soft irq handler之前给 softirq count加一，退出soft irq handler之后给 softirq count减去一。由于soft irq handler在一个CPU上是不会并发的，总是串行执行，因此，这个场景下只需要一个bit就够了，也就是上图中的bit 8。通过该bit可以知道当前task是否在sofirq context。

（2）由于内核同步的需求，进程上下文需要禁止softirq。这时候，kernel提供了local_bf_enable和local_bf_disable这样的接口函数。这部分的概念是和preempt disable/enable类似的，占用了bit9～15，最大可以支持127次嵌套。

#### 2、一个task的各种上下文

看完了preempt_count之后，我们来介绍各种context：
</code></pre>

<pre><code>#define in_irq()        (hardirq_count())
#define in_softirq()        (softirq_count())
#define in_interrupt()        (irq_count())

#define in_serving_softirq()    (softirq_count() &amp; SOFTIRQ_OFFSET)
</code></pre>

<pre><code>这里首先要介绍的是一个叫做IRQ context的术语。这里的IRQ context其实就是hard irq context，也就是说明当前正在执行中断handler（top half），只要preempt_count中的hardirq count大于0（＝1是没有中断嵌套，如果大于1，说明有中断嵌套），那么就是IRQ context。

softirq context并没有那么的直接，一般人会认为当sofirq handler正在执行的时候就是softirq context。这样说当然没有错，sofirq handler正在执行的时候，会增加softirq count，当然是softirq context。不过，在其他context的情况下，例如进程上下文中，有有可能因为同步的要求而调用local_bh_disable，这时候，通过local_bh_disable/enable保护起来的代码也是执行在softirq context中。当然，这时候其实并没有正在执行softirq handler。如果你确实想知道当前是否正在执行softirq handler，in_serving_softirq可以完成这个使命，这是通过操作preempt_count的bit 8来完成的。

所谓中断上下文，就是IRQ context ＋ softirq context＋NMI context。

### 四、softirq机制

softirq和hardirq（就是硬件中断啦）是对应的，因此softirq的机制可以参考hardirq对应理解，当然softirq是纯软件的，不需要硬件参与。

#### 1、softirq number

和IRQ number一样，对于软中断，linux kernel也是用一个softirq number唯一标识一个softirq，具体定义如下：
</code></pre>

<pre><code>enum
{
    HI_SOFTIRQ=0,
    TIMER_SOFTIRQ,
    NET_TX_SOFTIRQ,
    NET_RX_SOFTIRQ,
    BLOCK_SOFTIRQ,
    BLOCK_IOPOLL_SOFTIRQ,
    TASKLET_SOFTIRQ,
    SCHED_SOFTIRQ,
    HRTIMER_SOFTIRQ,
    RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */

    NR_SOFTIRQS
};
</code></pre>

<pre><code>HI_SOFTIRQ用于高优先级的tasklet，TASKLET_SOFTIRQ用于普通的tasklet。TIMER_SOFTIRQ是for software timer的（所谓software timer就是说该timer是基于系统tick的）。NET_TX_SOFTIRQ和NET_RX_SOFTIRQ是用于网卡数据收发的。BLOCK_SOFTIRQ和BLOCK_IOPOLL_SOFTIRQ是用于block device的。SCHED_SOFTIRQ用于多CPU之间的负载均衡的。HRTIMER_SOFTIRQ用于高精度timer的。RCU_SOFTIRQ是处理RCU的。这些具体使用情景分析会在各自的子系统中分析，本文只是描述softirq的工作原理。

#### 2、softirq描述符

我们前面已经说了，softirq是静态定义的，也就是说系统中有一个定义softirq描述符的数组，而softirq number就是这个数组的index。这个概念和早期的静态分配的中断描述符概念是类似的。具体定义如下：
</code></pre>

<pre><code>struct softirq_action
{
    void    (*action)(struct softirq_action *);
};

static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;
</code></pre>

<pre><code>系统支持多少个软中断，静态定义的数组就会有多少个entry。`____cacheline_aligned`保证了在SMP的情况下，softirq_vec是对齐到cache line的。softirq描述符非常简单，只有一个action成员，表示如果触发了该softirq，那么应该调用action回调函数来处理这个soft irq。对于硬件中断而言，其mask、ack等都是和硬件寄存器相关并封装在irq chip函数中，对于softirq，没有硬件寄存器，只有“软件寄存器”，定义如下：
</code></pre>

<pre><code>typedef struct {
    unsigned int __softirq_pending;
#ifdef CONFIG_SMP
    unsigned int ipi_irqs[NR_IPI];
#endif
} ____cacheline_aligned irq_cpustat_t;

irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;
</code></pre>

<pre><code>ipi_irqs这个成员用于处理器之间的中断，我们留到下一个专题来描述。`__softirq_pending`就是这个“软件寄存器”。softirq采用谁触发，谁负责处理的。例如：当一个驱动的硬件中断被分发给了指定的CPU，并且在该中断handler中触发了一个softirq，那么该CPU负责调用该softirq number对应的action callback来处理该软中断。因此，这个“软件寄存器”应该是每个CPU拥有一个（专业术语叫做banked register）。为了性能，irq_stat中的每一个entry被定义对齐到cache line。

#### 3、如何注册一个softirq

通过调用open_softirq接口函数可以注册softirq的action callback函数，具体如下：
</code></pre>

<pre><code>void open_softirq(int nr, void (*action)(struct softirq_action *))
{
    softirq_vec[nr].action = action;
}
</code></pre>

<pre><code>softirq_vec是一个多CPU之间共享的数据，不过，由于所有的注册都是在系统初始化的时候完成的，那时候，系统是串行执行的。此外，softirq是静态定义的，每个entry（或者说每个softirq number）都是固定分配的，因此，不需要保护。

#### 4、如何触发softirq？

在linux kernel中，可以调用raise_softirq这个接口函数来触发本地CPU上的softirq，具体如下：
</code></pre>

<pre><code>void raise_softirq(unsigned int nr)
{
    unsigned long flags;

    local_irq_save(flags);
    raise_softirq_irqoff(nr);
    local_irq_restore(flags);
}
</code></pre>

<pre><code>虽然大部分的使用场景都是在中断handler中（也就是说关闭本地CPU中断）来执行softirq的触发动作，但是，这不是全部，在其他的上下文中也可以调用raise_softirq。因此，触发softirq的接口函数有两个版本，一个是raise_softirq，有关中断的保护，另外一个是raise_softirq_irqoff，调用者已经关闭了中断，不需要关中断来保护“soft irq status register”。

所谓trigger softirq，就是在`__softirq_pending`（也就是上面说的soft irq status register）的某个bit置一。从上面的定义可知，`__softirq_pending`是per cpu的，因此不需要考虑多个CPU的并发，只要disable本地中断，就可以确保对，`__softirq_pending`操作的原子性。

具体raise_softirq_irqoff的代码如下：
</code></pre>

<pre><code>inline void raise_softirq_irqoff(unsigned int nr)
{
    __raise_softirq_irqoff(nr); ---------- （1）

    if (!in_interrupt())
        wakeup_softirqd();      ---------- （2）
}
</code></pre>

<pre><code>（1）`__raise_softirq_irqoff`函数设定本CPU上的`__softirq_pending`的某个bit等于1，具体的bit是由soft irq number（nr参数）指定的。

（2）如果在中断上下文，我们只要set `__softirq_pending`的某个bit就OK了，在中断返回的时候自然会进行软中断的处理。但是，如果在context上下文调用这个函数的时候，我们必须要调用wakeup_softirqd函数用来唤醒本CPU上的softirqd这个内核线程。具体softirqd的内容请参考下一个章节。

#### 5、disable/enable softirq

在linux kernel中，可以使用local_irq_disable和local_irq_enable来disable和enable本CPU中断。和硬件中断一样，软中断也可以disable，接口函数是local_bh_disable和local_bh_enable。虽然和想像的local_softirq_enable/disable有些出入，不过bh这个名字更准确反应了该接口函数的意涵，因为local_bh_disable/enable函数就是用来disable/enable bottom half的，这里就包括softirq和tasklet。

先看disable吧，毕竟禁止bottom half比较简单：
</code></pre>

<pre><code>static inline void local_bh_disable(void)
{
    __local_bh_disable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET);
}

static __always_inline void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
{
    preempt_count_add(cnt);
    barrier();
}
</code></pre>

<pre><code>看起来disable bottom half比较简单，就是讲current thread info上的preempt_count成员中的softirq count的bit field9～15加上一就OK了。barrier是优化屏障（Optimization barrier），会在内核同步系列文章中描述。

enable函数比较复杂，如下：
</code></pre>

<pre><code>static inline void local_bh_enable(void)
{
    __local_bh_enable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET);
}

void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
{
    WARN_ON_ONCE(in_irq() || irqs_disabled()); --------- （1）

    preempt_count_sub(cnt - 1);                --------- （2）

    if (unlikely(!in_interrupt() &amp;&amp; local_softirq_pending())) {  ------- （3）
        do_softirq();
    }

    preempt_count_dec();                       --------- （4）
    preempt_check_resched();
}
</code></pre>

<pre><code>（1）disable/enable bottom half是一种内核同步机制。在硬件中断的handler（top half）中，不应该调用disable/enable bottom half函数来保护共享数据，因为bottom half其实是不可能抢占top half的。同样的，soft irq也不会抢占另外一个soft irq的执行，也就是说，一旦一个softirq handler被调度执行（无论在哪一个processor上），那么，本地的softirq handler都无法抢占其运行，要等到当前的softirq handler运行完毕后，才能执行下一个soft irq handler。注意：上面我们说的是本地，是local，softirq handler是可以在多个CPU上同时运行的，但是，linux kernel中没有disable all softirq的接口函数（就好像没有disable all CPU interrupt的接口一样，注意体会local_bh_enable/disable中的local的含义）。

说了这么多，一言以蔽之，local_bh_enable/disable是给进程上下文使用的，用于防止softirq handler抢占local_bh_enable/disable之间的临界区的。

irqs_disabled接口函数可以获知当前本地CPU中断是否是disable的，如果返回1，那么当前是disable 本地CPU的中断的。如果irqs_disabled返回1，有可能是下面这样的代码造成的：
</code></pre>

<pre><code>local_irq_disable();
......
local_bh_disable();

......

local_bh_enable();
......
local_irq_enable();
</code></pre>

<pre><code>本质上，关本地中断是一种比关本地bottom half更强劲的锁，关本地中断实际上是禁止了top half和bottom half抢占当前进程上下文的运行。也许你会说：这也没有什么，就是有些浪费，至少代码逻辑没有问题。但事情没有这么简单，在`local_bh_enable---&gt;do_softirq---&gt;__do_softirq`中，有一条无条件打开当前中断的操作，也就是说，原本想通过local_irq_disable/local_irq_enable保护的临界区被破坏了，其他的中断handler可以插入执行，从而无法保证local_irq_disable/local_irq_enable保护的临界区的原子性，从而破坏了代码逻辑。

in_irq()这个函数如果不等于0的话，说明local_bh_enable被irq_enter和irq_exit包围，也就是说在中断handler中调用了local_bh_enable/disable。这道理是和上面类似的，这里就不再详细描述了。

（2）在local_bh_disable中我们为preempt_count增加了SOFTIRQ_DISABLE_OFFSET，在local_bh_enable函数中应该减掉同样的数值。这一步，我们首先减去了（SOFTIRQ_DISABLE_OFFSET-1），为何不一次性的减去SOFTIRQ_DISABLE_OFFSET呢？考虑下面运行在进程上下文的代码场景：
</code></pre>

<pre><code>......

local_bh_disable

...需要被保护的临界区...

local_bh_enable
......
</code></pre>

<pre><code>在临界区内，有进程context 和softirq共享的数据，因此，在进程上下文中使用local_bh_enable/disable进行保护。假设在临界区代码执行的时候，发生了中断，由于代码并没有阻止top half的抢占，因此中断handler会抢占当前正在执行的thread。在中断handler中，我们raise了softirq，在返回中断现场的时候，由于disable了bottom half，因此虽然触发了softirq，但是不会调度执行。因此，代码返回临界区继续执行，直到local_bh_enable。一旦enable了bottom half，那么之前raise的softirq就需要调度执行了，因此，这也是为什么在local_bh_enable会调用do_softirq函数。

调用do_softirq函数来处理pending的softirq的时候，当前的task是不能被抢占的，因为一旦被抢占，下一次该task被调度运行的时候很可能在其他的CPU上去了（还记得吗？softirq的pending 寄存器是per cpu的）。因此，我们不能一次性的全部减掉，那样的话有可能preempt_count等于0，那样就允许抢占了。因此，这里减去了（SOFTIRQ_DISABLE_OFFSET-1），既保证了softirq count的bit field9~15被减去了1，又保持了preempt disable的状态。

（3）如果当前不是interrupt context的话，并且有pending的softirq，那么调用do_softirq函数来处理软中断。

（4）该来的总会来，在step 2中我们少减了1，这里补上，其实也就是preempt count-1。

（5）在softirq handler中很可能wakeup了高优先级的任务，这里最好要检查一下，看看是否需要进行调度，确保高优先级的任务得以调度执行。


#### 5、如何处理一个被触发的soft irq

我们说softirq是一种defering task的机制，也就是说top half没有做的事情，需要延迟到bottom half中来执行。那么具体延迟到什么时候呢？这是本节需要讲述的内容，也就是说soft irq是如何调度执行的。

在上一节已经描述一个softirq被调度执行的场景，本节主要关注在中断返回现场时候调度softirq的场景。我们来看中断退出的代码，具体如下：
</code></pre>

<pre><code>void irq_exit(void)
{
    ......
    if (!in_interrupt() &amp;&amp; local_softirq_pending())
        invoke_softirq();

    ......
}
</code></pre>

<pre><code>代码中“!in_interrupt()”这个条件可以确保下面的场景不会触发sotfirq的调度：

（1）中断handler是嵌套的。也就是说本次irq_exit是退出到上一个中断handler。当然，在新的内核中，这种情况一般不会发生，因为中断handler都是关中断执行的。

（2）本次中断是中断了softirq handler的执行。也就是说本次irq_exit是不是退出到进程上下文，而是退出到上一个softirq context。这一点也保证了在一个CPU上的softirq是串行执行的（注意：多个CPU上还是有可能并发的）

我们继续看invoke_softirq的代码：
</code></pre>

<pre><code>static inline void invoke_softirq(void)
{
    if (!force_irqthreads) {
#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
        __do_softirq();
#else
        do_softirq_own_stack();
#endif
    } else {
        wakeup_softirqd();
    }
}
</code></pre>

<pre><code>force_irqthreads是和强制线程化相关的，主要用于interrupt handler的调试（一般而言，在线程环境下比在中断上下文中更容易收集调试数据）。如果系统选择了对所有的interrupt handler进行线程化处理，那么softirq也没有理由在中断上下文中处理（中断handler都在线程中执行了，softirq怎么可能在中断上下文中执行）。本身invoke_softirq这个函数是在中断上下文中被调用的，如果强制线程化，那么系统中所有的软中断都在sofirq的daemon进程中被调度执行。

如果没有强制线程化，softirq的处理也分成两种情况，主要是和softirq执行的时候使用的stack相关。如果arch支持单独的IRQ STACK，这时候，由于要退出中断，因此irq stack已经接近全空了（不考虑中断栈嵌套的情况，因此新内核下，中断不会嵌套），因此直接调用`__do_softirq()`处理软中断就OK了，否则就调用do_softirq_own_stack函数在softirq自己的stack上执行。当然对ARM而言，softirq的处理就是在当前的内核栈上执行的，因此do_softirq_own_stack的调用就是调用`__do_softirq()`，代码如下（删除了部分无关代码）：
</code></pre>

<pre><code>asmlinkage void __do_softirq(void)
{
......
    pending = local_softirq_pending();  ----------- 获取softirq pending的状态
    __local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET); ---- 标识下面的代码是正在处理softirq
    cpu = smp_processor_id();
restart:
    set_softirq_pending(0);  ------------- 清除pending标志
    local_irq_enable();      ------------- 打开中断，softirq handler是开中断执行的
    h = softirq_vec;         ------------- 获取软中断描述符指针

    while ((softirq_bit = ffs(pending))) { --------- 寻找pending中第一个被设定为1的bit
        unsigned int vec_nr;
        int prev_count;

        h += softirq_bit - 1; ----------- 指向pending的那个软中断描述符
        vec_nr = h - softirq_vec; ------- 获取soft irq number
        h-&gt;action(h);         ----------- 指向softirq handler
        h++;
        pending &gt;&gt;= softirq_bit;
    }

    local_irq_disable();      ----------- 打开中断

    pending = local_softirq_pending(); ------ （注1）
    if (pending) {
        if (time_before(jiffies, end) &amp;&amp; !need_resched() &amp;&amp;
            --max_restart)
            goto restart;

        wakeup_softirqd();
    }
    __local_bh_enable(SOFTIRQ_OFFSET); ----------- 标识softirq处理完毕
}
</code></pre>

<p>```
（注1）再次检查softirq pending，有可能上面的softirq handler在执行过程中，发生了中断，又raise了softirq。如果的确如此，那么我们需要跳转到restart那里重新处理soft irq。当然，也不能总是在这里不断的loop，因此linux kernel设定了下面的条件：</p>

<p>（1）softirq的处理时间没有超过2个ms</p>

<p>（2）上次的softirq中没有设定TIF_NEED_RESCHED，也就是说没有有高优先级任务需要调度</p>

<p>（3）loop的次数小于 10次</p>

<p>因此，只有同时满足上面三个条件，程序才会跳转到restart那里重新处理soft irq。否则wakeup_softirqd就OK了。这样的设计也是一个平衡的方案。一方面照顾了调度延迟：本来，发生一个中断，系统期望在限定的时间内调度某个进程来处理这个中断，如果softirq handler不断触发，其实linux kernel是无法保证调度延迟时间的。另外一方面，也照顾了硬件的thoughput：已经预留了一定的时间来处理softirq。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[内核源码分析之linux内核栈]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/07/kernel-irq-stack2/"/>
    <updated>2015-05-07T15:54:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/07/kernel-irq-stack2</id>
    <content type="html"><![CDATA[<p><a href="http://www.cnblogs.com/liangning/p/3879177.html">http://www.cnblogs.com/liangning/p/3879177.html</a></p>

<p>基于3.16-rc4</p>

<p>在3.16-rc4内核源码中，内核给每个进程分配的内核栈大小为8KB。这个内核栈被称为异常栈，在进程的内核空间运行时或者执行异常处理程序时，使用的都是异常栈，看下异常栈的代码（include/linux/sched.h）：
<code>
    union thread_union {
        struct thread_info thread_info;
        unsigned long stack[THREAD_SIZE/sizeof(long)];
    };
</code>
THREAD_SIZE值为8KB，因此内核为进程的异常栈（内核栈）分配了两个页框大小（页框大小4KB）。另外，进程的thread_info结构体保存在栈顶部。</p>

<p>此外，内核为每个cpu分配一个硬中断栈和一个软中断栈（这两个栈也是内核栈），用来执行中断服务例程和下半部（软中断），看看代码（arch/x86/kernel/irq_32.c）。这两个栈属于cpu，不属于进程，这和异常栈是有区别的。
<code>
    DEFINE_PER_CPU(struct irq_stack *, hardirq_stack);
    DEFINE_PER_CPU(struct irq_stack *, softirq_stack);
</code>
定义了两个数组hardirq_stack和softirq_stack，每个数组元素对应一个cpu，指向了该cpu的硬中断栈或者软中断栈。再来看下struct irq_stack结构体（arch/x86/include/asm/processor.h）：
<code>
    struct irq_stack {
        u32                     stack[THREAD_SIZE/sizeof(u32)];
    } __aligned(THREAD_SIZE);
</code>
可见，硬中断栈和软中断栈的大小均为8KB。</p>

<p>内核在执行中断处理程序时，在do_IRQ函数中会调用handle_irq函数，在handle_irq函数中要进行堆栈切换，代码如下（arch/x86/kernel/irq_32.c）：
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bool handle_irq(unsigned irq, struct pt_regs &lt;em&gt;regs)
</span><span class='line'>{
</span><span class='line'>    struct irq_desc &lt;/em&gt;desc;
</span><span class='line'>    int overflow;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    overflow = check_stack_overflow();
</span><span class='line'>
</span><span class='line'>desc = irq_to_desc(irq);
</span><span class='line'>if (unlikely(!desc))
</span><span class='line'>return false;
</span><span class='line'>
</span><span class='line'>if (user_mode_vm(regs) || !execute_on_irq_stack(overflow, desc, irq)) {
</span><span class='line'>    if (unlikely(overflow))
</span><span class='line'>        print_stack_overflow();
</span><span class='line'>    desc-&gt;handle_irq(irq, desc);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>return true;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>第12行中执行execute_on_irq_stack函数来判断是否需要堆栈切换，如果不需要，则执行if体的中断服务例程，即在当前堆栈中执行中断服务例程，如果需要切换堆栈，则在execute_on_irq_stack函数中切换堆栈并在该函数中（新堆栈中）执行中断服务例程。下面看下execute_on_irq_stack代码（arch/x86/kernel/irq_32.c）：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;static inline int
</span><span class='line'>execute_on_irq_stack(int overflow, struct irq_desc *desc, int irq)
</span><span class='line'>{
</span><span class='line'>struct irq_stack *curstk, *irqstk;
</span><span class='line'>u32 *isp, *prev_esp, arg1, arg2;
</span><span class='line'>
</span><span class='line'>curstk = (struct irq_stack *) current_stack();
</span><span class='line'>irqstk = __this_cpu_read(hardirq_stack);
</span><span class='line'>
</span><span class='line'>/*
</span><span class='line'> * this is where we switch to the IRQ stack. However, if we are
</span><span class='line'> * already using the IRQ stack (because we interrupted a hardirq
</span><span class='line'> * handler) we can't do that and just have to keep using the
</span><span class='line'> * current stack (which is the irq stack already after all)
</span><span class='line'> */
</span><span class='line'>if (unlikely(curstk == irqstk))
</span><span class='line'>    return 0;
</span><span class='line'>
</span><span class='line'>isp = (u32 *) ((char *)irqstk + sizeof(*irqstk));
</span><span class='line'>
</span><span class='line'>/* Save the next esp at the bottom of the stack */
</span><span class='line'>prev_esp = (u32 *)irqstk;
</span><span class='line'>*prev_esp = current_stack_pointer;
</span><span class='line'>
</span><span class='line'>if (unlikely(overflow))
</span><span class='line'>    call_on_stack(print_stack_overflow, isp);
</span><span class='line'>
</span><span class='line'>asm volatile("xchgl    %%ebx,%%esp    \n"
</span><span class='line'>         "call    *%%edi        \n"
</span><span class='line'>         "movl    %%ebx,%%esp    \n"
</span><span class='line'>         : "=a" (arg1), "=d" (arg2), "=b" (isp)
</span><span class='line'>         :  "0" (irq),   "1" (desc),  "2" (isp),
</span><span class='line'>        "D" (desc-&gt;handle_irq)
</span><span class='line'>         : "memory", "cc", "ecx");
</span><span class='line'>return 1;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;```&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;第7行获取当前堆栈的指针，第8行获取本地cpu的硬中断栈指针，第16行对二者进行比较，如果相等，则不需要切换堆栈（说明当前堆栈就是硬中断栈，也说明是在中断处理程序中时又发生了中断）。如果不相等，就要进行堆栈切换，第22-23行将当前堆栈指针保存在将要切换到的堆栈中（用于返回）。第28行，交换ebx和esp寄存器的值（实现了堆栈切换，将中断栈指针给了esp），第29行跳转到相应的中断服务例程，第30行从中断服务例程返回后，又将原来的堆栈指针赋给esp，切换到原先堆栈。第33行将中断服务例程函数名存放在%edi中。&lt;/p&gt;
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[中断栈溢出后的结果]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2015/05/07/kernel-irq-stack/"/&gt;
</span><span class='line'>&lt;updated&gt;2015-05-07T15:54:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2015/05/07/kernel-irq-stack&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;p&gt;&lt;a href="http://www.lenky.info/archives/2013/03/2247"&gt;http://www.lenky.info/archives/2013/03/2247&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;说一下上文中最开始提到的“某个问题”：如果一台主机网卡比较多，然后每个网卡分队列又比较多，总之结果就是系统里的网卡设备的中断号比较多（关于超过256个中断数的情况，请见参考1，2，3），一旦所有这些中断都绑定到同一个CPU，那么如果网卡收到数据包进而触发中断，而众多硬中断一嵌套就非常容易出现中断栈溢出。一旦中断栈溢出，那么将会导致怎样的结果，这曾在之前的文章里隐含的提到过，这里再重新整理一遍。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;在继续下面的描述之前，先看两个知识点：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;1，Linux 2.4.x的中断栈：&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;a)，由硬中断/软中断共同使用同一个中断栈&lt;br/&gt;
</span><span class='line'>b)，中断栈与内核栈共享一个栈&lt;br/&gt;
</span><span class='line'>c)，中断执行的时候使用的栈就是当前进程的内核栈&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;2，Linux 2.6.x的中断栈：&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;a)，硬中断与软中断分离使用不同的中断栈&lt;br/&gt;
</span><span class='line'>b)，中断栈与内核栈分离&lt;br/&gt;
</span><span class='line'>c)，X86_64 double fault、NMI还可以有额外的栈（64bit特性：IST(Interrupt Stack Table)）&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;可以看到，对于Linux 2.4.x内核而言，因为中断处理函数使用内核栈作为中断栈，所以导致更加容易发生内核栈溢出（因内核函数本身用栈过多导致溢出，或内核函数本身还未导致内核栈溢出，但此时来了一个中断，因中断函数使用栈而导致溢出，即中断函数成了压死骆驼的最后一根稻草），而内核栈溢出的直接结果就是踩坏task结构体，从而无法正常执行对应的task进程而出现oops宕机。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;由于“中断执行的时候使用的栈就是当前进程的内核栈”，所以如果是执行到中断函数后才溢出，那么导致oops里提示的进程信息可能每次都不一样，因此如果出现这种情况，需要考虑是中断函数导致内核栈溢出，否则需怀疑普通的内核函数导致栈溢出即可。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;对于Linux 2.6.x内核而言，因为其中断/内核栈分离、软/硬中断栈分离，即每个CPU私有两个栈（见下面注释）分别处理软中断和硬中断，因此出现内核栈溢出，特别是中断栈溢出的概率大大降低。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;注释：这个说法来之书本《Understanding.the.Linux.Kernel.3rd.Edition》4.6.1.4. Multiple Kernel Mode stacks，而这本书针对的内核版本是2.6.11，且主要是指32位架构，所以与现在的新版内核源码有些许出入（比如现在情况的栈大小可能是占用2页），但这些细微改变与本文的具体问题相关不大（无非是溢出的难易程度问题），这里不再深入研究，具体情况请参考源代码自行斟酌。</span></code></pre></td></tr></table></div></figure>
    The hard IRQ stack is used when handling interrupts. There is one hard IRQ stack for each CPU in the system, and each stack is contained in a single page frame.</p>

<pre><code>The soft IRQ stack is used when handling deferrable functions (softirqs or tasklets; see the later section “Softirqs and Tasklets”). There is one soft IRQ stack for each CPU in the system, and each stack is contained in a single page frame. 
</code></pre>

<pre><code>回到本文的主题，在之前的文章里提到过，即如果中断/异常处理函数本身在处理的过程中出现异常，那么就有可能发生double fault，比如中断栈溢出。中断栈溢出导致的最终结果有两种情况，这由所使用的具体Linux内核版本来决定，更具体点说是由double fault异常的栈是否单独来决定（见参考1）。

1，double fault的栈被单独出来  
这意味着double fault的处理函数还能正常执行，因此打印oops，宕机。

2，double fault的栈没有被单独出来  
这意味着double fault的处理函数也无法正常执行，进而触发triple fault，机器直接重启。

对于86-64架构下的Linux 2.6.x内核，因为IST(Interrupt Stack Table)的帮助，所以中断栈溢出导致的最终结果就是打印oops，宕机。

下面来看内核源码文档kernel-stacks，  
1，每一个活动线程都有一个内核栈，大小为2页。  
2，每一个cpu有一些专门的栈，只有当cpu执行在内核态时，这些栈才有用；一旦cpu回退到用户态，这些特定栈就不再包含任何有用数据。  
3，主要的特定栈有：  
a，中断栈：外部硬件中断的处理函数使用，单独的栈可以提供给中断处理函数更多的栈空间。  
这里还提到，在2.6.x-i386下，如果设置内核栈只有4K，即CONFIG_4KSTACKS，那么中断栈也是单独开的。备注：这个已有修改，2010-06-29 x86: Always use irq stacks，即不管设置的内核栈是否只有4K，中断栈都是独立的了。  

另外，这里有个说法与前面的引用有点出入：
</code></pre>

<pre><code>The interrupt stack is also used when processing a softirq. 
</code></pre>

<p>```
即软中断和硬中断一样，也是使用这个中断栈。</p>

<p>b，x86_64所特有的（也就是i386没有，即同时2.6.30.8内核，32位的Linux就不具备下面所说的这个特性），为double fault或NMI单独准备的栈，这个特性被称为Interrupt Stack Table(IST)。每个cpu最多支持7个IST。关于IST的具体原理与实现暂且不说，直接来看当前已经分配的IST独立栈：</p>

<ul>
<li><p>STACKFAULT_STACK. EXCEPTION_STKSZ (PAGE_SIZE)<br/>
12号中断Stack Fault Exception (#SS)使用</p></li>
<li><p>DOUBLEFAULT_STACK. EXCEPTION_STKSZ (PAGE_SIZE)<br/>
8号中断Double Fault Exception (#DF)使用</p></li>
<li><p>NMI_STACK. EXCEPTION_STKSZ (PAGE_SIZE)<br/>
2号中断non-maskable interrupts (NMI)使用</p></li>
<li><p>DEBUG_STACK. DEBUG_STKSZ<br/>
1号中断硬件调试和3号中断软件调试使用</p></li>
<li><p>MCE_STACK. EXCEPTION_STKSZ (PAGE_SIZE)<br/>
18号中断Machine Check Exception (#MC)使用</p></li>
</ul>


<p>正因为double fault异常处理函数所使用的栈被单独了出来，所以在出现中断栈溢出时，double fault异常的处理函数还能正常执行，顺利打印出oops信息。</p>

<p>最后的最后，有补丁移除IST功能（貌似是因为如果没有IST功能，那么kvm可以得到更好的优化，具体请见参考5），但通过对比补丁修改与实际源码（2.6.30.8以及3.6.11）来看，这个补丁并没有合入mainline主线。</p>

<h4>参考资料：</h4>

<p>1，where is hardware timer interrupt?<br/>
<a href="http://stackoverflow.com/questions/14481032/where-is-hardware-timer-interrupt">http://stackoverflow.com/questions/14481032/where-is-hardware-timer-interrupt</a></p>

<p>2，The MSI Driver Guide HOWTO<br/>
<a href="https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/Documentation/PCI/MSI-HOWTO.txt?id=v2.6.30.8  ">https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/Documentation/PCI/MSI-HOWTO.txt?id=v2.6.30.8  </a>
对应的翻译版：<a href="http://blog.csdn.net/reviver/article/details/6802347">http://blog.csdn.net/reviver/article/details/6802347</a></p>

<p>3，[PATCH] x86: 64bit support more than 256 irq v2<br/>
<a href="http://linux-kernel.2935.n7.nabble.com/PATCH-x86-64bit-support-more-than-256-irq-v2-td323261.html">http://linux-kernel.2935.n7.nabble.com/PATCH-x86-64bit-support-more-than-256-irq-v2-td323261.html</a></p>

<p>4，How is an Interrupt handled in Linux?<br/>
<a href="http://unix.stackexchange.com/questions/5788/how-is-an-interrupt-handled-in-linux">http://unix.stackexchange.com/questions/5788/how-is-an-interrupt-handled-in-linux</a></p>

<p>5，Remove interrupt stack table usage from x86_64 kernel (v2)<br/>
<a href="http://lwn.net/Articles/313029/  ">http://lwn.net/Articles/313029/  </a>
<a href="http://thread.gmane.org/gmane.comp.emulators.kvm.devel/26741">http://thread.gmane.org/gmane.comp.emulators.kvm.devel/26741</a></p>

<p>6，Interrupt Descriptor Table<br/>
<a href="http://wiki.osdev.org/IDT">http://wiki.osdev.org/IDT</a></p>

<p>转载请保留地址：<a href="http://www.lenky.info/archives/2013/03/2247">http://www.lenky.info/archives/2013/03/2247</a> 或 <a href="http://lenky.info/?p=2247">http://lenky.info/?p=2247</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对Linux x86-64架构上硬中断的重新认识]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/07/kernel-irq-irq/"/>
    <updated>2015-05-07T15:48:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/07/kernel-irq-irq</id>
    <content type="html"><![CDATA[<p><a href="http://www.lenky.info/archives/2013/03/2245">http://www.lenky.info/archives/2013/03/2245</a></p>

<p>对于x86硬中断的概念，一直都落在理论的认识之上，直到这两天才（因某个问题）发现Linux的实现却并非如此，这里纠正一下（注意：Linux内核源码更新太快，一个说法的时效性太短，所以需注意我提到的香草内核版本，并且以x86-64架构为基准）。</p>

<p>以前的认识：Linux对硬中断（本文如无特殊说明，都是指普通意义上的可屏蔽硬件中断）的处理有优先级概念，高优先级硬中断可以打断低优先级硬中断。</p>

<h4>重新认识：</h4>

<p>1，对于x86硬件而言，在文档325462.pdf卷3章节6.9 PRIORITY AMONG SIMULTANEOUS EXCEPTIONS AND INTERRUPTS 提到一个表格，是指如果在同一时刻有多个异常或中断到达，那么CPU会按照一个指定的优先级顺序对它们进行响应和服务，而并不是我之前所想的判断是否可相互打断执行的高低级别。</p>

<p>2，对于Linux系统而言，硬中断之间并没有优先级的概念（虽然Intel CPU提供支持，请参考文档325462.pdf卷3章节10.8.3 Interrupt, Task, and Processor Priority），或者说优先级只有两个，全部关闭或全部开启，如下：</p>

<blockquote><blockquote><p>Regardless of what the hardware might support, typical UNIX-type systems only make use of two levels: the minimum (all interrupts enabled) and the maximum (all interrupts disabled).</p></blockquote></blockquote>

<p>这意味着，如果一个硬中断处理函数正在执行，只要当前是处于开启中断的情况，那么此时发生的任何另外一个中断都可以打断当前处理函数，从而出现中断嵌套的情况。
值得注意的是，Linux提供对单个中断开启/禁止的接口（以软件实现为主，比如给对应中断描述符desc的status打上IRQ_DISABLED旗标）：
<code>
    void disable_irq(unsigned int irq)
    void enable_irq(unsigned int irq)
</code>
下面来看看Linux的实际处理，其硬中断的一般处理流程（具体可见参考1、2、3以及源代码，以2.6.30.8为例）：
<code>
硬件中断 -&gt; common_interrupt -&gt; do_IRQ -&gt; handle_irq -&gt; generic_handle_irq_desc -&gt; desc-&gt;handle_irq或__do_IRQ。
</code></p>

<p>其中desc->handle_irq是一个回调函数，会根据不同中断类型（I/O APIC、MSI）有不同的指向，比如：handle_fasteoi_irq()、handle_edge_irq()，这可以参考设置函数ioapic_register_intr()和setup_msi_irq()。通过/proc/interrupts可以看到各个中断的具体类型：
<code>
    [root@localhost ~]# cat /proc/interrupts
               CPU0       CPU1      
      0:        888          0   IO-APIC-edge      timer
      1:         96        112   IO-APIC-edge      i8042
      3:          1          0   IO-APIC-edge   
      4:          1          0   IO-APIC-edge   
      7:          0          0   IO-APIC-edge      parport0
      8:          1          0   IO-APIC-edge      rtc0
      9:          0          0   IO-APIC-fasteoi   acpi
     12:        204          0   IO-APIC-edge      i8042
     14:          0          0   IO-APIC-edge      ata_piix
     15:     460641        900   IO-APIC-edge      ata_piix
     16:          0          0   IO-APIC-fasteoi   Ensoniq AudioPCI
     17:     118347          0   IO-APIC-fasteoi   ehci_hcd:usb1, ioc0
     18:         70          0   IO-APIC-fasteoi   uhci_hcd:usb2
     19:     115143          0   IO-APIC-fasteoi   eth0
     24:          0          0   PCI-MSI-edge      pciehp
     25:          0          0   PCI-MSI-edge      pciehp
     26:          0          0   PCI-MSI-edge      pciehp
     27:          0          0   PCI-MSI-edge      pciehp
     28:          0          0   PCI-MSI-edge      pciehp
    ...
</code>
不管是desc->handle_irq还是__do_IRQ，它们都会调入到另外一个函数handle_IRQ_event()。重点：从CPU接收到中断信号并开始处理，到这个函数为止，都是处于中断禁止状态。为什么？很简单，因为Intel开发者手册上是这么说的，在文档325462.pdf卷3章节6.8.1 Masking Maskable Hardware Interrupts提到：
<code>
    When an interrupt is handled through an interrupt gate, the IF flag is automati-
    cally cleared, which disables maskable hardware interrupts. (If an interrupt is
    handled through a trap gate, the IF flag is not cleared.)
</code>
在CPU开始处理一个硬中断到进入函数handle_IRQ_event()为止的这段时间里，因为处于中断禁止状态，所以不会出现被其它中断打断的情况。但是，在进入到函数handle_IRQ_event()后，立马有了这么两句：
```
    irqreturn_t handle_IRQ_event(unsigned int irq, struct irqaction *action)
    {
        irqreturn_t ret, retval = IRQ_NONE;
        unsigned int status = 0;</p>

<pre><code>    if (!(action-&gt;flags &amp; IRQF_DISABLED))
        local_irq_enable_in_hardirq();
...
</code></pre>

<pre><code>函数local_irq_enable_in_hardirq()的定义如下：
</code></pre>

<pre><code>#ifdef CONFIG_LOCKDEP
# define local_irq_enable_in_hardirq()  do { } while (0)
#else
# define local_irq_enable_in_hardirq()  local_irq_enable()
#endif
</code></pre>

<pre><code>宏CONFIG_LOCKDEP用于表示当前是否开启内核Lockdep功能，这是一个调试功能，用于检测潜在的死锁类风险，如果开启，那么函数local_irq_enable_in_hardirq()为空，即继续保持中断禁止状态，为什么Lockdep功能需要保持中断禁止待后文再述，这里考虑一般情况，即不开启Lockdep功能，那么执行函数local_irq_enable_in_hardirq()就会开启中断。
看函数handle_IRQ_event()里的代码，如果没有带上IRQF_DISABLED旗标，那么就会执行函数local_irq_enable_in_hardirq()，从而启用中断。旗标IRQF_DISABLED可在利用函数request_irq()注册中断处理回调时设置，比如：
</code></pre>

<pre><code>if (request_irq(uart-&gt;port.irq, bfin_serial_rx_int, IRQF_DISABLED,
     "BFIN_UART_RX", uart)) {
</code></pre>

<pre><code>如果没有设置，那么到函数handle_IRQ_event()这里的代码后，因为中断已经开启，当前中断的后续处理就可能被其它中断打断，从而出现中断嵌套的情况。

3，如果新来的中断类型与当前正在执行的中断类型相同，那么会暂时挂起。主要实现代码在函数__do_IRQ()（handle_fasteoi_irq()、handle_edge_irq()类似）内：
</code></pre>

<pre><code>/*
 * If the IRQ is disabled for whatever reason, we cannot
 * use the action we have.
 */
action = NULL;
if (likely(!(status &amp; (IRQ_DISABLED | IRQ_INPROGRESS)))) {
    action = desc-&gt;action;
    status &amp;= ~IRQ_PENDING; /* we commit to handling */
    status |= IRQ_INPROGRESS; /* we are handling it */
}
desc-&gt;status = status;

/*
 * If there is no IRQ handler or it was disabled, exit early.
 * Since we set PENDING, if another processor is handling
 * a different instance of this same irq, the other processor
 * will take care of it.
 */
if (unlikely(!action))
    goto out;
</code></pre>

<pre><code>逻辑很简单，如果当前中断被禁止（IRQ_DISABLED）或正在执行（IRQ_INPROGRESS），那么goto cot，所以同种类型中断不会相互嵌套。

4，从这个补丁开始，Linux内核已经全面禁止硬中断嵌套了，即从2.6.35开始，默认就是：
</code></pre>

<pre><code>run the irq handlers with interrupts disabled.
</code></pre>

<pre><code>因为这个补丁，所以旗标IRQF_DISABLED没用了，mainline内核在逐步删除它。

我仔细检查了一下，对于2.6.34以及以前的内核，如果要合入这个补丁，那么有略微影响的主要是两个慢速驱动，分别为rtc-twl4030和twl4030-usb，需要按照类似开启Lockdep功能一样：
</code></pre>

<pre><code>#ifdef CONFIG_LOCKDEP
/* WORKAROUND for lockdep forcing IRQF_DISABLED on us, which
 * we don't want and can't tolerate.  Although it might be
 * friendlier not to borrow this thread context...
 */
local_irq_enable();
#endif
</code></pre>

<p>```
进行主动启用中断。还有另个一个慢速驱动IDE，其驱动中调用的是函数local_irq_enable_in_hardirq()，即它在开启Lockdep功能的情况下并没有明确要求启用中断，所以它应该不受补丁合入影响。嘛，我只是理论分析研究一下，仅供参考，如有风险，请实际操作者自行承担，:)。其它请看参考4，5，6。</p>

<h4>参考：</h4>

<p>1，Linux下386中断处理<br/>
2，Linux中断基础构架<br/>
3，linux源码entry_32.S中interrupt数组的分析<br/>
4，<a href="http://lwn.net/Articles/321663/  ">http://lwn.net/Articles/321663/  </a>
5，<a href="http://lwn.net/Articles/380931/  ">http://lwn.net/Articles/380931/  </a>
6，<a href="http://thread.gmane.org/gmane.linux.kernel/801267">http://thread.gmane.org/gmane.linux.kernel/801267</a></p>

<p>转载请保留地址：<a href="http://www.lenky.info/archives/2013/03/2245">http://www.lenky.info/archives/2013/03/2245</a> 或 <a href="http://lenky.info/?p=2245">http://lenky.info/?p=2245</a></p>
]]></content>
  </entry>
  
</feed>
