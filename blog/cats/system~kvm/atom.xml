<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: system~kvm | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/system~kvm/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2016-08-31T00:28:04+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[kvm虚拟化学习笔记]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/07/29/kvm-blog/"/>
    <updated>2015-07-29T15:32:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/07/29/kvm-blog</id>
    <content type="html"><![CDATA[<p><a href="http://koumm.blog.51cto.com/703525/1288795">http://koumm.blog.51cto.com/703525/1288795</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[编译qemu-kvm和安装qemu-kvm]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/07/29/kvm-qemu/"/>
    <updated>2015-07-29T15:22:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/07/29/kvm-qemu</id>
    <content type="html"><![CDATA[<p><a href="http://smilejay.com/2012/06/qemu-kvm_compilation_installation/">http://smilejay.com/2012/06/qemu-kvm_compilation_installation/</a></p>

<h4>3.4 编译和安装qemu-kvm</h4>

<p>除了在内核空间的KVM模块之外，在用户空间需要QEMU[注6]来模拟所需要CPU和设备模型以及用于启动客户机进程，这样才有了一个完整的KVM运行环境。而qemu-kvm是为了针对KVM专门做了修改和优化的QEMU分支[注7]，在本书写作的2012年，qemu-kvm分支里面的小部分特性还没有完全合并进入到qemu的主干代码之中，所以本书中采用qemu-kvm来讲解。</p>

<p>在编译和安装了KVM并且启动到编译的内核之后，下面来看一下qemu-kvm的编译和安装。</p>

<h5>3.4.1 下载qemu-kvm源代码</h5>

<p>目前的QEMU项目针对KVM的代码分支qemu-kvm也是由Redhat公司的Gleb Natapov和Marcelo Tosatti作维护者（Maintainer），代码也是托管在kernel.org上。</p>

<p>qemu-kvm开发代码仓库的网页连接为：<a href="http://git.kernel.org/?p=virt/kvm/qemu-kvm.git">http://git.kernel.org/?p=virt/kvm/qemu-kvm.git</a></p>

<p>其中，可以看到有如下3个URL连接可供下载开发中的最新qemu-kvm的代码仓库。
git://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git<br/>
<a href="http://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git  ">http://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git  </a>
<a href="https://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git">https://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git</a></p>

<p>可以根据自己实际需要选择3个中任一个用git clone命令下载即可，它们是完全一样的。</p>

<p>另外，可以到sourceforge.net的如下链接中根据需要下载qemu-kvm各个发布版本的代码压缩包（作者建议使用最新的正式发布版本，因为它的功能更多，同时也比较稳定）。</p>

<p><a href="http://sourceforge.net/projects/kvm/files/qemu-kvm/">http://sourceforge.net/projects/kvm/files/qemu-kvm/</a></p>

<p>在本节讲解编译时，以下载开发中的最新的qemu-kvm.git为例，获取其代码仓库过程如下：
<code>
    [root@jay-linux kvm_demo]# git clone\ git://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git qemu-kvm.git
    Initialized empty Git repository in /root/kvm_demo/qemu-kvm.git/.git/
    remote: Counting objects: 145222, done.
    remote: Compressing objects: 100% (35825/35825), done.
    remote: Total 145222 (delta 114656), reused 137663 (delta 107444)
    Receiving objects: 100% (145222/145222), 40.83 MiB | 10.33 MiB/s, done.
    Resolving deltas: 100% (114656/114656), done.
    [root@jay-linux kvm_demo]# cd qemu-kvm.git
    [root@jay-linux kvm.git]# pwd
    /root/kvm_demo/qemu-kvm.git
</code></p>

<h5>3.4.2 配置和编译qemu-kvm</h5>

<p>qemu-kvm的配置并不复杂，通常情况下，可以直接运行代码仓库中configure文件进行配置即可。当然，如果对其配置并不熟悉，可以运行“./configure –help”命令查看配置的一些选项及其帮助信息。</p>

<p>显示配置的帮助手册信息如下：
```
    [root@jay-linux qemu-kvm.git]# ./configure &ndash;help
    Usage: configure [options]
    Options: [defaults in brackets after descriptions]</p>

<pre><code>Standard options:
--help                   print this message
--prefix=PREFIX          install in PREFIX [/usr/local]
--interp-prefix=PREFIX   where to find shared libraries, etc.
use %M for cpu name [/usr/gnemul/qemu-%M]
--target-list=LIST       set target list (default: build everything)
Available targets: i386-softmmu x86_64-softmmu
&lt;!- 此处省略百余行帮助信息的输出 -&gt;
--disable-guest-agent    disable building of the QEMU Guest Agent
--enable-guest-agent     enable building of the QEMU Guest Agent
--with-coroutine=BACKEND coroutine backend. Supported options:
gthread, ucontext, sigaltstack, windows

NOTE: The object files are built at the place where configure is launched
</code></pre>

<pre><code>
执行configure文件进行配置的过程如下：
</code></pre>

<pre><code>[root@jay-linux qemu-kvm.git]# ./configure
Install prefix    /usr/local
BIOS directory    /usr/local/share/qemu
binary directory  /usr/local/bin
library directory /usr/local/lib
include directory /usr/local/include
config directory  /usr/local/etc
Manual directory  /usr/local/share/man
ELF interp prefix /usr/gnemul/qemu-%M
Source path       /root/kvm_demo/qemu-kvm.git
C compiler        gcc
Host C compiler   gcc
&lt;!– 此处省略数十行 –&gt;
VNC support       yes     #通常需要通过VNC连接到客户机中
&lt;!– 此处省略十余行 –&gt;
KVM support       yes     #这是对KVM的支持
TCG interpreter   no
KVM device assig. yes    #这是对KVM中VT-d功能的支持
&lt;!– 此处省略十余行 –&gt;
OpenGL support    yes
libiscsi support  no
build guest agent yes
coroutine backend ucontext
</code></pre>

<pre><code>需要注意的是，上面命令行输出的KVM相关的选项需要是配置为yes，另外，一般VNC的支持也是配置为yes的（因为通常需要用VNC连接到客户机中）。

【2013.05.13 updated】 在configure时，可能遇到“glib-2.12 required to compile QEMU”的错误，是需要安装glib2和glib2-dev软件包，在RHEL上的安装命令为“yum install glib2 glib2-devel”，在Ubuntu上安装的过程为“apt-get install libglib2.0 libglib2.0-dev”。

经过配置之后，进行编译就很简单了，直接执行make即可进行编译，如下所示：
</code></pre>

<pre><code>[root@jay-linux qemu-kvm.git]# make -j 20
GEN   config-host.h
GEN   trace.h
GEN   qemu-options.def
GEN   qmp-commands.h
GEN   qapi-types.h
GEN   qapi-visit.h
GEN   tests/test-qapi-types.h
GEN   tests/test-qapi-visit.h
GEN   tests/test-qmp-commands.h
CC    libcacard/cac.o
CC    libcacard/event.o
&lt;!– 此处省略数百行的编译时输出信息 –&gt;
CC    x86_64-softmmu/target-i386/cpu.o
CC    x86_64-softmmu/target-i386/machine.o
CC    x86_64-softmmu/target-i386/arch_memory_mapping.o
CC    x86_64-softmmu/target-i386/arch_dump.o
CC    x86_64-softmmu/target-i386/kvm.o
CC    x86_64-softmmu/target-i386/hyperv.o
LINK  x86_64-softmmu/qemu-system-x86_64
</code></pre>

<pre><code>
可以看到，最后有编译生成qemu-system-x86_64文件，它就是我们常用的qemu-kvm的命令行工具（在多数Linux发行版中自带的qemu-kvm软件包的命令行是qemu-kvm，只是名字不同而已）。

##### 3.4.2 安装qemu-kvm

编译完成之后，运行“make install”命令即可安装qemu-kvm，其过程如下：
</code></pre>

<pre><code>[root@jay-linux qemu-kvm.git]# make install | tee make-install.log
install -d -m 0755 “/usr/local/share/qemu”
install -d -m 0755 “/usr/local/etc/qemu”
install -c -m 0644 /root/kvm_demo/qemu-kvm.git/sysconfigs/target/target-x86_64.conf “/usr/local/etc/qemu”
install -c -m 0644 /root/kvm_demo/qemu-kvm.git/sysconfigs/target/cpus-x86_64.conf “/usr/local/share/qemu”
install -d -m 0755 “/usr/local/bin”
install -c -m 0755  vscclient qemu-ga qemu-nbd qemu-img qemu-io  “/usr/local/bin”
install -d -m 0755 “/usr/local/libexec”
&lt;!– 此处省略数行的安装时输出信息 –&gt;
make[1]: Entering directory `/root/kvm_demo/qemu-kvm.git/x86_64-softmmu’
install -m 755 qemu-system-x86_64 “/usr/local/bin”
strip “/usr/local/bin/qemu-system-x86_64″
make[1]: Leaving directory `/root/kvm_demo/qemu-kvm.git/x86_64-softmmu’
</code></pre>

<pre><code>
qemu-kvm的安装过程的主要任务有这几个：创建qemu的一些目录，拷贝一些配置文件到相应的目录下，拷贝一些firmware文件(如：sgabios.bin, kvmvapic.bin)到目录下以便qemu-kvm的命令行启动时可以找到对应的固件提供给客户机使用，拷贝keymaps到相应的目录下以便在客户机中支持各种所需键盘类型，拷贝qemu-system-x86_64、qemu-img等可执行程序到对应的目录下。下面的一些命令行检查了qemu-kvm被安装了之后的系统状态。
</code></pre>

<pre><code>[root@jay-linux qemu-kvm.git]# which qemu-system-x86_64
/usr/local/bin/qemu-system-x86_64
[root@jay-linux qemu-kvm.git]# which qemu-img
/usr/local/bin/qemu-img
[root@jay-linux qemu-kvm.git]# ls /usr/local/share/qemu/
bamboo.dtb        mpc8544ds.dtb     petalogix-ml605.dtb       pxe-pcnet.rom    slof.bin            vgabios-vmware.bin
bios.bin          multiboot.bin     petalogix-s3adsp1800.dtb  pxe-rtl8139.rom  spapr-rtas.bin
cpus-x86_64.conf  openbios-ppc      ppc_rom.bin               pxe-virtio.rom   vgabios.bin
keymaps           openbios-sparc32  pxe-e1000.rom             qemu-icon.bmp    vgabios-cirrus.bin
kvmvapic.bin      openbios-sparc64  pxe-eepro100.rom          s390-zipl.rom    vgabios-qxl.bin
linuxboot.bin     palcode-clipper   pxe-ne2k_pci.rom          sgabios.bin      vgabios-stdvga.bin
[root@jay-linux qemu-kvm.git]# ls /usr/local/share/qemu/keymaps/
ar    common  de     en-gb  es  fi  fr     fr-ca  hr  is  ja  lv  modifiers  nl-be  pl  pt-br  sl  th
bepo  da      de-ch  en-us  et  fo  fr-be  fr-ch  hu  it  lt  mk  nl         no     pt  ru     sv  tr
</code></pre>

<p>```</p>

<p>由于qemu-kvm是用户空间的程序，安装之后不用重启系统，直接用qemu-system-x86_64、qemu-img这样的命令行工具即可使用qemu-kvm了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KVM源代码分析4:内存虚拟化]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/07/29/kvm-src4/"/>
    <updated>2015-07-29T14:49:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/07/29/kvm-src4</id>
    <content type="html"><![CDATA[<p><a href="http://www.oenhan.com/kvm-src-4-mem">http://www.oenhan.com/kvm-src-4-mem</a></p>

<p>在虚拟机的创建与运行中pc_init_pci负责在qemu中初始化虚拟机，内存初始化也是在这里完成的，还是一步步从qemu说起，在vl.c的main函数中有ram_size参数，由qemu入参标识QEMU_OPTION_m设定，顾名思义就是虚拟机内存的大小，通过machine->init一步步传递给pc_init1函数。在这里分出了above_4g_mem_size和below_4g_mem_size，即高低端内存（也不一定是32bit机器..），然后开始初始化内存，即pc_memory_init，内存通过memory_region_init_ram下面的qemu_ram_alloc分配，使用qemu_ram_alloc_from_ptr。</p>

<p>插播qemu对内存条的模拟管理，是通过RAMBlock和ram_list管理的，RAMBlock就是每次申请的内存池，ram_list则是RAMBlock的链表，他们结构如下：</p>

<pre><code>    typedef struct RAMBlock {
        //对应宿主的内存地址
        uint8_t *host;
        //block在ramlist中的偏移
        ram_addr_t offset;
        //block长度
        ram_addr_t length;
        uint32_t flags;
        //block名字
        char idstr[256];
        QLIST_ENTRY(RAMBlock) next;
    #if defined(__linux__) &amp;&amp; !defined(TARGET_S390X)
        int fd;
    #endif
    } RAMBlock;

    typedef struct RAMList {
        //看代码理解就是list的head，但是不知道为啥叫dirty...
        uint8_t *phys_dirty;
        QLIST_HEAD(ram, RAMBlock) blocks;
    } RAMList;
</code></pre>

<p>下面再回到qemu_ram_alloc_from_ptr函数，使用find_ram_offset赋值给new block的offset，find_ram_offset具体工作模型已经在KVM源代码分析2:虚拟机的创建与运行中提到了，不赘述。然后是一串判断，在kvm_enabled的情况下使用new_block->host = kvm_vmalloc(size)，最终内存是qemu_vmalloc分配的，使用qemu_memalign干活。</p>

<pre><code>    void *qemu_memalign(size_t alignment, size_t size)
    {
        void *ptr;
        //使用posix进行内存针对页大小对齐
    #if defined(_POSIX_C_SOURCE) &amp;&amp; !defined(__sun__)
        int ret;
        ret = posix_memalign(&amp;ptr, alignment, size);
        if (ret != 0) {
            fprintf(stderr, "Failed to allocate %zu B: %s\n",
                    size, strerror(ret));
            abort();
        }
    #elif defined(CONFIG_BSD)
        ptr = qemu_oom_check(valloc(size));
    #else
        //所谓检查oom就是看memalign对应malloc申请内存是否成功
        ptr = qemu_oom_check(memalign(alignment, size));
    #endif
        trace_qemu_memalign(alignment, size, ptr);
        return ptr;
    }
</code></pre>

<p>以上qemu_vmalloc进行内存申请就结束了。在qemu_ram_alloc_from_ptr函数末尾则是将block添加到链表，realloc整个ramlist，用memset初始化整个ramblock，madvise对内存使用限定。
然后一层层的退回到pc_memory_init函数。</p>

<p>此时pc.ram已经分配完成，ram_addr已经拿到了分配的内存地址，MemoryRegion ram初始化完成。下面则是对已有的ram进行分段，即ram-below-4g和ram-above-4g，也就是高端内存和低端内存。用memory_region_init_alias初始化子MemoryRegion，然后将memory_region_add_subregion添加关联起来，memory_region_add_subregion具体细节“KVM源码分析2”中已经说了，参考对照着看吧，中间很多映射代码过程也只是qemu遗留的软件实现，没看到具体存在的意义，直接看到kvm_set_user_memory_region函数，内核真正需要kvm_vm_ioctl传递过去的参数是什么， struct kvm_userspace_memory_region mem而已，也就是</p>

<pre><code>    struct kvm_userspace_memory_region {
        __u32 slot;
        __u32 flags;
        __u64 guest_phys_addr;
        __u64 memory_size;    /* bytes */
        __u64 userspace_addr; /* start of the userspace allocated memory */
    };
</code></pre>

<p>kvm_vm_ioctl进入到内核是在KVM_SET_USER_MEMORY_REGION参数中，即执行kvm_vm_ioctl_set_memory_region，然后一直向下，到<code>__kvm_set_memory_region</code>函数，check_memory_region_flags检查mem->flags是否合法，而当前flag也就使用了两位，KVM_MEM_LOG_DIRTY_PAGES和KVM_MEM_READONLY，从qemu传递过来只能是KVM_MEM_LOG_DIRTY_PAGES,下面是对mem中各参数的合规检查，(mem->memory_size &amp; (PAGE_SIZE – 1))要求以页为单位，(mem->guest_phys_addr &amp; (PAGE_SIZE – 1))要求guest_phys_addr页对齐，而<code>((mem-&gt;userspace_addr &amp; (PAGE_SIZE – 1)) || !access_ok(VERIFY_WRITE,(void __user *)(unsigned long)mem-&gt;userspace_addr,mem-&gt;memory_size))</code>则保证host的线性地址页对齐而且该地址域有写权限。</p>

<p>id_to_memslot则是根据qemu的内存槽号得到kvm结构下的内存槽号，转换关系来自id_to_index数组，那映射关系怎么来的，映射关系是一一对应的，在kvm_create_vm虚拟机创建过程中，kvm_init_memslots_id初始化对应关系，即slots->id_to_index[i] = slots->memslots[i].id = i，当前映射是没有意义的，估计是为了后续扩展而存在的。</p>

<p>扩充了new的kvm_memory_slot，下面直接在代码中注释更方便：</p>

<pre><code>    //映射内存有大小，不是删除内存条
    if (npages) {
        //内存槽号没有虚拟内存条，意味内存新创建
        if (!old.npages)
            change = KVM_MR_CREATE;
        else { /* Modify an existing slot. */
            //修改已存在的内存修改标志或者平移映射地址
            //下面是不能处理的状态（内存条大小不能变，物理地址不能变，不能修改只读）
            if ((mem-&gt;userspace_addr != old.userspace_addr) ||
                (npages != old.npages) ||
                ((new.flags ^ old.flags) &amp; KVM_MEM_READONLY))
                goto out;
            //guest地址不同，内存条平移
            if (base_gfn != old.base_gfn)
                change = KVM_MR_MOVE;
            else if (new.flags != old.flags)
                //修改属性
                change = KVM_MR_FLAGS_ONLY;
            else { /* Nothing to change. */
                r = 0;
                goto out;
            }
        }
    } else if (old.npages) {
        //申请插入的内存为0，而内存槽上有内存，意味删除
        change = KVM_MR_DELETE;
    } else /* Modify a non-existent slot: disallowed. */
        goto out;
</code></pre>

<p>另外看kvm_mr_change就知道memslot的变动值了：</p>

<pre><code>    enum kvm_mr_change {
        KVM_MR_CREATE,
        KVM_MR_DELETE,
        KVM_MR_MOVE,
        KVM_MR_FLAGS_ONLY,
    };
</code></pre>

<p>在往下是一段检查</p>

<pre><code>    if ((change == KVM_MR_CREATE) || (change == KVM_MR_MOVE)) {
        /* Check for overlaps */
        r = -EEXIST;
        kvm_for_each_memslot(slot, kvm-&gt;memslots) {
            if ((slot-&gt;id &gt;= KVM_USER_MEM_SLOTS) ||
                //下面排除掉准备操作的内存条，在KVM_MR_MOVE中是有交集的
                (slot-&gt;id == mem-&gt;slot))
                continue;
            //下面就是当前已有的slot与new在guest线性区间上有交集
            if (!((base_gfn + npages &lt;= slot-&gt;base_gfn) ||
                  (base_gfn &gt;= slot-&gt;base_gfn + slot-&gt;npages)))
                goto out;
                //out错误码就是EEXIST
        }
    }
</code></pre>

<p>如果是新插入内存条，代码则走入kvm_arch_create_memslot函数，里面主要是一个循环，KVM_NR_PAGE_SIZES是分页的级数，此处是3，第一次循环，lpages = gfn_to_index(slot->base_gfn + npages – 1,slot->base_gfn, level) + 1，lpages就是一级页表所需要的page数，大致是npages>>0<em>9,然后为slot->arch.rmap[i]申请了内存空间，此处可以猜想，rmap就是一级页表了，继续看，lpages约为npages>>1</em>9,此处又多为lpage_info申请了同等空间，然后对lpage_info初始化赋值，现在看不到lpage_info的具体作用，看到后再补上。整体上看kvm_arch_create_memslot做了一个3级的软件页表。</p>

<p>如果有脏页,并且脏页位图为空,则分配脏页位图, kvm_create_dirty_bitmap实际就是”页数/8″.</p>

<pre><code>    if ((new.flags &amp; KVM_MEM_LOG_DIRTY_PAGES) &amp;&amp; !new.dirty_bitmap) {
        if (kvm_create_dirty_bitmap(&amp;new) &lt; 0)
            goto out_free;
    }
</code></pre>

<p>当内存条的改变是KVM_MR_DELETE或者KVM_MR_MOVE,先申请一个slots,把kvm->memslots暂存到这里,首先通过id_to_memslot获取准备插入的内存条对应到kvm的插槽是slot,无论删除还是移动,将其先标记为KVM_MEMSLOT_INVALID,然后是install_new_memslots,其实就是更新了一下slots->generation的值,</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KVM源代码分析3:CPU虚拟化]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/07/29/kvm-src3/"/>
    <updated>2015-07-29T14:48:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/07/29/kvm-src3</id>
    <content type="html"><![CDATA[<p><a href="http://www.oenhan.com/kvm-src-3-cpu">http://www.oenhan.com/kvm-src-3-cpu</a></p>

<p>在虚拟机的创建与运行章节里面笼统的介绍了KVM在qemu中的创建和运行，基本的qemu代码流程已经梳理清楚，后续主要写一些硬件虚拟化的原理和代码流程，主要写原理和qemu控制KVM运行的的ioctl接口，后续对内核代码的梳理也从这些接口下手。</p>

<h4>1.VT-x 技术</h4>

<p>Intel处理器支持的虚拟化技术即是VT-x，之所以CPU支持硬件虚拟化是因为软件虚拟化的效率太低。</p>

<p>处理器虚拟化的本质是分时共享，主要体现在状态恢复和资源隔离，实际上每个VM对于VMM看就是一个task么，之前Intel处理器在虚拟化上没有提供默认的硬件支持，传统 x86 处理器有4个特权级，Linux使用了0,3级别，0即内核，3即用户态，（更多参考CPU的运行环、特权级与保护）而在虚拟化架构上，虚拟机监控器的运行级别需要内核态特权级，而CPU特权级被传统OS占用，所以Intel设计了VT-x，提出了VMX模式，即VMX root operation 和 VMX non-root operation，虚拟机监控器运行在VMX root operation，虚拟机运行在VMX non-root operation。每个模式下都有相对应的0~3特权级。</p>

<p>为什么引入这两种特殊模式，在传统x86的系统中，CPU有不同的特权级，是为了划分不同的权限指令，某些指令只能由系统软件操作，称为特权指令，这些指令只能在最高特权级上才能正确执行，反之则会触发异常，处理器会陷入到最高特权级，由系统软件处理。还有一种需要操作特权资源（如访问中断寄存器）的指令，称为敏感指令。OS运行在特权级上，屏蔽掉用户态直接执行的特权指令，达到控制所有的硬件资源目的；而在虚拟化环境中，VMM控制所有所有硬件资源，VM中的OS只能占用一部分资源，OS执行的很多特权指令是不能真正对硬件生效的，所以原特权级下有了root模式，OS指令不需要修改就可以正常执行在特权级上，但这个特权级的所有敏感指令都会传递到root模式处理，这样达到了VMM的目的。</p>

<p>在KVM源代码分析1:基本工作原理章节中也说了kvm分3个模式，对应到VT-x 中即是客户模式对应vmx非root模式，内核模式对应VMX root模式下的0特权级，用户模式对应vmx root模式下的3特权级。</p>

<p>如下图<br/>
<img src="/images/system/kvm/2015-07-29-6.jpg" alt="" /></p>

<p>在非根模式下敏感指令引发的陷入称为VM-Exit，VM-Exit发生后，CPU从非根模式切换到根模式；对应的，VM-Entry则是从根模式到非根模式，通常意味着调用VM进入运行态。VMLAUCH/VMRESUME命令则是用来发起VM-Entry。</p>

<h4>2.VMCS</h4>

<p>VMCS保存虚拟机的相关CPU状态，每个VCPU都有一个VMCS（内存的），每个物理CPU都有VMCS对应的寄存器（物理的），当CPU发生VM-Entry时，CPU则从VCPU指定的内存中读取VMCS加载到物理CPU上执行，当发生VM-Exit时，CPU则将当前的CPU状态保存到VCPU指定的内存中，即VMCS，以备下次VMRESUME。</p>

<p>VMLAUCH指VM的第一次VM-Entry，VMRESUME则是VMLAUCH之后后续的VM-Entry。VMCS下有一些控制域：</p>

<table>
<tr>
    <td>
        VM-execution controls
    </td>
    <td>
        Determines what operations cause VM exits
    </td>
    <td>
        CR0, CR3, CR4, Exceptions, IO Ports, Interrupts, Pin Events, etc
    </td>
</tr>
<tr>
    <td>
        Guest-state area
    </td>
    <td>
        Saved on VM exits，Reloaded on VM entry
    </td>
    <td>
        EIP, ESP, EFLAGS, IDTR, Segment Regs, Exit info, etc
    </td>
</tr>
<tr>
    <td>
        Host-state area
    </td>
    <td>
        Loaded on VM exits
    </td>
    <td>
        CR3, EIP set to monitor entry point, EFLAGS hardcoded, etc
    </td>
</tr>
<tr>
    <td>
        VM-exit controls
    </td>
    <td>
        Determines which state to save, load, how to transition
    </td>
    <td>
        Example: MSR save-load list  
    </td>
</tr>
<tr>
    <td>
        VM-entry controls
    </td>
    <td>
        Determines which state to load, how to transition
    </td>
    <td>
        Including injecting events (interrupts, exceptions) on entry
    </td>
</tr>
</table>


<p>关于具体控制域的细节，还是翻Intel手册吧。</p>

<h4>3.VM-Entry/VM-Exit</h4>

<p>VM-Entry是从根模式切换到非根模式，即VMM切换到guest上，这个状态由VMM发起，发起之前先保存VMM中的关键寄存器内容到VMCS中，然后进入到VM-Entry，VM-Entry附带参数主要有3个：1.guest是否处于64bit模式，2.MSR VM-Entry控制，3.注入事件。1应该只在VMLAUCH有意义，3更多是在VMRESUME，而VMM发起VM-Entry更多是因为3，2主要用来每次更新MSR。</p>

<p>VM-Exit是CPU从非根模式切换到根模式，从guest切换到VMM的操作，VM-Exit触发的原因就很多了，执行敏感指令，发生中断，模拟特权资源等。</p>

<p>运行在非根模式下的敏感指令一般分为3个方面：</p>

<p>1.行为没有变化的，也就是说该指令能够正确执行。</p>

<p>2.行为有变化的，直接产生VM-Exit。</p>

<p>3.行为有变化的，但是是否产生VM-Exit受到VM-Execution控制域控制。</p>

<p>主要说一下”受到VM-Execution控制域控制”的敏感指令，这个就是针对性的硬件优化了，一般是1.产生VM-Exit；2.不产生VM-Exit，同时调用优化函数完成功能。典型的有“RDTSC指令”。除了大部分是优化性能的，还有一小部分是直接VM-Exit执行指令结果是异常的，或者说在虚拟化场景下是不适用的，典型的就是TSC offset了。</p>

<p>VM-Exit发生时退出的相关信息，如退出原因、触发中断等，这些内容保存在VM-Exit信息域中。</p>

<h4>4.KVM_CREATE_VM</h4>

<p>创建VM就写这里吧，kvm_dev_ioctl_create_vm函数是主干，在kvm_create_vm中，主要有两个函数，kvm_arch_init_vm和hardware_enable_all，需要注意，但是更先一步的是KVM结构体，下面的struct是精简后的版本。</p>

<pre><code>    struct kvm {
        struct mm_struct *mm; /* userspace tied to this vm */
        struct kvm_memslots *memslots;  /*qemu模拟的内存条模型*/
        struct kvm_vcpu *vcpus[KVM_MAX_VCPUS]; /* 模拟的CPU */
        atomic_t online_vcpus;
        int last_boosted_vcpu;
        struct list_head vm_list;  //HOST上VM管理链表，
        struct kvm_io_bus *buses[KVM_NR_BUSES];
        struct kvm_vm_stat stat;
        struct kvm_arch arch; //这个是host的arch的一些参数
        atomic_t users_count;

        long tlbs_dirty;
        struct list_head devices;
    };
</code></pre>

<p>kvm_arch_init_vm基本没有特别动作，初始化了KVM->arch，以及更新了kvmclock函数，这个另外再说。</p>

<p>而hardware_enable_all，针对于每个CPU执行“on_each_cpu(hardware_enable_nolock, NULL, 1）”，在hardware_enable_nolock中先把cpus_hardware_enabled置位，进入到kvm_arch_hardware_enable中，有hardware_enable和TSC初始化规则，主要看hardware_enable，crash_enable_local_vmclear清理位图，判断MSR_IA32_FEATURE_CONTROL寄存器是否满足虚拟环境，不满足则将条件写入到寄存器内，CR4将X86_CR4_VMXE置位，另外还有kvm_cpu_vmxon打开VMX操作模式，外层包了vmm_exclusive的判断，它是kvm_intel.ko的外置参数，默认唯一，可以让用户强制不使用VMM硬件支持。</p>

<h4>5.KVM_CREATE_VCPU</h4>

<p>kvm_vm_ioctl_create_vcpu主要有三部分，kvm_arch_vcpu_create，kvm_arch_vcpu_setup和kvm_arch_vcpu_postcreate，重点自然是kvm_arch_vcpu_create。老样子，在这之前先看一下VCPU的结构体。</p>

<pre><code>    struct kvm_vcpu {
        struct kvm *kvm;  //归属的KVM
    #ifdef CONFIG_PREEMPT_NOTIFIERS
        struct preempt_notifier preempt_notifier;
    #endif
        int cpu;
        int vcpu_id;
        int srcu_idx;
        int mode;
        unsigned long requests;
        unsigned long guest_debug;

        struct mutex mutex;
        struct kvm_run *run;  //运行时的状态

        int fpu_active;
        int guest_fpu_loaded, guest_xcr0_loaded;
        wait_queue_head_t wq; //队列
        struct pid *pid;
        int sigset_active;
        sigset_t sigset;
        struct kvm_vcpu_stat stat; //一些数据

    #ifdef CONFIG_HAS_IOMEM
        int mmio_needed;
        int mmio_read_completed;
        int mmio_is_write;
        int mmio_cur_fragment;
        int mmio_nr_fragments;
        struct kvm_mmio_fragment mmio_fragments[KVM_MAX_MMIO_FRAGMENTS];
    #endif

    #ifdef CONFIG_KVM_ASYNC_PF
        struct {
            u32 queued;
            struct list_head queue;
            struct list_head done;
            spinlock_t lock;
        } async_pf;
    #endif

    #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
        /*
         * Cpu relax intercept or pause loop exit optimization
         * in_spin_loop: set when a vcpu does a pause loop exit
         *  or cpu relax intercepted.
         * dy_eligible: indicates whether vcpu is eligible for directed yield.
         */
        struct {
            bool in_spin_loop;
            bool dy_eligible;
        } spin_loop;
    #endif
        bool preempted;
        struct kvm_vcpu_arch arch;  //当前VCPU虚拟的架构，默认介绍X86
    };
</code></pre>

<p>借着看kvm_arch_vcpu_create，它借助kvm_x86_ops->vcpu_create即vmx_create_vcpu完成任务，vmx是X86硬件虚拟化层，从代码看，qemu用户态是一层，kernel 中KVM通用代码是一层，类似kvm_x86_ops是一层，针对各个不同硬件架构，而vcpu_vmx则是具体架构的虚拟化方案一层。首先是kvm_vcpu_init初始化，主要是填充结构体，可以注意的是vcpu->run分派了一页内存，下面有kvm_arch_vcpu_init负责填充x86 CPU结构体，下面就是kvm_vcpu_arch：</p>

<pre><code>    struct kvm_vcpu_arch {
        /*
         * rip and regs accesses must go through
         * kvm_{register,rip}_{read,write} functions.
         */
        unsigned long regs[NR_VCPU_REGS];
        u32 regs_avail;
        u32 regs_dirty;
        //类似这些寄存器就是就是用来缓存真正的CPU值的
        unsigned long cr0;
        unsigned long cr0_guest_owned_bits;
        unsigned long cr2;
        unsigned long cr3;
        unsigned long cr4;
        unsigned long cr4_guest_owned_bits;
        unsigned long cr8;
        u32 hflags;
        u64 efer;
        u64 apic_base;
        struct kvm_lapic *apic;    /* kernel irqchip context */
        unsigned long apic_attention;
        int32_t apic_arb_prio;
        int mp_state;
        u64 ia32_misc_enable_msr;
        bool tpr_access_reporting;
        u64 ia32_xss;

        /*
         * Paging state of the vcpu
         *
         * If the vcpu runs in guest mode with two level paging this still saves
         * the paging mode of the l1 guest. This context is always used to
         * handle faults.
         */
        struct kvm_mmu mmu; //内存管理，更多的是附带了直接操作函数

        /*
         * Paging state of an L2 guest (used for nested npt)
         *
         * This context will save all necessary information to walk page tables
         * of the an L2 guest. This context is only initialized for page table
         * walking and not for faulting since we never handle l2 page faults on
         * the host.
         */
        struct kvm_mmu nested_mmu;

        /*
         * Pointer to the mmu context currently used for
         * gva_to_gpa translations.
         */
        struct kvm_mmu *walk_mmu;

        struct kvm_mmu_memory_cache mmu_pte_list_desc_cache;
        struct kvm_mmu_memory_cache mmu_page_cache;
        struct kvm_mmu_memory_cache mmu_page_header_cache;

        struct fpu guest_fpu;
        u64 xcr0;
        u64 guest_supported_xcr0;
        u32 guest_xstate_size;

        struct kvm_pio_request pio;
        void *pio_data;

        u8 event_exit_inst_len;

        struct kvm_queued_exception {
            bool pending;
            bool has_error_code;
            bool reinject;
            u8 nr;
            u32 error_code;
        } exception;

        struct kvm_queued_interrupt {
            bool pending;
            bool soft;
            u8 nr;
        } interrupt;

        int halt_request; /* real mode on Intel only */

        int cpuid_nent;
        struct kvm_cpuid_entry2 cpuid_entries[KVM_MAX_CPUID_ENTRIES];

        int maxphyaddr;

        /* emulate context */
        //下面是KVM的软件模拟模式，也就是没有vmx的情况，估计也没人用这一套
        struct x86_emulate_ctxt emulate_ctxt;
        bool emulate_regs_need_sync_to_vcpu;
        bool emulate_regs_need_sync_from_vcpu;
        int (*complete_userspace_io)(struct kvm_vcpu *vcpu);

        gpa_t time;
        struct pvclock_vcpu_time_info hv_clock;
        unsigned int hw_tsc_khz;
        struct gfn_to_hva_cache pv_time;
        bool pv_time_enabled;
        /* set guest stopped flag in pvclock flags field */
        bool pvclock_set_guest_stopped_request;

        struct {
            u64 msr_val;
            u64 last_steal;
            u64 accum_steal;
            struct gfn_to_hva_cache stime;
            struct kvm_steal_time steal;
        } st;

        u64 last_guest_tsc;
        u64 last_host_tsc;
        u64 tsc_offset_adjustment;
        u64 this_tsc_nsec;
        u64 this_tsc_write;
        u64 this_tsc_generation;
        bool tsc_catchup;
        bool tsc_always_catchup;
        s8 virtual_tsc_shift;
        u32 virtual_tsc_mult;
        u32 virtual_tsc_khz;
        s64 ia32_tsc_adjust_msr;

        atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
        unsigned nmi_pending; /* NMI queued after currently running handler */
        bool nmi_injected;    /* Trying to inject an NMI this entry */

        struct mtrr_state_type mtrr_state;
        u64 pat;

        unsigned switch_db_regs;
        unsigned long db[KVM_NR_DB_REGS];
        unsigned long dr6;
        unsigned long dr7;
        unsigned long eff_db[KVM_NR_DB_REGS];
        unsigned long guest_debug_dr7;

        u64 mcg_cap;
        u64 mcg_status;
        u64 mcg_ctl;
        u64 *mce_banks;

        /* Cache MMIO info */
        u64 mmio_gva;
        unsigned access;
        gfn_t mmio_gfn;
        u64 mmio_gen;

        struct kvm_pmu pmu;

        /* used for guest single stepping over the given code position */
        unsigned long singlestep_rip;

        /* fields used by HYPER-V emulation */
        u64 hv_vapic;

        cpumask_var_t wbinvd_dirty_mask;

        unsigned long last_retry_eip;
        unsigned long last_retry_addr;

        struct {
            bool halted;
            gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];
            struct gfn_to_hva_cache data;
            u64 msr_val;
            u32 id;
            bool send_user_only;
        } apf;

        /* OSVW MSRs (AMD only) */
        struct {
            u64 length;
            u64 status;
        } osvw;

        struct {
            u64 msr_val;
            struct gfn_to_hva_cache data;
        } pv_eoi;

        /*
         * Indicate whether the access faults on its page table in guest
         * which is set when fix page fault and used to detect unhandeable
         * instruction.
         */
        bool write_fault_to_shadow_pgtable;

        /* set at EPT violation at this point */
        unsigned long exit_qualification;

        /* pv related host specific info */
        struct {
            bool pv_unhalted;
        } pv;
    };
</code></pre>

<p>整个arch结构真是长，很适合凑篇幅，很多结构其他过程涉及到的再提吧，反正我也不知道。</p>

<p>kvm_arch_vcpu_init初始化了x86在虚拟化底层的实现函数，首先是pv和emulate_ctxt，这些不支持VMX下的模拟虚拟化，尤其是vcpu->arch.emulate_ctxt.ops = &amp;emulate_ops，emulate_ops初始化虚拟化模拟的对象函数。</p>

<pre><code class="">    static struct x86_emulate_ops emulate_ops = {
        .read_std            = kvm_read_guest_virt_system,
        .write_std           = kvm_write_guest_virt_system,
        .fetch               = kvm_fetch_guest_virt,
        .read_emulated       = emulator_read_emulated,
        .write_emulated      = emulator_write_emulated,
        .cmpxchg_emulated    = emulator_cmpxchg_emulated,
        .invlpg              = emulator_invlpg,
        .pio_in_emulated     = emulator_pio_in_emulated,
        .pio_out_emulated    = emulator_pio_out_emulated,
        .get_segment         = emulator_get_segment,
        .set_segment         = emulator_set_segment,
        .get_cached_segment_base = emulator_get_cached_segment_base,
        .get_gdt             = emulator_get_gdt,
        .get_idt         = emulator_get_idt,
        .set_gdt             = emulator_set_gdt,
        .set_idt         = emulator_set_idt,
        .get_cr              = emulator_get_cr,
        .set_cr              = emulator_set_cr,
        .cpl                 = emulator_get_cpl,
        .get_dr              = emulator_get_dr,
        .set_dr              = emulator_set_dr,
        .set_msr             = emulator_set_msr,
        .get_msr             = emulator_get_msr,
        .halt                = emulator_halt,
        .wbinvd              = emulator_wbinvd,
        .fix_hypercall       = emulator_fix_hypercall,
        .get_fpu             = emulator_get_fpu,
        .put_fpu             = emulator_put_fpu,
        .intercept           = emulator_intercept,
        .get_cpuid           = emulator_get_cpuid,
    };
</code></pre>

<p>x86_emulate_ops函数看看就好，实际上也很少有人放弃vmx直接软件模拟。后面又有mp_state，给pio_data分配了一个page，kvm_set_tsc_khz设置TSC，kvm_mmu_create则是初始化MMU的函数，里面的函数都是地址转换的重点，在内存虚拟化重点提到。kvm_create_lapic初始化lapic，初始化mce_banks结构，还有pv_time,xcr0,xstat,pmu等，类似x86硬件结构上需要存在的，OS底层需要看到的硬件名称都要有对应的软件结构。</p>

<p>回到vmx_create_vcpu，vmx的guest_msrs分配得到一个page，后面是vmcs的分配，vmx->loaded_vmcs->vmcs = alloc_vmcs()，alloc_vmcs为当前cpu执行alloc_vmcs_cpu，alloc_vmcs_cpu中alloc_pages_exact_node分配给vmcs，alloc_pages_exact_node调用<code>__alloc_pages</code>实现，原来以为vmcs占用了一个page，但此处从伙伴系统申请了2<sup>vmcs</sup>_config.order页，此处vmcs_config在setup_vmcs_config中初始化，vmcs_conf->order = get_order(vmcs_config.size)，而vmcs_conf->size = vmx_msr_high &amp; 0x1fff，又rdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high)，此处size由于与0x1fff与运算，大小必然小于4k，order则为0，然来绕去还是一个page大小。这么做估计是为了兼容vmcs_config中的size计算。</p>

<p>下面根据vmm_exclusive进行kvm_cpu_vmxon，进入vmx模式，初始化loaded_vmcs，然后用kvm_cpu_vmxoff退出vmx模式。</p>

<p>vmx_vcpu_load加载VCPU的信息，切换到指定cpu，进入到vmx模式，将loaded_vmcs的vmcs和当前cpu的vmcs绑定到一起。vmx_vcpu_setup则是初始化vmcs内容，主要是赋值计算，下面的vmx_vcpu_put则是vmx_vcpu_load的反运算。下面还有一些apic，nested，pml就不说了。</p>

<p>vmx_create_vcpu结束就直接回到kvm_vm_ioctl_create_vcpu函数，下面是kvm_arch_vcpu_setup，整个就一条线到kvm_arch_vcpu_load函数，主要有kvm_x86_ops->vcpu_load(vcpu, cpu)和tsc处理，vcpu_load就是vmx_vcpu_load，刚说了，就是进入vcpu模式下准备工作。</p>

<p>kvm_arch_vcpu_setup后面是create_vcpu_fd为proc创建控制fd，让qemu使用。kvm_arch_vcpu_postcreate则是马后炮般，重新vcpu_load，写msr，tsc。</p>

<p>如此整个vcpu就创建完成了。</p>

<h4>6.KVM_RUN</h4>

<p>KVM run涉及内容也不少，先写完内存虚拟化之后再开篇专门写RUN流程。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KVM源代码分析2:虚拟机的创建与运行]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/07/29/kvm-src2/"/>
    <updated>2015-07-29T14:42:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/07/29/kvm-src2</id>
    <content type="html"><![CDATA[<p><a href="http://www.oenhan.com/kvm-src-2-vm-run">http://www.oenhan.com/kvm-src-2-vm-run</a></p>

<p>基本原理里面提到kvm虚拟化由用户态程序Qemu和内核态驱动kvm配合完成，qemu负责HOST用户态层面进程管理，IO处理等，KVM负责把qemu的部分指令在硬件上直接实现，从虚拟机的创建和运行上看，qemu的代码占了流程上的主要部分。下面的代码主要主要针对与qemu。 而Qemu和kvm的配合流程如下：</p>

<p><img src="/images/system/kvm/2015-07-29-3.png" alt="" /></p>

<p>接下来参考上图分析qemu代码流程： 从vl.c代码的main函数开始。 atexit(qemu_run_exit_notifiers)注册了qemu的退出处理函数，后面在具体看qemu_run_exit_notifiers函数。 module_call_init则开始初始化qemu的各个模块，陆陆续续的有以下参数：</p>

<pre><code>    typedef enum {
        MODULE_INIT_BLOCK,
        MODULE_INIT_MACHINE,
        MODULE_INIT_QAPI,
        MODULE_INIT_QOM,
        MODULE_INIT_MAX
    } module_init_type;
</code></pre>

<p>最开始初始化的MODULE_INIT_QOM，QOM是qemu实现的一种模拟设备，具体可以参考 <a href="http://wiki.qemu.org/Features/QOM">http://wiki.qemu.org/Features/QOM</a> ，代码下面的不远处就MODULE_INIT_MACHINE的初始化，这两条语句放到一起看，直接说一下module_call_init的机制。 module_call_init实际设计的一个函数链表，ModuleTypeList ，链表关系如下图</p>

<p><img src="/images/system/kvm/2015-07-29-4.png" alt="" /></p>

<p>它把相关的函数注册到对应的数组链表上，通过执行init项目完成所有设备的初始化。module_call_init就是执行e->init()完成功能的，而e->init是什么时候通过register_module_init注册到ModuleTypeList上的ModuleEntry，是在machine_init(pc_machine_init)函数注册的，pc_machine_init则是针对PC（即是X86）的qemu虚拟化方案，至于它被谁调用的，把machine_init这个宏展开，看到它前面的修饰是<code>__attribute__((constructor))</code>,这个导致machine_init或者type_init等会在main()之前就被执行。module_call_init针对X86则是调用machine_init，即pc_machine_init，完成了虚拟的机器类型注册。</p>

<pre><code>    static void pc_machine_init(void)
    {
        qemu_register_machine(&amp;pc_machine_v1_3);
        qemu_register_machine(&amp;pc_machine_v1_2);
        qemu_register_machine(&amp;pc_machine_v1_1);
        qemu_register_machine(&amp;pc_machine_v1_0);
        qemu_register_machine(&amp;pc_machine_v1_0_qemu_kvm);
        qemu_register_machine(&amp;pc_machine_v0_15);
        qemu_register_machine(&amp;pc_machine_v0_14);
        qemu_register_machine(&amp;pc_machine_v0_13);
        qemu_register_machine(&amp;pc_machine_v0_12);
        qemu_register_machine(&amp;pc_machine_v0_11);
        qemu_register_machine(&amp;pc_machine_v0_10);
    }
    machine_init(pc_machine_init);
</code></pre>

<p>下面涉及对OPT入参的解析过程略过不提。 qemu准备模拟的机器的类型从下面语句获得:</p>

<pre><code>    current_machine = MACHINE(object_new(object_class_get_name(
                          OBJECT_CLASS(machine_class))));
</code></pre>

<p>machine_class则是通过入参传入的</p>

<pre><code>    case QEMU_OPTION_machine:
        olist = qemu_find_opts("machine");
        opts = qemu_opts_parse(olist, optarg, 1);
        if (!opts) {
            exit(1);
        }
        optarg = qemu_opt_get(opts, "type");
        if (optarg) {
            machine_class = machine_parse(optarg);
        }
        break;
</code></pre>

<p>man qemu</p>

<pre><code>    -machine [type=]name[,prop=value[,...]]
        Select the emulated machine by name.
        Use "-machine help" to list available machines
</code></pre>

<p>cpu_exec_init_all中记录了CPU执行前的一些初始化工作。</p>

<p>qemu_set_log设置日志输出，kvm对外的日志是从这里配置的。</p>

<p>中间的代码忽略过，直接到configure_accelerator函数，进行虚拟机模拟器的配置， 这是一个重点关注的函数，它调用了accel_list[i].init()函数，而accel_list初始化如下：</p>

<pre><code>    static struct {
        const char *opt_name;
        const char *name;
        int (*available)(void);
        int (*init)(QEMUMachine *);
        bool *allowed;
    } accel_list[] = {
        { "tcg", "tcg", tcg_available, tcg_init, &amp;tcg_allowed },
        { "xen", "Xen", xen_available, xen_init, &amp;xen_allowed },
        { "kvm", "KVM", kvm_available, kvm_init, &amp;kvm_allowed },
        { "qtest", "QTest", qtest_available, qtest_init_accel, &amp;qtest_allowed },
    };
</code></pre>

<p>kvm_available很简单，重点在kvm_init上，实际调用kvm_init函数，kvm_init通过qemu_open(“/dev/kvm”)检查内核驱动插入情况，通过kvm_ioctl(s, KVM_GET_API_VERSION, 0)获取API接口版本，最重点是调用了kvm_ioctl(s, KVM_CREATE_VM, type);创建了KVM虚拟机，获取虚拟机句柄。具体KVM_CREATE_VM在内核态做了什么，ioctl的工作等另外再说，现在假定KVM_CREATE_VM所代表的虚拟机创建成功，下面通过检查kvm_check_extension结果填充KVMState，kvm_arch_init初始化KVMState，其中有IDENTITY_MAP_ADDR，TSS_ADDR，NR_MMU_PAGES等，cpu_register_phys_memory_client注册qemu对内存管理的函数集，kvm_create_irqchip创建kvm中断管理内容，通过kvm_vm_ioctl(s, KVM_CREATE_IRQCHIP)实现，具体内核态的工作内容后面分析。到此模拟器init的工作就完成了，最主要的工作就是创建的虚拟机。</p>

<p>下面是guest启动的内核配置，qemu线程的初始化等，涉及虚拟机CPU，内存初始化在下面：</p>

<pre><code>    QEMUMachineInitArgs args = { .machine = machine,
                    .ram_size = ram_size,
                    .boot_order = boot_order,
                    .kernel_filename = kernel_filename,
                    .kernel_cmdline = kernel_cmdline,
                    .initrd_filename = initrd_filename,
                    .cpu_model = cpu_model };

    current_machine-&gt;init_args = args;
    machine-&gt;init(&amp;current_machine-&gt;init_args);
</code></pre>

<p>前面提到了pc_machine_init注册虚拟机器类型，我们直接看pc_machine_v1_0_qemu_kvm即可，QEMUMachine对应的结构如下:</p>

<pre><code>    static QEMUMachine pc_machine_v1_0_qemu_kvm = {
        PC_I440FX_1_2_MACHINE_OPTIONS,
        .name = "pc-1.0-qemu-kvm",
        .alias = "pc-1.0-precise",
        .init = pc_init_pci_1_2_qemu_kvm,
        .compat_props = (GlobalProperty[]) {
            PC_COMPAT_1_0_QEMU_KVM,
            { /* end of list */ }
        },
        .hw_version = "1.0",
    };
</code></pre>

<p>init函数是pc_init_pci_1_2_qemu_kvm,去除中间的一些兼容性代码工作，流程就是pc_init_pci->pc_init1。</p>

<p>在pc_init1中重点看两个函数，pc_cpus_init和pc_memory_init，顾名思义，CPU和内存的初始化，中断等初始化先忽略掉，先看这两个。</p>

<p>pc_cpus_init首先for循环中针对每个CPU初始化，即pc_new_cpu，里面有cpu_x86_init函数，主要就是把CPUX86State填充了一下，涉及到CPUID和其他的feature。下面是x86_cpu_realize，即唤醒CPU，重点是qemu_init_vcpu，MCE忽略掉，走到qemu_kvm_start_vcpu，qemu创建VCPU，如下：</p>

<pre><code>    //创建VPU对于的qemu线程，线程函数是qemu_kvm_cpu_thread_fn
    qemu_thread_create(cpu-&gt;thread, thread_name, qemu_kvm_cpu_thread_fn,
                       cpu, QEMU_THREAD_JOINABLE);
    //如果线程没有创建成功，则一直在此处循环阻塞。说明多核vcpu的创建是顺序的
    while (!cpu-&gt;created) {
        qemu_cond_wait(&amp;qemu_cpu_cond, &amp;qemu_global_mutex);
    }
</code></pre>

<p>线程创建完成，具体任务支线提，回到主流程上，qemu_init_vcpu执行完成后，下面就是cpu_reset，此处的作用是什么呢？答案是无用，本质是一个空函数，它的主要功能就是CPUClass的reset函数，reset在cpu_class_init里面注册的，注册的是cpu_common_reset，这是一个空函数，没有任何作用。cpu_class_init则是被cpu_type_info即TYPE_CPU使用，而cpu_type_info则由type_init(cpu_register_types)完成，type_init则是前面提到的和machine_init对应的注册关系。根据下句完成工作</p>

<pre><code>    #define type_init(function) module_init(function, MODULE_INIT_QOM)
</code></pre>

<p>从上面看，pc_cpus_init函数过程已经理顺了，下面看一下，vcpu所在的线程对应的qemu_kvm_cpu_thread_fn中：</p>

<pre><code>    //初始化VCPU
    r = kvm_init_vcpu(env);
    //初始化KVM中断
    qemu_kvm_init_cpu_signals(env);

    //标志VCPU创建完成，和上面判断是对应的
    cpu-&gt;created = true;
    qemu_cond_signal(&amp;qemu_cpu_cond);
    while (1) {
        if (cpu_can_run(env)) {
            //CPU进入执行状态
            r = kvm_cpu_exec(env);
            if (r == EXCP_DEBUG) {
                cpu_handle_guest_debug(env);
            }
        }
        qemu_kvm_wait_io_event(env);
    }
</code></pre>

<p>CPU进入执行状态的时候我们看到其他的VCPU包括内存可能还没有初始化，关键是此处有一个开关，qemu_cpu_cond,打开这个开关才能进入到CPU执行状态，谁来打开这个开关，后面再说。先看kvm_init_vcpu，通过kvm_vm_ioctl，KVM_CREATE_VCPU创建VCPU，用KVM_GET_VCPU_MMAP_SIZE获取env->kvm_run对应的内存映射，kvm_arch_init_vcpu则填充对应的kvm_arch内容，具体内核部分，后面单独写。kvm_init_vcpu就是获取了vcpu，将相关内容填充了env。</p>

<p>qemu_kvm_init_cpu_signals则是将中断组合掩码传递给kvm_set_signal_mask，最终给内核KVM_SET_SIGNAL_MASK。kvm_cpu_exec此时还在阻塞过程中，先挂起来，看内存的初始化。
内存初始化函数是pc_memory_init,memory_region_init_ram传入了高端内存和低端内存的值，memory_region_init负责填充mr，重点在qemu_ram_alloc，即qemu_ram_alloc_from_ptr，首先有RAMBlock，ram_list，那就直接借助find_ram_offset函数一起看一下qemu的内存分布模型。</p>

<p><img src="/images/system/kvm/2015-07-29-5.png" alt="" /></p>

<p>qemu模拟了普通内存分布模型，内存的线性也是分块被使用的，每个块称为RAMBlock，由ram_list统领，RAMBlock.offset则是区块的线性地址，即相对于开始的偏移位，RAMBlock.length(size)则是区块的大小，find_ram_offset则是在线性区间内找到没有使用的一段空间，可以完全容纳新申请的ramblock length大小，代码就是进行了所有区块的遍历，找到满足新申请length的最小区间，把ramblock安插进去即可，返回的offset即是新分配区间的开始地址。</p>

<p>而RAMBlock的物理则是在RAMBlock.host,由kvm_vmalloc(size)分配真正物理内存，内部qemu_vmalloc使用qemu_memalign页对齐分配内存。后续的都是对RAMBlock的插入等处理。
从上面看，memory_region_init_ram已经将qemu内存模型和实际的物理内存初始化了。</p>

<p>vmstate_register_ram_global这个函数则是负责将前面提到的ramlist中的ramblock和memory region的初始地址对应一下，将mr->name填充到ramblock的idstr里面，就是让二者有确定的对应关系，如此mr就有了物理内存使用。</p>

<p>后面则是subregion的处理，memory_region_init_alias初始化，其中将ram传递给mr->owner确定了隶属关系，memory_region_add_subregion则是大头，memory_region_add_subregion_common前面的判断忽略，QTAILQ_INSERT_TAIL(&amp;mr->subregions, subregion, subregions_link)就是插入了链表而已，主要内容在memory_region_transaction_commit。</p>

<p>memory_region_transaction_commit中引入了新的结构address_spaces（AS），注释里面提到“AddressSpace: describes a mapping of addresses to #MemoryRegion objects”，就是内存地址的映射关系，因为内存有不同的应用类型，address_spaces以链表形式存在，commit函数则是对所有AS执行address_space_update_topology，先看AS在哪里注册的，就是前面提到的kvm_init里面，执行memory_listener_register，注册了address_space_memory和address_space_io两个，涉及的另外一个结构体则是MemoryListener，有kvm_memory_listener和kvm_io_listener，就是用于监控内存映射关系发生变化之后执行回调函数。</p>

<p>下面进入到address_space_update_topology函数，FlatView则是“Flattened global view of current active memory hierarchy”，address_space_get_flatview直接获取当前的，generate_memory_topology则根据前面已经变化的mr重新生成FlatView,然后通过address_space_update_topology_pass比较，简单说address_space_update_topology_pass就是两个FlatView逐条的FlatRange进行对比，以后一个FlatView为准，如果前面FlatView的FlatRange和后面的不一样，则对前面的FlatView的这条FlatRange进行处理，差别就是3种情况，如代码：</p>

<pre><code>    while (iold &lt; old_view-&gt;nr || inew &lt; new_view-&gt;nr) {
       if (iold &lt; old_view-&gt;nr) {
           frold = &amp;old_view-&gt;ranges[iold];
       } else {
           frold = NULL;
       }
       if (inew &lt; new_view-&gt;nr) {
           frnew = &amp;new_view-&gt;ranges[inew];
       } else {
           frnew = NULL;
       }

       if (frold
           &amp;&amp; (!frnew
               || int128_lt(frold-&gt;addr.start, frnew-&gt;addr.start)
               || (int128_eq(frold-&gt;addr.start, frnew-&gt;addr.start)
                   &amp;&amp; !flatrange_equal(frold, frnew)))) {
           /* In old but not in new, or in both but attributes changed. */

           if (!adding) { //这个判断代码添加的无用，可以直接删除,
               //address_space_update_topology里面的两个pass也可以删除一个
               MEMORY_LISTENER_UPDATE_REGION(frold, as, Reverse, region_del);
           }

           ++iold;
       } else if (frold &amp;&amp; frnew &amp;&amp; flatrange_equal(frold, frnew)) {
           /* In both and unchanged (except logging may have changed) */

           if (adding) {
               MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_nop);
               if (frold-&gt;dirty_log_mask &amp;&amp; !frnew-&gt;dirty_log_mask) {
                   MEMORY_LISTENER_UPDATE_REGION(frnew, as, Reverse, log_stop);
               } else if (frnew-&gt;dirty_log_mask &amp;&amp; !frold-&gt;dirty_log_mask) {
                   MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, log_start);
               }
           }

           ++iold;
           ++inew;
       } else {
           /* In new */

           if (adding) {
               MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_add);
           }

           ++inew;
       }
   }
</code></pre>

<p>重点在MEMORY_LISTENER_UPDATE_REGION函数上，将变化的FlatRange构造一个MemoryRegionSection，然后遍历所有的memory_listeners，如果memory_listeners监控的内存区域和MemoryRegionSection一样，则执行第四个入参函数，如region_del函数，即kvm_region_del函数，这个是在kvm_init中初始化的。kvm_region_del主要是kvm_set_phys_mem函数，主要是将MemoryRegionSection有效值转换成KVMSlot形式，在kvm_set_user_memory_region中使用kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &amp;mem)传递给kernel。</p>

<p>我们看内存初始化真正需要做的是什么？就是qemu申请内存，把申请物理地址传递给kernel进行映射，那我们直接就可以KVMSlot申请内存，然后传递给kvm_vm_ioctl，这样也是OK的，之所以有这么多代码，因为qemu本身是一个软件虚拟机，mr涉及的地址已经是vm的地址，对于KVM是多余的，只是方便函数复用而已。</p>

<p>内存初始化之后还是pci等处理先跳过，如此pc_init就完成了，但是前面VM线程已经初始化成功，在qemu_kvm_cpu_thread_fn函数中等待运行：</p>

<pre><code>    while (1) {
        if (cpu_can_run(cpu)) {
            r = kvm_cpu_exec(cpu);
            if (r == EXCP_DEBUG) {
                cpu_handle_guest_debug(cpu);
            }
        }
        qemu_kvm_wait_io_event(cpu);
    }
</code></pre>

<p>判断条件就是cpu_can_run函数，即cpu->stop &amp;&amp; cpu->stopped &amp;&amp; current_run_state ！= running 都是false，而这几个参数都是由vm_start函数决定的</p>

<pre><code>    void vm_start(void)
    {
        if (!runstate_is_running()) {
            cpu_enable_ticks();
            runstate_set(RUN_STATE_RUNNING);
            vm_state_notify(1, RUN_STATE_RUNNING);
            resume_all_vcpus();
            monitor_protocol_event(QEVENT_RESUME, NULL);
        }
    }
</code></pre>

<p>如此kvm_cpu_exec就真正进入执行阶段，即通过kvm_vcpu_ioctl传递KVM_RUN给内核。</p>
]]></content>
  </entry>
  
</feed>
