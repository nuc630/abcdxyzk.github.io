<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel~net | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel~net/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2016-01-12T16:14:03+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Web压力测试工具]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/12/27/kernel-net-test-tool/"/>
    <updated>2015-12-27T02:51:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/12/27/kernel-net-test-tool</id>
    <content type="html"><![CDATA[<p><a href="http://297020555.blog.51cto.com/1396304/592386">http://297020555.blog.51cto.com/1396304/592386</a></p>

<h4>一、http_load</h4>

<p>http_load以并行复用的方式运行，用以测试web服务器的吞吐量与负载。但是它不同于大多数压力测试工具，它可以以一个单一的进程运行，一般不会把客户机搞死。还可以测试HTTPS类的网站请求。</p>

<p>下载地址：<a href="http://www.acme.com/software/http_load/">http://www.acme.com/software/http_load/</a></p>

<pre><code>    ./http_load -verbose -proxy 192.168.99.6:80 -parallel 24 -seconds 1000 url.txt
</code></pre>

<h4>二、webbench</h4>

<p>webbench是Linux下的一个网站压力测试工具，最多可以模拟3万个并发连接去测试网站的负载能力。
<code>
    用法：webbench -c 并发数 -t 运行测试时间 URL
    如：webbench -c 5000 -t 120 http://www.163.com
</code></p>

<h4>三、ab</h4>

<p>ab是apache自带的一款功能强大的测试工具。安装了apache一般就自带了，用法可以查看它的说明</p>

<p>参数众多，一般我们用到的是-n 和-c</p>

<p>例如：
<code>
    ./ab -c 1000 -n 100 http://www.vpser.net/index.php
</code>
这个表示同时处理1000个请求并运行100次index.php文件.</p>

<h4>四、Siege</h4>

<p>一款开源的压力测试工具，可以根据配置对一个WEB站点进行多用户的并发访问，记录每个用户所有请求过程的相应时间，并在一定数量的并发访问下重复进行。
官方：<a href="http://www.joedog.org/">http://www.joedog.org/</a></p>

<p>使用
<code>
    siege -c 200 -r 10 -f example.url
</code></p>

<p>-c是并发量，-r是重复次数。 url文件就是一个文本，每行都是一个url，它会从里面随机访问的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP拥塞控制窗口有效性验证机制]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/12/08/kernel-net-cwnd-test/"/>
    <updated>2015-12-08T15:49:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/12/08/kernel-net-cwnd-test</id>
    <content type="html"><![CDATA[<p>blog.csdn.net/zhangskd/article/details/7609465</p>

<h4>概述</h4>

<p>问题1：当发送方长时间受到应用程序的限制，不能发送数据时，会使拥塞窗口无效。TCP是根据拥塞窗口来动态地估计网络带宽的。发送方受到应用程序的限制后，没有数据可以发送。那么此时的拥塞窗口就不能准确的反应网络状况，因为这个拥塞窗口是很早之前的。</p>

<p>问题2：当发送方受到应用程序限制，不能利用完拥塞窗口，会使拥塞窗口的增长无效。TCP不断调整cwnd来测试网络带宽。如果不能完全使用掉cwnd，就不知道网络能否承受得了cwnd的数据量，这种情况下的cwnd增长是无效的。</p>

<h4>原理</h4>

<p>TCP sender受到的两种限制</p>

<p>(1) application-limited ：when the sender sends less than is allowed by the congestion or receiver window.</p>

<p>(2) network-limited：when the sender is limited by the TCP window. More precisely, we define a network-limited period as any period when the sender is sending a full window of data.</p>

<h5>问题1描述</h5>

<p>TCP&rsquo;s congestion window controls the number of packets a TCP flow may have in the
network at any time. However, long periods when the sender is idle or application-limited
can lead to the invalidation of the congestion window, in that the congestion window no longer
reflects current information about the state of the network.</p>

<p>The congestion window is set using an Additive-Increase, Multiplicative-Decrease(AIMD) mechanism
that probes for available bandwidth, dynamically adapting to changing network conditions. This AIMD
works well when the sender continually has data to send, as is typically the case for TCP used for
bulk-data transfer. In contrast, for TCP used with telnet applications, the data sender often has little
or no data to send, and the sending rate is often determined by the rate at which data is generated
by the user.</p>

<h5>问题2描述</h5>

<p>An invalid congestion window also results when the congestion window is increased (i.e.,
in TCP&rsquo;s slow-start or congestion avoidance phases) during application-limited periods, when the
previous value of the congestion window might never have been fully utilized. As far as we know, all
current TCP implementations increase the congestion window when an acknowledgement arrives,
if allowed by the receiver&rsquo;s advertised window and the slow-start or congestion avoidance window
increase algorithm, without checking to see if the previous value of the congestion window has in
fact been used.</p>

<p>This document proposes that the window increase algorithm not be invoked during application-
limited periods. This restriction prevents the congestion window from growing arbitrarily large,
in the absence of evidence that the congestion window can be supported by the network.</p>

<h4>实现(1)</h4>

<p>发送方在发送数据包时，如果发送的数据包有负载，则会检测拥塞窗口是否超时。如果超时，则会使拥塞窗口失效并重新计算拥塞窗口。然后根据最近接收段的时间，确定是否进入pingpong模式。
```
    /<em> Congestion state accounting after a packet has been sent. </em>/<br/>
    static void tcp_event_data_sent (struct tcp_sock <em>tp, struct sock </em>sk)<br/>
    {<br/>
        struct inet_connection_sock *icsk = inet_csk(sk);<br/>
        const u32 now = tcp_time_stamp;</p>

<pre><code>    if (sysctl_tcp_slow_start_after_idle &amp;&amp;   
        (!tp-&gt;packets_out &amp;&amp; (s32) (now - tp-&gt;lsndtime) &gt; icsk-&gt;icsk_rto))  
        tcp_cwnd_restart(sk, __sk_dst_get(sk)); /* 重置cnwd */  

    tp-&gt;lsndtime = now; /* 更新最近发包的时间*/  

    /* If it is a reply for ato after last received packets,  
     * enter pingpong mode. */  
    if ((u32)(now - icsk-&gt;icsk_ack.lrcvtime) &lt; icsk.icsk_ack.ato)  
        icsk-&gt;icsk_ack.pingpong = 1;  
}  
</code></pre>

<pre><code>
tcp_event_data_sent()中，符合三个条件才重置cwnd：

（1）tcp_slow_start_after_idle选项设置，这个内核默认置为1
（2）tp-&gt;packets_out == 0，表示网络中没有未确认数据包
（3）now - tp-&gt;lsndtime &gt; icsk-&gt;icsk_rto，距离上次发送数据包的时间超过了RTO
</code></pre>

<pre><code>/* RFC2861. Reset CWND after idle period longer RTO to "restart window". 
 * This is the first part of cnwd validation mechanism. 
 */  
static void tcp_cwnd_restart (struct sock *sk, const struct dst_entry *dst)  
{  
    struct tcp_sock *tp = tcp_sk(sk);  
    s32 delta = tcp_time_stamp - tp-&gt;lsndtime;  

    /* 关于tcp_init_cwnd()可见上一篇blog.*/  
    u32 restart_cwnd = tcp_init_cwnd(tp, dst);  
    u32 cwnd = tp-&gt;snd_cwnd;  

    /* 触发拥塞窗口重置事件*/  
    tcp_ca_event(sk, CA_EVENT_CWND_RESTART);  

    /* 阈值保存下来，并没有重置。*/  
    tp-&gt;snd_ssthresh = tcp_current_ssthresh(sk);  
    restart_cwnd = min(restart_cwnd, cwnd);  

    /* 闲置时间每超过一个RTO且cwnd比重置后的大时，cwnd减半。*/  
    while((delta -= inet_csk(sk)-&gt;icsk_rto) &gt; 0 &amp;&amp; cwnd &gt; restart_cwnd)  
        cwnd &gt;&gt; 1;  

    tp-&gt;snd_cwnd = max(cwnd, restart_cwnd); /* 取其大者！*/  
    tp-&gt;snd_cwnd_stamp = tcp_time_stamp;  
    tp-&gt;snd_cwnd_used = 0;  
}  
</code></pre>

<pre><code>
那么调用tcp_cwnd_restart()后，tp-&gt;snd_cwnd是多少呢？这个是不确定的，要看闲置时间delta、闲置前的cwnd、路由器中设置的initcwnd。当然，最大概率的是：拥塞窗口降为闲置前cwnd的一半。

#### 实现(2)

在发送方成功发送一个数据包后，会检查从发送队列发出而未确认的数据包是否用完拥塞窗口。
如果拥塞窗口被用完了，说明发送方收到网络限制；
如果拥塞窗口没被用完，且距离上次检查时间超过了RTO，说明发送方收到应用程序限制。
</code></pre>

<pre><code>/* Congestion window validation.(RFC2861) */  
static void tcp_cwnd_validate(struct sock *sk) {  
    struct tcp_sock *tp = tcp_sk(sk);  

    if (tp-&gt;packets_out &gt;= tp-&gt;snd_cwnd) {  
        /* Network is feed fully. */  
        tp-&gt;snd_cwnd_used = 0; /*不用这个变量*/  
        tp-&gt;snd_cwnd_stamp = tcp_time_stamp; /* 更新检测时间*/  

    } else {  
        /* Network starves. */  
        if (tp-&gt;packets_out &gt; tp-&gt;snd_cwnd_used)  
            tp-&gt;snd_cwnd_used = tp-&gt;packets_out; /* 更新已使用窗口*/  

            /* 如果距离上次检测的时间，即距离上次发包时间已经超过RTO*/  
            if (sysctl_tcp_slow_start_after_idle &amp;&amp;  
                (s32) (tcp_time_stamp - tp-&gt;snd_cwnd_stamp) &gt;= inet_csk(sk)-&gt;icsk_rto)  
                tcp_cwnd_application_limited(sk);  
    }  
}  
</code></pre>

<pre><code>
在发送方收到应用程序的限制期间，每隔RTO时间，都会调用tcp_cwnd_application_limited()来重新设置sshresh和cwnd，具体如下：
</code></pre>

<pre><code>/* RFC2861, slow part. Adjust cwnd, after it was not full during one rto. 
 * As additional protections, we do not touch cwnd in retransmission phases, 
 * and if application hit its sndbuf limit recently. 
 */  
void tcp_cwnd_application_limited(struct sock *sk)  
{  
    struct tcp_sock *tp = tcp_sk(sk);  

    /* 只有处于Open态，应用程序没受到sndbuf限制时，才进行 
     * ssthresh和cwnd的重置。 
     */  
    if (inet_csk(sk)-&gt;icsk_ca_state == TCP_CA_Open &amp;&amp;   
        sk-&gt;sk_socket &amp;&amp; !test_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags)) {  

        /* Limited by application or receiver window. */  
        u32 init_win = tcp_init_cwnd(tp, __sk_dst_get(sk));  
        u32 win_used = max(tp-&gt;snd_cwnd_used, init_win);  

        /* 没用完拥塞窗口*/  
        if (win_used &lt; tp-&gt;snd_cwnd) {  
            /* 并没有减小ssthresh，反而增大，保留了过去的信息，以便之后有数据发送 
              * 时能快速增大到接近此时的窗口。 
              */  
            tp-&gt;snd_ssthresh = tcp_current_ssthresh(sk);   
            /* 减小了snd_cwnd */  
            tp-&gt;snd_cwnd = (tp-&gt;snd_cwnd + win_used) &gt;&gt; 1;  
        }  
        tp-&gt;snd_cwnd_used = 0;  
    }  
    tp-&gt;snd_cwnd_stamp = tcp_time_stamp; /* 更新最近的数据包发送时间*/  
}  
</code></pre>

<p>```</p>

<p>发送方受到应用程序限制，且限制的时间每经过RTO后，就会调用以上函数来处理snd_ssthresh和snd_cwnd：</p>

<p>（1）snd_ssthresh = max(snd_ssthresh, &frac34; cwnd)</p>

<p>慢启动阈值并没有减小，相反，如果此时cwnd较大，ssthresh会相应的增大。ssthresh是一个很重要的参数，它保留了旧的信息。这样一来，如果应用程序产生了大量的数据，发送方不再受到限制后，经过慢启动阶段，拥塞窗口就能快速恢复到接近以前的值了。</p>

<p>（2）snd_cwnd = (snd_cwnd + snd_cwnd_used) / 2</p>

<p>因为snd_cwnd_used &lt; snd_cwnd，所以snd_cwnd是减小了的。减小snd_cwnd是为了不让它盲目的增长。因为发送方没有利用完拥塞窗口，并不能检测到网络是否能承受该拥塞窗口，这时的增长是无根据的。</p>

<h4>结论</h4>

<p>在发送完数据包后，通过对拥塞窗口有效性的检验，能够避免使用不合理的拥塞窗口。</p>

<p>拥塞窗口代表着网络的状况，通过避免使用不合理的拥塞窗口，就能得到正确的网络状况，而不会采取一些不恰当的措施。</p>

<p>在上文的两种情况下，通过TCP的拥塞窗口有效性验证机制（TCP congestion window validationmechanism），能够更合理的利用网络、避免丢包，从而提高传输效率。</p>

<h4>Reference</h4>

<p>RFC2861</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ixgbe]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-ixgbe/"/>
    <updated>2015-11-17T15:16:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-ixgbe</id>
    <content type="html"><![CDATA[<p><a href="http://www.pagefault.info/?p=403">http://www.pagefault.info/?p=403</a></p>

<p>这里分析的驱动代码是给予linux kernel 3.4.4</p>

<p>对应的文件在drivers/net/ethernet/intel 目录下，这个分析不涉及到很细节的地方，主要目的是理解下数据在协议栈和驱动之间是如何交互的。</p>

<p>首先我们知道网卡都是pci设备，因此这里每个网卡驱动其实就是一个pci驱动。并且intel这里是把好几个万兆网卡(82599/82598/x540)的驱动做在一起的。</p>

<p>首先我们来看对应的pci_driver的结构体，这里每个pci驱动都是一个pci_driver的结构体，而这里是多个万兆网卡共用这个结构体ixgbe_driver.</p>

<pre><code>    static struct pci_driver ixgbe_driver = {
        .name     = ixgbe_driver_name,
        .id_table = ixgbe_pci_tbl,
        .probe    = ixgbe_probe,
        .remove   = __devexit_p(ixgbe_remove),
    #ifdef CONFIG_PM
        .suspend  = ixgbe_suspend,
        .resume   = ixgbe_resume,
    #endif
        .shutdown = ixgbe_shutdown,
        .err_handler = &amp;ixgbe_err_handler
    };
</code></pre>

<p>然后是模块初始化方法,这里其实很简单，就是调用pci的驱动注册方法，把ixgbe挂载到pci设备链中。 这里不对pci设备的初始化做太多介绍，我以前的blog有这方面的介绍，想了解的可以去看看。这里我们只需要知道最终内核会调用probe回调来初始化ixgbe。</p>

<pre><code>    char ixgbe_driver_name[] = "ixgbe";
    static const char ixgbe_driver_string[] =
                    "Intel(R) 10 Gigabit PCI Express Network Driver";

    static int __init ixgbe_init_module(void)
    {
        int ret;
        pr_info("%s - version %s\n", ixgbe_driver_string, ixgbe_driver_version);
        pr_info("%s\n", ixgbe_copyright);

    #ifdef CONFIG_IXGBE_DCA
        dca_register_notify(&amp;dca_notifier);
    #endif

        ret = pci_register_driver(&amp;ixgbe_driver);
        return ret;
    }
</code></pre>

<p>这里不去追究具体如何调用probe的细节，我们直接来看probe函数，这个函数中通过硬件的信息来确定需要初始化那个驱动(82598/82599/x540),然后核心的驱动结构就放在下面的这个数组中。</p>

<pre><code>    static const struct ixgbe_info *ixgbe_info_tbl[] = {
        [board_82598] = &amp;ixgbe_82598_info,
        [board_82599] = &amp;ixgbe_82599_info,
        [board_X540] = &amp;ixgbe_X540_info,
    };
</code></pre>

<p>ixgbe_probe函数很长，我们这里就不详细分析了，因为这部分就是对网卡进行初始化。不过我们关注下面几个代码片段。</p>

<p>首先是根据硬件的参数来取得对应的驱动值:</p>

<pre><code>    const struct ixgbe_info *ii = ixgbe_info_tbl[ent-&gt;driver_data];
</code></pre>

<p>然后就是如何将不同的网卡驱动挂载到对应的回调中，这里做的很简单，就是通过对应的netdev的结构取得adapter，然后所有的核心操作都是保存在adapter中的，最后将ii的所有回调拷贝给adapter就可以了。我们来看代码：</p>

<pre><code>        struct net_device *netdev;
        struct ixgbe_adapter *adapter = NULL;
        struct ixgbe_hw *hw;
        .....................................

        adapter = netdev_priv(netdev);
        pci_set_drvdata(pdev, adapter);

        adapter-&gt;netdev = netdev;
        adapter-&gt;pdev = pdev;
        hw = &amp;adapter-&gt;hw;
        hw-&gt;back = adapter;
        .......................................
        memcpy(&amp;hw-&gt;mac.ops, ii-&gt;mac_ops, sizeof(hw-&gt;mac.ops));
        hw-&gt;mac.type  = ii-&gt;mac;

        /* EEPROM */
        memcpy(&amp;hw-&gt;eeprom.ops, ii-&gt;eeprom_ops, sizeof(hw-&gt;eeprom.ops));
        .....................................
</code></pre>

<p>最后需要关注的就是设置网卡属性，这些属性一般来说都是通过ethtool 可以设置的属性(比如tso/checksum等),这里我们就截取一部分:</p>

<pre><code>        netdev-&gt;features = NETIF_F_SG |
                   NETIF_F_IP_CSUM |
                   NETIF_F_IPV6_CSUM |
                   NETIF_F_HW_VLAN_TX |
                   NETIF_F_HW_VLAN_RX |
                   NETIF_F_HW_VLAN_FILTER |
                   NETIF_F_TSO |
                   NETIF_F_TSO6 |
                   NETIF_F_RXHASH |
                   NETIF_F_RXCSUM;

        netdev-&gt;hw_features = netdev-&gt;features;

        switch (adapter-&gt;hw.mac.type) {
        case ixgbe_mac_82599EB:
        case ixgbe_mac_X540:
            netdev-&gt;features |= NETIF_F_SCTP_CSUM;
            netdev-&gt;hw_features |= NETIF_F_SCTP_CSUM |
                           NETIF_F_NTUPLE;
            break;
        default:
            break;
        }

        netdev-&gt;hw_features |= NETIF_F_RXALL;
        ..................................................

        netdev-&gt;priv_flags |= IFF_UNICAST_FLT;
        netdev-&gt;priv_flags |= IFF_SUPP_NOFCS;

        if (adapter-&gt;flags &amp; IXGBE_FLAG_SRIOV_ENABLED)
            adapter-&gt;flags &amp;= ~(IXGBE_FLAG_RSS_ENABLED |
                        IXGBE_FLAG_DCB_ENABLED);
        ...................................................................
        if (pci_using_dac) {
            netdev-&gt;features |= NETIF_F_HIGHDMA;
            netdev-&gt;vlan_features |= NETIF_F_HIGHDMA;
        }

        if (adapter-&gt;flags2 &amp; IXGBE_FLAG2_RSC_CAPABLE)
            netdev-&gt;hw_features |= NETIF_F_LRO;
        if (adapter-&gt;flags2 &amp; IXGBE_FLAG2_RSC_ENABLED)
            netdev-&gt;features |= NETIF_F_LRO;
</code></pre>

<p>然后我们来看下中断的注册，因为万兆网卡大部分都是多对列网卡(配合msix)，因此对于上层软件来说，就好像有多个网卡一样，它们之间的数据是相互独立的，这里读的话主要是napi驱动的poll方法，后面我们会分析这个.</p>

<p>到了这里或许要问那么网卡是如何挂载回调给上层，从而上层来发送数据呢，这里是这样子的，每个网络设备都有一个回调函数表(比如ndo_start_xmit)来供上层调用，而在ixgbe中的话，就是ixgbe_netdev_ops，下面就是这个结构，不过只是截取了我们很感兴趣的几个地方.</p>

<p>不过这里注意，读回调并不在里面，这是因为写是软件主动的，而读则是硬件主动的。现在ixgbe是NAPI的，因此它的poll回调是ixgbe_poll，是中断注册时候通过netif_napi_add添加进去的。</p>

<pre><code>    static const struct net_device_ops ixgbe_netdev_ops = {
        .ndo_open       = ixgbe_open,
        .ndo_stop       = ixgbe_close,
        .ndo_start_xmit     = ixgbe_xmit_frame,
        .ndo_select_queue   = ixgbe_select_queue,
        .ndo_set_rx_mode    = ixgbe_set_rx_mode,
        .ndo_validate_addr  = eth_validate_addr,
        .ndo_set_mac_address    = ixgbe_set_mac,
        .ndo_change_mtu     = ixgbe_change_mtu,
        .ndo_tx_timeout     = ixgbe_tx_timeout,
        .................................................
        .ndo_set_features = ixgbe_set_features,
        .ndo_fix_features = ixgbe_fix_features,
    };
</code></pre>

<p>这里我们最关注的其实就是ndo_start_xmit回调，这个回调就是驱动提供给协议栈的发送回调接口。我们来看这个函数.</p>

<p>它的实现很简单，就是选取对应的队列，然后调用ixgbe_xmit_frame_ring来发送数据。</p>

<pre><code>    static netdev_tx_t ixgbe_xmit_frame(struct sk_buff *skb,
                        struct net_device *netdev)
    {
        struct ixgbe_adapter *adapter = netdev_priv(netdev);
        struct ixgbe_ring *tx_ring;

        if (skb-&gt;len &lt;= 0) {
            dev_kfree_skb_any(skb);
            return NETDEV_TX_OK;
        }

        /*
         * The minimum packet size for olinfo paylen is 17 so pad the skb
         * in order to meet this minimum size requirement.
         */
        if (skb-&gt;len &lt; 17) {
            if (skb_padto(skb, 17))
                return NETDEV_TX_OK;
            skb-&gt;len = 17;
        }
        //取得对应的队列
        tx_ring = adapter-&gt;tx_ring[skb-&gt;queue_mapping];
        //发送数据
        return ixgbe_xmit_frame_ring(skb, adapter, tx_ring);
    }
</code></pre>

<p>而在ixgbe_xmit_frame_ring中，我们就关注两个地方，一个是tso(什么是TSO，请自行google)，一个是如何发送.</p>

<pre><code>        tso = ixgbe_tso(tx_ring, first, &amp;hdr_len);
        if (tso &lt; 0)
            goto out_drop;
        else if (!tso)
            ixgbe_tx_csum(tx_ring, first);

        /* add the ATR filter if ATR is on */
        if (test_bit(__IXGBE_TX_FDIR_INIT_DONE, &amp;tx_ring-&gt;state))
            ixgbe_atr(tx_ring, first);

    #ifdef IXGBE_FCOE
    xmit_fcoe:
    #endif /* IXGBE_FCOE */
        ixgbe_tx_map(tx_ring, first, hdr_len);
</code></pre>

<p>调用ixgbe_tso处理完tso之后，就会调用ixgbe_tx_map来发送数据。而ixgbe_tx_map所做的最主要是两步，第一步请求DMA，第二步写寄存器，通知网卡发送数据.</p>

<pre><code>        dma = dma_map_single(tx_ring-&gt;dev, skb-&gt;data, size, DMA_TO_DEVICE);
        if (dma_mapping_error(tx_ring-&gt;dev, dma))
            goto dma_error;

        /* record length, and DMA address */
        dma_unmap_len_set(first, len, size);
        dma_unmap_addr_set(first, dma, dma);

        tx_desc-&gt;read.buffer_addr = cpu_to_le64(dma);

        for (;;) {
            while (unlikely(size &gt; IXGBE_MAX_DATA_PER_TXD)) {
                tx_desc-&gt;read.cmd_type_len =
                    cmd_type | cpu_to_le32(IXGBE_MAX_DATA_PER_TXD);

                i++;
                tx_desc++;
                if (i == tx_ring-&gt;count) {
                    tx_desc = IXGBE_TX_DESC(tx_ring, 0);
                    i = 0;
                }

                dma += IXGBE_MAX_DATA_PER_TXD;
                size -= IXGBE_MAX_DATA_PER_TXD;

                tx_desc-&gt;read.buffer_addr = cpu_to_le64(dma);
                tx_desc-&gt;read.olinfo_status = 0;
            }

            ...................................................
            data_len -= size;

            dma = skb_frag_dma_map(tx_ring-&gt;dev, frag, 0, size,
                           DMA_TO_DEVICE);
            ..........................................................

            frag++;
        }
        .................................
        tx_ring-&gt;next_to_use = i;

        /* notify HW of packet */
        writel(i, tx_ring-&gt;tail);
        .................
</code></pre>

<p>上面的操作是异步的，也就是说此时内核还不能释放SKB，而是网卡硬件发送完数据之后，会再次产生中断通知内核，然后内核才能释放内存.接下来我们来看这部分代码。</p>

<p>首先来看的是中断注册的代码，这里我们假设启用了MSIX,那么网卡的中断注册回调就是ixgbe_request_msix_irqs函数，这里我们可以看到调用request_irq函数来注册回调，并且每个队列都有自己的中断号。</p>

<pre><code>    static int ixgbe_request_msix_irqs(struct ixgbe_adapter *adapter)
    {
        struct net_device *netdev = adapter-&gt;netdev;
        int q_vectors = adapter-&gt;num_msix_vectors - NON_Q_VECTORS;
        int vector, err;
        int ri = 0, ti = 0;

        for (vector = 0; vector &lt; q_vectors; vector++) {
            struct ixgbe_q_vector *q_vector = adapter-&gt;q_vector[vector];
            struct msix_entry *entry = &amp;adapter-&gt;msix_entries[vector];
            .......................................................................
            err = request_irq(entry-&gt;vector, &amp;ixgbe_msix_clean_rings, 0,
                      q_vector-&gt;name, q_vector);
            if (err) {
                e_err(probe, "request_irq failed for MSIX interrupt "
                      "Error: %d\n", err);
                goto free_queue_irqs;
            }
            /* If Flow Director is enabled, set interrupt affinity */
            if (adapter-&gt;flags &amp; IXGBE_FLAG_FDIR_HASH_CAPABLE) {
                /* assign the mask for this irq */
                irq_set_affinity_hint(entry-&gt;vector,
                              &amp;q_vector-&gt;affinity_mask);
            }
        }

        ..............................................

        return 0;

    free_queue_irqs:
        ...............................
        return err;
    }
</code></pre>

<p>而对应的中断回调是ixgbe_msix_clean_rings,而这个函数呢，做的事情很简单(需要熟悉NAPI的原理，我以前的blog有介绍),就是调用napi_schedule来重新加入软中断处理.</p>

<pre><code>    static irqreturn_t ixgbe_msix_clean_rings(int irq, void *data)
    {
        struct ixgbe_q_vector *q_vector = data;

        /* EIAM disabled interrupts (on this vector) for us */

        if (q_vector-&gt;rx.ring || q_vector-&gt;tx.ring)
            napi_schedule(&amp;q_vector-&gt;napi);

        return IRQ_HANDLED;
    }
</code></pre>

<p>而NAPI驱动我们知道，最终是会调用网卡驱动挂载的poll回调，在ixgbe中，对应的回调就是ixgbe_poll，那么也就是说这个函数要做两个工作，一个是处理读，一个是处理写完之后的清理.</p>

<pre><code>    int ixgbe_poll(struct napi_struct *napi, int budget)
    {
        struct ixgbe_q_vector *q_vector =
                    container_of(napi, struct ixgbe_q_vector, napi);
        struct ixgbe_adapter *adapter = q_vector-&gt;adapter;
        struct ixgbe_ring *ring;
        int per_ring_budget;
        bool clean_complete = true;

    #ifdef CONFIG_IXGBE_DCA
        if (adapter-&gt;flags &amp; IXGBE_FLAG_DCA_ENABLED)
            ixgbe_update_dca(q_vector);
    #endif
        //清理写
        ixgbe_for_each_ring(ring, q_vector-&gt;tx)
            clean_complete &amp;= !!ixgbe_clean_tx_irq(q_vector, ring);

        /* attempt to distribute budget to each queue fairly, but don't allow
         * the budget to go below 1 because we'll exit polling */
        if (q_vector-&gt;rx.count &gt; 1)
            per_ring_budget = max(budget/q_vector-&gt;rx.count, 1);
        else
            per_ring_budget = budget;
        //读数据，并清理已完成的
        ixgbe_for_each_ring(ring, q_vector-&gt;rx)
            clean_complete &amp;= ixgbe_clean_rx_irq(q_vector, ring,
                                 per_ring_budget);

        /* If all work not completed, return budget and keep polling */
        if (!clean_complete)
            return budget;

        /* all work done, exit the polling mode */
        napi_complete(napi);
        if (adapter-&gt;rx_itr_setting &amp; 1)
            ixgbe_set_itr(q_vector);
        if (!test_bit(__IXGBE_DOWN, &amp;adapter-&gt;state))
            ixgbe_irq_enable_queues(adapter, ((u64)1 &lt;&lt; q_vector-&gt;v_idx));

        return 0;
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[cubic]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-cubic/"/>
    <updated>2015-11-17T15:08:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-cubic</id>
    <content type="html"><![CDATA[<p><a href="http://www.pagefault.info/?p=145">http://www.pagefault.info/?p=145</a></p>

<p>这次主要来看一下内核拥塞控制算法cubic的实现，在linux kernel中实现了很多种拥塞控制算法，不过新的内核(2.6.19之后)默认是cubic(想得到当前内核使用的拥塞控制算法可以察看/proc/sys/net/ipv4/tcp_congestion_control这个值).下面是最新的redhat 6的拥塞控制算法(rh5还是bic算法):
<code>
    [root@rhel6 ~]# cat /proc/sys/net/ipv4/tcp_congestion_control
    cubic
</code>
这个算法的paper在这里：</p>

<p><a href="http://netsrv.csc.ncsu.edu/export/cubic_a_new_tcp_2008.pdf">http://netsrv.csc.ncsu.edu/export/cubic_a_new_tcp_2008.pdf</a></p>

<p>拥塞控制算法会在tcp_ack中被调用，如果是正常的ack(比如不是重复的，不是sack等等)就会进入拥塞控制算法。</p>

<p>cubic会调用tcp_slow_start这个方法(基本上每种拥塞控制算法都会调用它)，这个方法主要是处理slow start，而内核中的slow start是这样子的，接收一个ack，snd_cwnd就会加1，然后当cwnd大于设置的拥塞窗口阀值snd_ssthresh的时候，就会进入拥塞避免状态。而在发送数据包的时候，会判断in_flight(可以认为是发送还没确认的数据包，它等于发送未确认的数据包－sack的数据段－丢失的数据段＋重传的数据段，我的前面的blog有详细解释这个数据段)是否大于snd_cwnd,如果大于等于则不会发送数据，如果小于才会继续发送数据。</p>

<p>而进入拥塞避免状态之后，窗口的增长速度将会减缓，</p>

<p>来看一下我用jprobe hook tcp_slow_start(slow start处理函数) 和 tcp_cong_avoid_ai (拥塞避免处理)的数据。</p>

<p>在下面的数据中sk表示当前socket的地址， in_flight packet表示发送还未接收的包, snd_cwnd表示发送拥塞窗口。
然后详细解释下count后面的两个值，其中第一个是snd_cwnd_cnt，表示在当前的拥塞窗口中已经发送的数据段的个数，而第二个是struct bictcp的一个域cnt，它是cubic拥塞算法的核心，主要用来控制在拥塞避免状态的时候，什么时候才能增大拥塞窗口，具体实现是通过比较它和snd_cwnd_cnt，来决定是否增大拥塞窗口，而这个值的计算，我这里不会分析，想了解的，可以去看cubic的paper。</p>

<p>还有一个需要注意的地方就是ssthresh，可以看到这个值在一开始初始化为一个最大的值，然后在进入拥塞避免状态的时候被设置为前一次拥塞窗口的大小.这个处理可以看rfc2581的这段：</p>

<p>  The initial value of ssthresh may be arbitrarily high (i.e., the size of the advertised window), but it may be reduced in response to congestion. When cwnd &lt; ssthresh, the slow-start algorithm is used and when cwnd > ssthresh, the congestion avoidance algorithm is used. When cwnd and ssthresh are equal, the sender may use either of them.</p>

<p>我们后面会看到这个值在cubic中是如何被设置的。
<code>
    //进入slow start，可以看到拥塞窗口默认初始值是3，然后每次接收到ack，都会加1.
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 2, snd_cwnd is 3, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 3, snd_cwnd is 4, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 4, snd_cwnd is 5, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 5, snd_cwnd is 6, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 4, snd_cwnd is 7, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 7, snd_cwnd is 8, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 6, snd_cwnd is 9, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 9, snd_cwnd is 10, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 10, snd_cwnd is 11, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 11, snd_cwnd is 12, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 10, snd_cwnd is 13, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 9, snd_cwnd is 14, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 14, snd_cwnd is 15, ssthresh is 2147483647, count is [0:0]
    //ssthresh被更新为当前拥塞窗口的大小，后面会看到为什么是16
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 13, snd_cwnd is 16, ssthresh is 16, count is [0:0]
    //进入拥塞避免，可以清楚的看到，此时拥塞窗口大于对应的阀值ssthresh.
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 16, snd_cwnd is 17, ssthresh is 16, count is [0:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 15, snd_cwnd is 17, ssthresh is 16, count is [1:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 16, snd_cwnd is 17, ssthresh is 16, count is [2:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 15, snd_cwnd is 17, ssthresh is 16, count is [3:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 14, snd_cwnd is 17, ssthresh is 16, count is [4:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 13, snd_cwnd is 17, ssthresh is 16, count is [5:877]
    //这里注意，其中count的第一个值是一直线性增长的，也就是说下面省略了大概80条log，而在这80几次中拥塞窗口一直维持在17没有变化
    ....................................................................................................................
    //可以看到cnt变为3，也就是说明当执行完拥塞避免就会增加窗口了。
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 13, snd_cwnd is 17, ssthresh is 16, count is [91:3]
    //增加窗口的大小，然后将snd_cwnd_cnt reset为0.
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 11, snd_cwnd is 18, ssthresh is 16, count is [0:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 16, snd_cwnd is 18, ssthresh is 16, count is [1:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 14, snd_cwnd is 18, ssthresh is 16, count is [2:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 12, snd_cwnd is 18, ssthresh is 16, count is [3:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 12, snd_cwnd is 18, ssthresh is 16, count is [4:6]
</code></p>

<p>可以看到在slow start的状态，发送拥塞窗口就是很简单的每次加1，而当进入拥塞避免之后，明显的拥塞窗口的增大速度变慢很多。</p>

<p>接下来来看具体的代码是如何实现的.</p>

<p>首先来看bictcp_cong_avoid，也就是cubic拥塞控制算法的handler(一般来说在tcp_ack中被调用)，它有3个参数，第一个是对应的sock，第二个是对应的ack序列号，而第三个就是比较重要的一个变量，表示发送还没有被ack的数据包(在linux 内核tcp拥塞处理一中详细介绍过内核中这些变量)，这个变量是拥塞控制的核心。</p>

<pre><code>    static void bictcp_cong_avoid(struct sock *sk, u32 ack, u32 in_flight)
    {
        struct tcp_sock *tp = tcp_sk(sk);
        struct bictcp *ca = inet_csk_ca(sk);
        //判断发送拥塞窗口是否到达限制，如果到达限制则直接返回。
        if (!tcp_is_cwnd_limited(sk, in_flight))
            return;
        //开始决定进入slow start还是拥塞控制状态
        if (tp-&gt;snd_cwnd &lt;= tp-&gt;snd_ssthresh) {
            //是否需要reset对应的bictcp的值
            if (hystart &amp;&amp; after(ack, ca-&gt;end_seq))
                bictcp_hystart_reset(sk);
            //进入slow start状态
            tcp_slow_start(tp);
        } else {
            //进入拥塞避免状态，首先会更新ca-&gt;cnt.
            bictcp_update(ca, tp-&gt;snd_cwnd);
            //然后进入拥塞避免
            tcp_cong_avoid_ai(tp, ca-&gt;cnt);
        }
    }
</code></pre>

<p>接下来就是看tcp_is_cwnd_limited，这个函数主要是实现RFC2861中对拥塞窗口的检测。它返回1说明拥塞窗口被限制，我们需要增加拥塞窗口，否则的话，就不需要增加拥塞窗口。</p>

<p>然后这里还有两个判断，先来看第一个 gso的概念，gso是Generic Segmentation Offload的简写，他的主要功能就是尽量的延迟数据包的传输，以便与在最恰当的时机传输数据包，这个机制是处于数据包离开协议栈与进入驱动之间。比如如果驱动支持TSO的话，gso就会将多个unsegmented的数据段传递给驱动。而TSO是TCP Segmentation Offload的缩写，它表示驱动支持协议栈发送大的MTU的数据段，然后硬件负责来切包，然后将数据发送出去，这样子的话，就能提高系统的吞吐。这几个东西(还有GRO)，以后我会详细分析，现在只需要大概知道他们是干什么的。</p>

<p>而在这里如果支持gso，就有可能是tso defer住了数据包，因此这里会进行几个相关的判断，来看需不需要增加拥塞窗口。。</p>

<p>然后是burst的概念，主要用来控制网络流量的突发性增大，也就是说当left数据(还能发送的数据段数)大于burst值的时候，我们需要暂时停止增加窗口，因为此时有可能我们这边数据发送过快。</p>

<pre><code>    int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
    {
        const struct tcp_sock *tp = tcp_sk(sk);
        u32 left;
        //比较发送未确认和发送拥塞窗口的大小
        if (in_flight &gt;= tp-&gt;snd_cwnd)
            return 1;
        //得到还能发送的数据包的段数
        left = tp-&gt;snd_cwnd - in_flight;
        if (sk_can_gso(sk) &amp;&amp;
            left * sysctl_tcp_tso_win_divisor &lt; tp-&gt;snd_cwnd &amp;&amp;
            left * tp-&gt;mss_cache &lt; sk-&gt;sk_gso_max_size)
            return 1;
        //看是否还能发送的数据包是否小于等于burst
        return left &lt;= tcp_max_burst(tp);
    }
</code></pre>

<p>接下来来看snd_ssthresh是如何被设置的，这个值在加载cubic模块的时候可以传递一个我们制定的值给它，不过，默认是很大的值，我这里是2147483647,然后在接收ack期间(slow start)期间会调整这个值，在cubic中，默认是16（一般来说说当拥塞窗口到达16的时候，snd_ssthresh会被设置为16).</p>

<p>在cubic中有两个可以设置snd_ssthresh的地方一个是hystart_update，一个是bictcp_recalc_ssthresh，后一个我这里就不介绍了，以后介绍拥塞状态机的时候会详细介绍，现在只需要知道，只有遇到拥塞的时候，需要调整snd_ssthres的时候，我们才需要调用bictcp_recalc_ssthresh。</p>

<p>而hystart_update是在bictcp_acked中被调用，而bictcp_acked则是基本每次收到ack都会调用这个函数，我们来看在bictcp_acked中什么情况就会调用hystart_update：</p>

<pre><code>    /* hystart triggers when cwnd is larger than some threshold */
    if (hystart &amp;&amp; tp-&gt;snd_cwnd &lt;= tp-&gt;snd_ssthresh &amp;&amp;
        tp-&gt;snd_cwnd &gt;= hystart_low_window)
        hystart_update(sk, delay);
</code></pre>

<p>其中hystart是hybrid slow start打开的标志，默认是开启，hystart_low_window是设置snd_ssthresh的最小拥塞窗口值，默认是16。而tp->snd_ssthresh默认是一个很大的值，因此这里就知道了，当拥塞窗口增大到16的时候我们就会进去hystart_update来更新snd_ssthresh.因此hystart_updat换句话来说也就是主要用于是否退出slow start。</p>

<pre><code>    static void hystart_update(struct sock *sk, u32 delay)
    {
        struct tcp_sock *tp = tcp_sk(sk);
        struct bictcp *ca = inet_csk_ca(sk);

        if (!(ca-&gt;found &amp; hystart_detect)) {
            .................................................................
            /*
             * Either one of two conditions are met,
             * we exit from slow start immediately.
             */
            //found是一个是否退出slow start的标记
            if (ca-&gt;found &amp; hystart_detect)
                //设置snd_ssthresh
                tp-&gt;snd_ssthresh = tp-&gt;snd_cwnd;
        }
    }
</code></pre>

<p>然后是slow start的处理,这里有关abc的处理，注释都很详细了，这里就不解释了，我们主要看abc关闭的部分。这里使用cnt，也是主要为了打开abc之后的slow start。</p>

<p>这是abc（Appropriate Byte Counting）相关的rfc：</p>

<p><a href="http://www.faqs.org/rfcs/rfc3465.html">http://www.faqs.org/rfcs/rfc3465.html</a></p>

<p>Appropriate Byte Countin会导致拥塞控制算法很激进，比如打开它之后就不一定每次ack都会执行slow start，而且窗口也会增加的快很多。</p>

<pre><code>    void tcp_slow_start(struct tcp_sock *tp)
    {
        int cnt; /* increase in packets */

        /* RFC3465: ABC Slow start
         * Increase only after a full MSS of bytes is acked
         *
         * TCP sender SHOULD increase cwnd by the number of
         * previously unacknowledged bytes ACKed by each incoming
         * acknowledgment, provided the increase is not more than L
         */
        if (sysctl_tcp_abc &amp;&amp; tp-&gt;bytes_acked &lt; tp-&gt;mss_cache)
            return;
        //限制slow start的cnt
        if (sysctl_tcp_max_ssthresh &gt; 0 &amp;&amp; tp-&gt;snd_cwnd &gt; sysctl_tcp_max_ssthresh)
            cnt = sysctl_tcp_max_ssthresh &gt;&gt; 1; /* limited slow start */
        else
            cnt = tp-&gt;snd_cwnd;         /* exponential increase */

        /* RFC3465: ABC
         * We MAY increase by 2 if discovered delayed ack
         */
        if (sysctl_tcp_abc &gt; 1 &amp;&amp; tp-&gt;bytes_acked &gt;= 2*tp-&gt;mss_cache)
            cnt &lt;&lt;= 1;
        tp-&gt;bytes_acked = 0;
        //更新cnt，也就是当前拥塞窗口接受的段的个数.
        tp-&gt;snd_cwnd_cnt += cnt;
        while (tp-&gt;snd_cwnd_cnt &gt;= tp-&gt;snd_cwnd) {
            //这里snd_cwnd_cnt是snd_cwnd的几倍，拥塞窗口就增加几。
            tp-&gt;snd_cwnd_cnt -= tp-&gt;snd_cwnd;
            //如果拥塞窗口没有超过最大值，则加一
            if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp)
                tp-&gt;snd_cwnd++;
        }
    }
</code></pre>

<p>最后是拥塞避免的处理。这里主要的步骤就是通过判断当前的拥塞窗口下已经发送的数据段的个数是否大于算法计算出来的值w，如果大于我们才能增加拥塞窗口值，否则之需要增加snd_cwnd_cnt。</p>

<pre><code>    void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w)
    {
        //判断是否大于我们的标记值
        if (tp-&gt;snd_cwnd_cnt &gt;= w) {
            if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp)
                tp-&gt;snd_cwnd++;
            tp-&gt;snd_cwnd_cnt = 0;
        } else {
            //增加计数值
            tp-&gt;snd_cwnd_cnt++;
        }
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iptables]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-iptables/"/>
    <updated>2015-11-17T14:48:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-iptables</id>
    <content type="html"><![CDATA[<p><a href="http://my.oschina.net/kisops/blog/150995">http://my.oschina.net/kisops/blog/150995</a></p>

<p>解决：nf_conntrack: table full, dropping packet.</p>

<hr />

<p> “连接跟踪表已满，开始丢包”！相信不少用iptables的同学都会见过这个错误信息吧，这个问题曾经也困扰过我好长一段时间。此问题的解决办法有四种（nf_conntrack 在CentOS 5 / kernel &lt;= 2.6.19中名为 ip_conntrack ）：</p>

<h4>一、关闭防火墙。 简单粗暴，直接有效</h4>

<pre><code>    chkconfig iptables off
    chkconfig ip6tables off
    service iptables stop
    service ip6tables stop
</code></pre>

<p>切记：在防火墙关闭状态下，不要通过iptables指令（比如 iptables -nL）来查看当前状态！因为这样会导致防火墙被启动，而且规则为空。虽然不会有任何拦截效果，但所有连接状态都会被记录，浪费资源且影响性能并可能导致防火墙主动丢包！</p>

<h4>二、加大防火墙跟踪表的大小，优化对应的系统参数</h4>

<h5>1、状态跟踪表的最大行数的设定，理论最大值 CONNTRACK_MAX = RAMSIZE (in bytes) / 16384 / (ARCH / 32)</h5>

<p>以64G的64位操作系统为例，CONNTRACK_MAX = 64<em>1024</em>1024*1024/16384/2 = 2097152</p>

<p>即时生效请执行：
<code>
    sysctl -w net.netfilter.nf_conntrack_max = 2097152
</code></p>

<h5>2、其哈希表大小通常为总表的1/8，最大为1/2。</h5>

<p>CONNTRACK_BUCKETS = CONNTRACK_MAX / 8</p>

<p>同样64G的64位操作系统，哈希最佳范围是 262144 ~ 1048576 。</p>

<p>运行状态中通过 sysctl net.netfilter.nf_conntrack_buckets 进行查看，通过文件 /sys/module/nf_conntrack/parameters/hashsize 进行设置</p>

<p>或者新建 /etc/modprobe.d/iptables.conf ，重新加载模块才生效：
<code>
    options nf_conntrack hashsize = 262144
</code></p>

<h5>3、还有些相关的系统参数<code>sysctl -a | grep nf_conntrack</code>可以调优（/etc/sysctl.conf ）：</h5>

<pre><code>    net.netfilter.nf_conntrack_max  =   1048576
    net.netfilter.ip_conntrack_tcp_timeout_established  =   3600
    net.netfilter.nf_conntrack_tcp_timeout_close_wait  =   60
    net.netfilter.nf_conntrack_tcp_timeout_fin_wait  =   120
    net.netfilter.nf_conntrack_tcp_timeout_time_wait  =   120
</code></pre>

<p>三、使用祼表，添加“不跟踪”标识。如下示例更适合桌面系统或随意性强的服务器。因为它开启了连接的状态机制，方便和外部通信。修改 /etc/sysconfig/iptables 文件：
<code>
    *raw
    # 对TCP连接不启用追踪，解决ip_contrack满导致无法连接的问题
    -A PREROUTING -p tcp -m tcp --dport 80 -j NOTRACK
    -A PREROUTING -p tcp -m tcp --dport 22 -j NOTRACK
    -A PREROUTING -p tcp -m tcp --dport 21 -j NOTRACK
    -A PREROUTING -p tcp -m tcp --dport 11211 -j NOTRACK
    -A PREROUTING -p tcp -m tcp --dport 60000:60100 -j NOTRACK
    -A PREROUTING -p tcp -s 192.168.10.1 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 80 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 22 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 21 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 11211 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 60000:60100 -j NOTRACK
    -A OUTPUT -p tcp -s 192.168.10.1 -j NOTRACK
    COMMIT
    *filter
    # 允许ping
    -A INPUT -p icmp -j ACCEPT
    # 对本地回路、第5张网卡放行
    -A INPUT -i lo -j ACCEPT
    -A INPUT -i eth4 -j ACCEPT
    # 连接状态跟踪，已建立的连接允许传输数据
    -A INPUT -m state --state ESTABLISHED,RELATED,INVALID,UNTRACKED -j ACCEPT
    # filter表里存在但在raw里不存在的，默认会进行连接状态跟踪
    -A INPUT -s 192.168.10.31 -p tcp --dport 2669 -j ACCEPT
    -A INPUT -j REJECT --reject-with icmp-host-prohibited
    -A FORWARD -j REJECT --reject-with icmp-host-prohibited
    COMMIT
</code></p>

<p>或者干脆对所有连接都关闭跟踪，不跟踪任何连接状态。不过规则就限制比较严谨，进出都需要显式申明。示例 /etc/sysconfig/iptables ：</p>

<pre><code>    *raw
    # 对TCP/UDP连接不启用追踪，解决nf_contrack满导致无法连接的问题
    -A PREROUTING -p tcp -j NOTRACK
    -A PREROUTING -p udp -j NOTRACK
    -A OUTPUT -p tcp -j NOTRACK
    -A OUTPUT -p udp -j NOTRACK
    COMMIT
    *filter
    # 允许ping
    -A INPUT -p icmp -j ACCEPT
    # 对本地回路和eth1放行
    -A INPUT -i lo -j ACCEPT
    -A INPUT -i eth1 -j ACCEPT
    # 只允许符合条件的连接进行传输数据
    -A INPUT -p tcp --dport 22 -j ACCEPT
    -A INPUT -p tcp --sport 80 -j ACCEPT
    -A INPUT -p udp --sport 53 -j ACCEPT
    -A INPUT -p udp --sport 123 -j ACCEPT
    # 出去的包都不限制
    -A OUTPUT -p tcp -j ACCEPT
    -A OUTPUT -p udp -j ACCEPT
    # 输入和转发的包不符合规则的全拦截
    -A INPUT -j REJECT --reject-with icmp-host-prohibited
    -A FORWARD -j REJECT --reject-with icmp-host-prohibited
    COMMIT
</code></pre>

<p>效果如下图：</p>

<p><img src="/images/kernel/2015-11-17.png" alt="" /></p>

<h4>四、删除连接跟踪模块<code>lsmod | grep nf_conntrack</code>，不使用连接状态的跟踪功能。</h4>

<h5>1、删除nf_conntrack和相关的依赖模块，示例：</h5>

<pre><code>    rmmod nf_conntrack_ipv4
    rmmod nf_conntrack_ipv6
    rmmod xt_state
    rmmod xt_CT
    rmmod xt_conntrack
    rmmod iptable_nat
    rmmod ipt_REDIRECT
    rmmod nf_nat
    rmmod nf_conntrack
</code></pre>

<h5>2、禁用跟踪模块，把它加到黑名单（/etc/modprobe.d/blacklist.conf ）：</h5>

<pre><code>    # 禁用 nf_conntrack 模块
    blacklist nf_conntrack
    blacklist nf_conntrack_ipv6
    blacklist xt_conntrack
    blacklist nf_conntrack_ftp
    blacklist xt_state
    blacklist iptable_nat
    blacklist ipt_REDIRECT
    blacklist nf_nat
    blacklist nf_conntrack_ipv4
</code></pre>

<h5>3、去掉防火墙里所有和状态相关的配置（比如state状态，NAT功能），示例：</h5>

<pre><code>    *filter
    # 允许ping
    -A INPUT -p icmp -j ACCEPT
    # 对本地回路和第2张网卡放行
    -A INPUT -i lo -j ACCEPT
    -A INPUT -i eth1 -j ACCEPT
    # 对端口放行
    -A INPUT -p tcp --dport 1331 -j ACCEPT
    # 对IP放行
    -A INPUT -s 192.168.10.31 -j ACCEPT

    #允许本机进行DNS查询

    -A INPUT -p udp --sport 53 -j ACCEPT
    -A OUTPUT -p udp -j ACCEPT
    -A INPUT -j REJECT --reject-with icmp-host-prohibited
    -A FORWARD -j REJECT --reject-with icmp-host-prohibited
    COMMIT
</code></pre>

<p>另外，防火墙的配置文件最好也改下，不要加载任何额外模块（/etc/sysconfig/iptables-config）：</p>

<pre><code>    IPTABLES_MODULES="" # 不需要任何附加模块
    IPTABLES_MODULES_UNLOAD="no" # 避免iptables重启后sysctl中对应的参数被重置为系统默认值
    IPTABLES_SAVE_ON_STOP="no"
    IPTABLES_SAVE_ON_RESTART="no"
    IPTABLES_SAVE_COUNTER="no"
    IPTABLES_STATUS_NUMERIC="yes"
    IPTABLES_STATUS_VERBOSE="no"
    IPTABLES_STATUS_LINENUMBERS="no"
</code></pre>

<p>往往我们对连接的跟踪都是基于操作系统的（netstat / ss ），防火墙的连接状态完全是它自身实现产生的。</p>

<p>总结：防火墙有条件还是交给上层设备完成会更好，使用防火墙一定要做调优；如果不需要防火墙的跟踪功能，规则简单的可以开启NOTRACK选项，条件允许的情况下就删除它吧！</p>
]]></content>
  </entry>
  
</feed>
