<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel~net | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel~net/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-09-30T16:01:54+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[TCP的定时器系列 — 保活定时器]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/30/kernel-net-keepalive/"/>
    <updated>2015-09-30T15:32:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/30/kernel-net-keepalive</id>
    <content type="html"><![CDATA[<p><a href="http://blog.csdn.net/zhangskd/article/details/44177475">http://blog.csdn.net/zhangskd/article/details/44177475</a></p>

<p>主要内容：保活定时器的实现，TCP_USER_TIMEOUT选项的实现。<br/>
内核版本：3.15.2</p>

<h4>原理</h4>

<p>HTTP有Keepalive功能，TCP也有Keepalive功能，虽然都叫Keepalive，但是它们的目的却是不一样的。为了说明这一点，先来看下长连接和短连接的定义。</p>

<p>连接的“长短”是什么？<br/>
短连接：建立一条连接，传输一个请求，马上关闭连接。<br/>
长连接：建立一条连接，传输一个请求，过会儿，又传输若干个请求，最后再关闭连接。</p>

<p>长连接的好处是显而易见的，多个请求可以复用一条连接，省去连接建立和释放的时间开销和系统调用，但也意味着服务器的一部分资源会被长时间占用着。</p>

<p>HTTP的Keepalive，顾名思义，目的在于延长连接的时间，以便在同一条连接中传输多个HTTP请求。</p>

<p>HTTP服务器一般会提供Keepalive Timeout参数，用来决定连接保持多久，什么时候关闭连接。</p>

<p>当连接使用了Keepalive功能时，对于客户端发送过来的一个请求，服务器端会发送一个响应，然后开始计时，如果经过Timeout时间后，客户端没有再发送请求过来，服务器端就把连接关了，不再保持连接了。</p>

<p>TCP的Keepalive，是挂羊头卖狗肉的，目的在于看看对方有没有发生异常，如果有异常就及时关闭连接。</p>

<p>当传输双方不主动关闭连接时，就算双方没有交换任何数据，连接也是一直有效的。</p>

<p>如果这个时候对端、中间网络出现异常而导致连接不可用，本端如何得知这一信息呢？</p>

<p>答案就是保活定时器。它每隔一段时间会超时，超时后会检查连接是否空闲太久了，如果空闲的时间超过了设置时间，就会发送探测报文。然后通过对端是否响应、响应是否符合预期，来判断对端是否正常，如果不正常，就主动关闭连接，而不用等待HTTP层的关闭了。</p>

<p>当服务器发送探测报文时，客户端可能处于4种不同的情况：仍然正常运行、已经崩溃、已经崩溃并重启了、由于中间链路问题不可达。在不同的情况下，服务器会得到不一样的反馈。</p>

<p>(1) 客户主机依然正常运行，并且从服务器端可达</p>

<p>客户端的TCP响应正常，从而服务器端知道对方是正常的。保活定时器会在两小时以后继续触发。</p>

<p>(2) 客户主机已经崩溃，并且关闭或者正在重新启动</p>

<p>客户端的TCP没有响应，服务器没有收到对探测包的响应，此后每隔75s发送探测报文，一共发送9次。</p>

<p>socket函数会返回-1，errno设置为ETIMEDOUT，表示连接超时。</p>

<p>(3) 客户主机已经崩溃，并且重新启动了</p>

<p>客户端的TCP发送RST，服务器端收到后关闭此连接。</p>

<p>socket函数会返回-1，errno设置为ECONNRESET，表示连接被对端复位了。</p>

<p>(4) 客户主机依然正常运行，但是从服务器不可达</p>

<p>双方的反应和第二种是一样的，因为服务器不能区分对端异常与中间链路异常。</p>

<p>socket函数会返回-1，errno设置为EHOSTUNREACH，表示对端不可达。</p>

<h4>选项</h4>

<p>内核默认并不使用TCP Keepalive功能，除非用户设置了SO_KEEPALIVE选项。</p>

<p>有两种方式可以自行调整保活定时器的参数：一种是修改TCP参数，一种是使用TCP层选项。</p>

<p>(1) TCP参数</p>

<p>tcp_keepalive_time</p>

<p>最后一次数据交换到TCP发送第一个保活探测报文的时间，即允许连接空闲的时间，默认为7200s。</p>

<p>tcp_keepalive_intvl</p>

<p>保活探测报文的重传时间，默认为75s。</p>

<p>tcp_keepalive_probes</p>

<p>保活探测报文的发送次数，默认为9次。</p>

<p>Q：一次完整的保活探测需要花费多长时间？</p>

<p>A：tcp_keepalive_time + tcp_keepalive_intvl * tcp_keepalive_probes，默认值为7875s。如果觉得两个多小时太长了，可以自行调整上述参数。</p>

<p>(2) TCP层选项</p>

<p>TCP_KEEPIDLE：含义同tcp_keepalive_time。</p>

<p>TCP_KEEPINTVL：含义同tcp_keepalive_intvl。</p>

<p>TCP_KEEPCNT：含义同tcp_keepalive_probes。</p>

<p>Q：既然有了TCP参数可供调整，为什么还增加了上述的TCP层选项？</p>

<p>A：TCP参数是面向本机的所有TCP连接，一旦调整了，对所有的连接都有效。而TCP层选项是面向一条连接的，一旦调整了，只对本条连接有效。</p>

<h4>激活</h4>

<p>在连接建立后，可以通过设置SO_KEEPALIVE选项，来激活保活定时器。</p>

<pre><code>    int keepalive = 1;
    setsockopt(fd, SOL_SOCKET, SO_KEEPALIVE, &amp;keepalive, sizeof(keepalive));
</code></pre>

<pre><code>    int sock_setsockopt(struct socket *sock, int level, int optname, char __user *optval,   
        unsigned int optlen)  
    {  
        ...  
        case SO_KEEPALIVE:  
    #ifdef CONFIG_INET  
            if (sk-&gt;sk_protocol == IPPROTO_TCP &amp;&amp; sk-&gt;sk_type == SOCK_STREAM)  
                tcp_set_keepalive(sk, valbool); /* 激活或删除保活定时器 */  
    #endif  
            sock_valbool_flag(sk, SOCK_KEEPOPEN, valbool); /* 设置或取消SOCK_KEEPOPEN标志位 */  
            break;  
        ...  
    }  

    static inline void sock_valbool_flag (struct sock *sk, int bit, int valbool)  
    {  
        if (valbool)  
            sock_set_flag(sk, bit);  
        else  
            sock_reset_flag(sk, bit);  
    }  
</code></pre>

<pre><code>    void tcp_set_keepalive(struct sock *sk, int val)  
    {  
        /* 不在以下两个状态设置保活定时器： 
         * TCP_CLOSE：sk_timer用作FIN_WAIT2定时器 
         * TCP_LISTEN：sk_timer用作SYNACK重传定时器 
         */  
        if ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN))  
            return;  

        /* 如果SO_KEEPALIVE选项值为1，且此前没有设置SOCK_KEEPOPEN标志， 
         * 则激活sk_timer，用作保活定时器。 
         */  
        if (val &amp;&amp; !sock_flag(sk, SOCK_KEEPOPEN))  
            inet_csk_reset_keepalive_timer(sk, keepalive_time_when(tcp_sk(sk)));  
        else if (!val)  
            /* 如果SO_KEEPALIVE选项值为0，则删除保活定时器 */  
            inet_csk_delete_keepalive_timer(sk);  
    }  

    /* 保活定时器的超时时间 */  
    static inline int keepalive_time_when(const struct tcp_sock *tp)  
    {  
        return tp-&gt;keepalive_time ? : sysctl_tcp_keepalive_time;  
    }  

    void inet_csk_reset_keepalive_timer (struc sock *sk, unsigned long len)  
    {  
        sk_reset_timer(sk, &amp;sk-&gt;sk_timer, jiffies + len);  
    }  
</code></pre>

<p>可以使用TCP层选项来动态调整保活定时器的参数。</p>

<pre><code>    int keepidle = 600;
    int keepintvl = 10;
    int keepcnt = 6;

    setsockopt(fd, SOL_TCP, TCP_KEEPIDLE, &amp;keepidle, sizeof(keepidle));
    setsockopt(fd, SOL_TCP, TCP_KEEPINTVL, &amp;keepintvl, sizeof(keepintvl));
    setsockopt(fd, SOL_TCP, TCP_KEEPCNT, &amp;keepcnt, sizeof(keepcnt));
</code></pre>

<pre><code>    struct tcp_sock {  
        ...  
        /* 最后一次接收到ACK的时间 */  
        u32 rcv_tstamp; /* timestamp of last received ACK (for keepalives) */  
        ...  
        /* time before keep alive takes place, 空闲多久后才发送探测报文 */  
        unsigned int keepalive_time;  
        /* time iterval between keep alive probes */  
        unsigned int keepalive_intvl; /* 探测报文之间的时间间隔 */  
        /* num of allowed keep alive probes */  
        u8 keepalive_probes; /* 探测报文的发送次数 */  
        ...  
        struct {  
            ...  
            /* 最后一次接收到带负荷的报文的时间 */  
            __u32 lrcvtime; /* timestamp of last received data packet */  
            ...  
        } icsk_ack;  
        ...  
    };  

    #define TCP_KEEPIDLE 4 /* Start Keepalives after this period */  
    #define TCP_KEEPINTVL 5 /* Interval between keepalives */  
    #define TCP_KEEPCNT 6 /* Number of keepalives before death */  

    #define MAX_TCP_KEEPIDLE 32767  
    #define MAX_TCP_KEEPINTVL 32767  
    #define MAX_TCP_KEEPCNT 127  
</code></pre>

<pre><code>    static int do_tcp_setsockopt(struct sock *sk, int level, int optname, char __user *optval,  
        unsigned int optlen)  
    {  
        ...  
        case TCP_KEEPIDLE:  
           if (val &lt; 1 || val &gt; MAX_TCP_KEEPIDLE)  
               err = -EINVAL;  
            else {  
                tp-&gt;keepalive_time = val * HZ; /* 设置新的空闲时间 */  

                /* 如果有使用SO_KEEPALIVE选项，连接处于非监听非结束的状态。 
                 * 这个时候保活定时器已经在计时了，这里设置新的超时时间。 
                 */  
                if (sock_flag(sk, SOCK_KEEPOPEN) &amp;&amp;   
                    !((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN))) {  
                    u32 elapsed = keepalive_time_elapsed(tp); /* 连接已经经历的空闲时间 */  

                    if (tp-&gt;keepalive_time &gt; elapsed)  
                        elapsed = tp-&gt;keepalive_time - elapsed; /* 接着等待的时间，然后超时 */  
                    else  
                        elapsed = 0; /* 会导致马上超时 */  
                    inet_csk_reset_keepalive_timer(sk, elapsed);  
                }  
            }  
            break;  

        case TCP_KEEPINTVL:  
            if (val &lt; 1 || val &gt; MAX_TCP_KEEPINTVL)  
                err = -EINVAL;  
            else  
                tp-&gt;keepalive_intvl = val * HZ; /* 设置新的探测报文间隔 */  
            break;  

        case TCP_KEEPCNT:  
            if (val &lt; 1 || val &gt; MAX_TCP_KEEPCNT)  
                err = -EINVAL;  
            else  
                tp-&gt;keepalive_probes = val; /* 设置新的探测次数 */  
            break;  
        ...  
    }  
</code></pre>

<p>到目前为止，连接已经经历的空闲时间，即最后一次接收到报文至今的时间。</p>

<pre><code>    static inline u32 keepalive_time_elapsed (const struct tcp_sock *tp)  
    {  
        const struct inet_connection_sock *icsk = &amp;tp-&gt;inet_conn;  

        /* lrcvtime是最后一次接收到数据报的时间 
         * rcv_tstamp是最后一次接收到ACK的时间 
         * 返回值就是最后一次接收到报文，到现在的时间，即经历的空闲时间。 
         */  
        return min_t(u32, tcp_time_stamp - icsk-&gt;icsk_ack.lrcvtime,  
            tcp_time_stamp - tp-&gt;rcv_tstamp);  
    }  
</code></pre>

<h4>超时处理函数</h4>

<p>我们知道保活定时器、SYNACK重传定时器、FIN_WAIT2定时器是共用一个定时器实例sk->sk_timer，所以它们的超时处理函数也是一样的，都为tcp_keepalive_timer()。而在函数内部，可以根据此时连接所处的状态，来判断是哪个定时器触发了超时。</p>

<p>Q：什么时候判断对端为异常并关闭连接？</p>

<p>A：分两种情况。</p>

<ol>
<li><p>用户使用了TCP_USER_TIMEOUT选项。当连接的空闲时间超过了用户设置的时间，且有发送过探测报文。</p></li>
<li><p>用户没有使用TCP_USER_TIMEOUT选项。当发送保活探测包的次数达到了保活探测的最大次数时。</p></li>
</ol>


<pre><code>    static void tcp_keepalive_timer (unsigned long data)  
    {  
        struct sock *sk = (struct sock *) data;  
        struct inet_connection_sock *icsk = inet_csk(sk);  
        struct tcp_sock *tp = tcp_sk(sk);  
        u32 elapsed;  

        /* Only process if socket is not in use. */  
        bh_lock_sock(sk);  

        /* 加锁以保证在此期间，连接状态不会被用户进程修改。 
         * 如果用户进程正在使用此sock，那么过50ms再来看看。 
         */  
        if (sock_owned_by_user(sk)) {  
            /* Try again later. */  
            inet_csk_reset_keepalive_timer(sk, HZ/20);  
            goto out;  
        }  

        /* 三次握手期间，用作SYNACK定时器 */  
        if (sk-&gt;sk_state == TCP_LISTEN) {  
            tcp_synack_timer(sk);  
            goto out;  
        }      

        /* 连接释放期间，用作FIN_WAIT2定时器 */  
        if (sk-&gt;sk_state == TCP_FIN_WAIT2 &amp;&amp; sock_flag(sk, SOCK_DEAD)) {  
            ...  
        }  

        /* 接下来就是用作保活定时器了 */  
        if (!sock_flag(sk, SOCK_KEEPOPEN) || sk-&gt;sk_state == TCP_CLOSE)  
            goto out;  

        elapsed = keepalive_time_when(tp); /* 连接的空闲时间超过此值，就发送保活探测报文 */  

        /* It is alive without keepalive. 
         * 如果网络中有发送且未确认的数据包，或者发送队列不为空，说明连接不是idle的？ 
         * 既然连接不是idle的，就没有必要探测对端是否正常。 
         * 保活定时器重新开始计时即可。 
         *  
         * 而实际上当网络中有发送且未确认的数据包时，对端也可能会发生异常而没有响应。 
         * 这个时候会导致数据包的不断重传，只能依靠重传超过了允许的最大时间，来判断连接超时。 
         * 为了解决这一问题，引入了TCP_USER_TIMEOUT，允许用户指定超时时间，可见下文：） 
         */  
        if (tp-&gt;packets_out || tcp_send_head(sk))  
            goto resched; /* 保活定时器重新开始计时 */  

        /* 连接经历的空闲时间，即上次收到报文至今的时间 */  
        elapsed = keepalive_time_elapsed(tp);  

        /* 如果连接空闲的时间超过了设置的时间值 */  
        if (elapsed &gt;= keepalive_time_when(tp)) {  

            /* 什么时候关闭连接？ 
             * 1. 使用了TCP_USER_TIMEOUT选项。当连接空闲时间超过了用户设置的时间，且有发送过探测报文。 
             * 2. 用户没有使用选项。当发送的保活探测包达到了保活探测的最大次数。 
             */  
            if (icsk-&gt;icsk_user_timeout != 0 &amp;&amp; elapsed &gt;= icsk-&gt;icsk_user_timeout &amp;&amp;  
                icsk-&gt;icsk_probes_out &gt; 0) || (icsk-&gt;icsk_user_timeout == 0 &amp;&amp;  
                icsk-&gt;icsk_probes_out &gt;= keepalive_probes(tp))) {  
                tcp_send_active_reset(sk, GFP_ATOMIC); /* 构造一个RST包并发送 */  
                tcp_write_err(sk); /* 报告错误，关闭连接 */  
                goto out;  
            }  

            /* 如果还不到关闭连接的时候，就继续发送保活探测包 */  
            if (tcp_write_wakeup(sk) &lt;= 0) {  
                icsk-&gt;icsk_probes_out++; /* 已发送的保活探测包个数 */  
                elapsed = keepalive_intvl_when(tp); /* 下次超时的时间，默认为75s */  
            } else {  
                /* If keepalive was lost due to local congestion, try harder. */  
                elapsd = TCP_RESOURCE_PROBE_INTERVAL; /* 默认为500ms，会使超时更加频繁 */  
            }  

        } else {  
            /* 如果连接的空闲时间，还没有超过设定值，则接着等待 */  
            elapsed = keepalive_time_when(tp) - elapsed;  
        }   

        sk_mem_reclaim(sk);  

    resched: /* 重设保活定时器 */  
        inet_csk_reset_keepalive_timer(sk, elapsed);  
        goto out;   

    out:  
        bh_unlock_sock(sk);  
        sock_put(sk);  
    }  
</code></pre>

<p>Q：TCP是如何发送Keepalive探测报文的？</p>

<p>A：分两种情况。</p>

<ol>
<li><p>有新的数据段可供发送，且对端接收窗口还没被塞满。发送新的数据段，来作为探测包。</p></li>
<li><p>没有新的数据段可供发送，或者对端的接收窗口满了。发送序号为snd_una - 1、长度为0的ACK包作为探测包。</p></li>
</ol>


<pre><code>    /* Initiate keepalive or window probe from timer. */  

    int tcp_write_wakeup (struct sock *sk)  
    {  
        struct tcp_sock *tp = tcp_sk(sk);  
        struct sk_buff *skb;  

        if (sk-&gt;sk_state == TCP_CLOSE)  
            return -1;  

        /* 如果还有未发送过的数据包，并且对端的接收窗口还没有满 */  
        if ((skb = tcp_send_head(sk)) != NULL &amp;&amp; before(TCP_SKB_CB(skb)-&gt;seq, tcp_wnd_end(tp))) {  
            int err;  
            unsigned int mss = tcp_current_mss(sk); /* 当前的MSS */  
            /* 对端接收窗口所允许的最大报文长度 */  
            unsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq;  

            /* pushed_seq记录发送出去的最后一个字节的序号 */  
            if (before(tp-&gt;pushed_seq, TCP_SKB_CB(skb)-&gt;end_seq))  
                tp-&gt;pushed_seq = TCP_SKB_CB(skb)-&gt;end_seq;  

            /* 如果对端接收窗口小于此数据段的长度，或者此数据段的长度超过了MSS，那么就要进行分段 */  
            if (seg_size &lt; TCP_SKB_CB(skb)-&gt;end_seq - TCP_SKB_CB(skb)-&gt;seq || skb-&gt;len &gt; mss) {  
                seg_size = min(seg_size, mss);  
                TCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH; /* 设置PSH标志，让对端马上把数据提交给程序 */  
                if (tcp_fragment(sk, skb, seg_size, mss)) /* 进行分段 */  
                    return -1;  
            } else if (! tcp_skb_pcount(skb)) /* 进行TSO分片 */  
                tcp_set_skb_tso_segs(sk, skb, mss); /* 初始化分片相关变量 */  

            TCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;  
            TCP_SKB_CB(skb)-&gt;when = tcp_time_stamp;  
            err = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC); /* 发送此数据段 */  
            if (!err)  
                tcp_event_new_data_sent(sk, skb); /* 发送了新的数据，更新相关参数 */  

        } else { /* 如果没有新的数据段可用作探测报文发送，或者对端的接收窗口为0 */  

           /* 处于紧急模式时，额外发送一个序号为snd_una的ACK包，告诉对端紧急指针 */  
           if (between(tp-&gt;snd_up, tp-&gt;snd_una + 1, tp-&gt;snd_una + 0xFFFF))  
               tcp_xmit_probe_skb(sk, 1);  

            /* 发送一个序号为snd_una -1的ACK包，长度为0，这是一个序号过时的报文。 
             * snd_una: first byte we want an ack for，所以snd_una - 1序号的字节已经被确认过了。 
             * 对端会响应一个ACK。 
             */  
            return tcp_xmit_probe_skb(sk, 0);  
        }  
    }  
</code></pre>

<p>Q：当没有新的数据可以用作探测包、或者对端的接收窗口为0时，怎么办呢？</p>

<p>A：发送一个序号为snd_una - 1、长度为0的ACK包，对端收到此包后会发送一个ACK响应。如此一来本端就能够知道对端是否还活着、接收窗口是否打开了。</p>

<pre><code>    /* This routine sends a packet with an out of date sequence number. 
     * It assumes the other end will try to ack it. 
     *  
     * Question: what should we make while urgent mode? 
     * 4.4BSD forces sending single byte of data. We cannot send out of window 
     * data, because we have SND.NXT == SND.MAX... 
     *  
     * Current solution: to send TWO zero-length segments in urgent mode: 
     * one is with SEG.SEG=SND.UNA to deliver urgent pointer, another is out-of-date with 
     * SND.UNA - 1 to probe window. 
     */  

    static int tcp_xmit_probe_skb (struct sock *sk, int urgent)  
    {  
        struct tcp_sock *tp = tcp_sk(sk);  
        struct sk_buff *skb;  

        /* We don't queue it, tcp_transmit_skb() sets ownership. */  
        skb = alloc_skb(MAX_TCP_HEADER, sk_gfp_atomic(sk, GFP_ATOMIC));  
        if (skb == NULL)  
            return -1;  

        /* Reserve space for headers and set control bits. */  
        skb_reserve(skb, MAX_TCP_HEADER);  

        /* Use a previous sequence. This should cause the other end to send an ack. 
         * Don't queue or clone SKB, just send it. 
         */  
        /* 如果没有设置紧急指针，那么发送的序号为snd_una - 1，否则发送的序号为snd_una */  
        tcp_init_nondata_skb(skb, tp-&gt;snd_una - !urgent, TCPHDR_ACK);  
        TCP_SKB_CB(skb)-&gt;when = tcp_time_stamp;  
        return tcp_transmit_skb(sk, skb, 0, GFP_ATOMIC); /* 发送探测包 */  
    }  
</code></pre>

<p>发送RST包。</p>

<pre><code>    /* We get here when a process closes a file descriptor (either due to an explicit close() 
     * or as a byproduct of exit()'ing) and there was unread data in the receive queue. 
     * This behavior is recommended by RFC 2525, section 2.17. -DaveM 
     */  

    void tcp_send_active_reset (struct sock *sk, gfp_t priority)  
    {  
        struct sk_buff *skb;  
        /* NOTE: No TCP options attached and we never retransmit this. */  
        skb = alloc_skb(MAX_TCP_HEADER, priority);  
        if (!skb) {  
            NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);  
            return;  
        }  

        /* Reserve space for headers and prepare control bits. */  
        skb_reserve(skb, MAX_TCP_HEADER); /* 为报文头部预留空间 */  
        /* 初始化不携带数据的skb的一些控制字段 */  
        tcp_init_nondata_skb(skb, tcp_acceptable_seq(sk), TCPHDR_ACK | TCPHDR_RST);  

        /* Send if off，发送此RST包*/  
        TCP_SKB_CB(skb)-&gt;when = tcp_time_stamp;  
        if (tcp_transmit_skb(sk, skb, 0, priority))  
            NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);  
        TCP_INC_STATS(sock_net(sk), TCP_MIB_OUTRSTS);  
    }  

    static inline __u32 tcp_acceptable_seq (const struct sock *sk)  
    {  
        const struct tcp_sock *tp = tcp_sk(sk);  

        /* 如果snd_nxt在对端接收窗口范围内 */  
        if (! before(tcp_wnd_end(tp), tp-&gt;snd_nxt))  
            return tp-&gt;snd_nxt;  
        else  
            return tcp_wnd_end(tp);  
    }  
</code></pre>

<h4>TCP_USER_TIMEOUT选项</h4>

<p>从上文可知同时符合以下条件时，保活定时器才会发送探测报文：</p>

<ol>
<li><p>网络中没有发送且未确认的数据包。</p></li>
<li><p>发送队列为空。</p></li>
<li><p>连接的空闲时间超过了设定的时间。</p></li>
</ol>


<p>Q：如果网络中有发送且未确认的数据包、或者发送队列不为空时，保活定时器不起作用了，岂不是不能够检测到对端的异常了？</p>

<p>A：可以使用TCP_USER_TIMEOUT，显式的指定当发送数据多久后还没有得到响应，就判定连接超时，从而主动关闭连接。</p>

<p>TCP_USER_TIMEOUT选项会影响到超时重传定时器和保活定时器。</p>

<p>(1) 超时重传定时器</p>

<p>判断连接是否超时，分3种情况：</p>

<ol>
<li><p>SYN包：当SYN包的重传次数达到上限时，判定连接超时。(默认允许重传5次，初始超时时间为1s，总共历时31s)</p></li>
<li><p>非SYN包，用户使用TCP_USER_TIMEOUT：当数据包发出去后的等待时间超过用户设置的时间时，判定连接超时。</p></li>
<li><p>非SYN包，用户没有使用TCP_USER_TIMEOUT：当数据包发出去后的等待时间超过以TCP_RTO_MIN为初始超时时间，重传boundary次所花费的时间后，判定连接超时。(boundary的最大值为tcp_retries2，默认值为15)</p></li>
</ol>


<p>(2) 保活定时器</p>

<p>判断连接是否异常，分2种情况：</p>

<ol>
<li><p>用户使用了TCP_USER_TIMEOUT选项。当连接的空闲时间超过了用户设置的时间，且有发送过探测报文。</p></li>
<li><p>用户没有使用TCP_USER_TIMEOUT选项。当发送保活探测包的次数达到了保活探测的最大次数时。</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TIME_WAIT状态下对接收到的数据包如何处理]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/29/kernel-net-timewait_state/"/>
    <updated>2015-09-29T17:53:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/29/kernel-net-timewait_state</id>
    <content type="html"><![CDATA[<p><a href="http://www.educity.cn/linux/1605134.html">http://www.educity.cn/linux/1605134.html</a></p>

<p>正常情况下主动关闭连接的一端在连接正常终止后，会进入TIME_WAIT状态，存在这个状态有以下两个原因（参考《Unix网络编程》）：</p>

<p>《UNIX网络编程.卷2：进程间通信(第2版)》[PDF]下载</p>

<p>1、保证TCP连接关闭的可靠性。如果最终发送的ACK丢失，被动关闭的一端会重传最终的FIN包，如果执行主动关闭的一端没有维护这个连接的状态信息，会发送RST包响应，导致连接不正常关闭。</p>

<p>2、允许老的重复分组在网络中消逝。假设在一个连接关闭后，发起建立连接的一端（客户端）立即重用原来的端口、IP地址和服务端建立新的连接。老的连接上的分组可能在新的连接建立后到达服务端，TCP必须防止来自某个连接的老的重复分组在连接终止后再现，从而被误解为同一个连接的化身。要实现这种功能，TCP不能给处于TIME_WAIT状态的连接启动新的连接。TIME_WAIT的持续时间是2MSL，保证在建立新的连接之前老的重复分组在网络中消逝。这个规则有一个例外：如果到达的SYN的序列号大于前一个连接的结束序列号，源自Berkeley的实现将给当前处于TIME_WAIT状态的连接启动新的化身。</p>

<p>最初在看《Unix网络编程》 的时候看到这个状态，但是在项目中发现对这个状态的理解有误，特别是第二个理由。原本认为在TIME_WAIT状态下肯定不会再使用相同的五元组（协议类型，源目的IP、源目的端口号）建立一个新的连接，看书还是不认真啊！为了加深理解，决定结合内核代码，好好来看下内核在TIME_WAIT状态下的处理。其实TIME_WAIT存在的第二个原因的解释更多的是从被动关闭一方的角度来说明的。如果是执行主动关闭的是客户端，客户端户进入TIME_WAIT状态，假设客户端重用端口号来和服务器建立连接，内核会不会允许客户端来建立连接？内核如何来处理这种情况？书本中不会对这些点讲的那么详细，要从内核源码中来找答案。</p>

<p>我们先来看服务器段进入TIME_WAIT后内核的处理，即服务器主动关闭连接。TCP层的接收函数是tcp_v4_rcv()，和TIME_WAIT状态相关的主要代码如下所示：</p>

<pre><code>    int tcp_v4_rcv(struct sk_buff *skb)
    {
        ......

        sk = __inet_lookup_skb(&amp;tcp_hashinfo, skb, th-&gt;source, th-&gt;dest);
        if (!sk)
            goto no_tcp_socket;
    process:
        if (sk-&gt;sk_state == TCP_TIME_WAIT)
            goto do_time_wait;   
            ......

    discard_it:
        /* Discard frame. */
        kfree_skb(skb);
        return 0;
        ......
    do_time_wait:
        ......

    switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {
        case TCP_TW_SYN: {
            struct sock *sk2 = inet_lookup_listener(dev_net(skb-&gt;dev),
                                &amp;tcp_hashinfo,
                                iph-&gt;daddr, th-&gt;dest,
                                inet_iif(skb));
            if (sk2) {
                inet_twsk_deschedule(inet_twsk(sk), &amp;tcp_death_row);
                inet_twsk_put(inet_twsk(sk));
                sk = sk2;
                goto process;
            }
            /* Fall through to ACK */
        }
        case TCP_TW_ACK:
            tcp_v4_timewait_ack(sk, skb);
            break;
        case TCP_TW_RST:
            goto no_tcp_socket;
        case TCP_TW_SUCCESS:;
        }
        goto discard_it;
    }
</code></pre>

<p>接收到SKb包后，会调用__inet_lookup_skb()查找对应的sock结构。如果套接字状态是TIME_WAIT状态，会跳转到do_time_wait标签处处理。从代码中可以看到，主要由tcp_timewait_state_process()函数来处理SKB包，处理后根据返回值来做相应的处理。</p>

<p>在看tcp_timewait_state_process()函数中的处理之前，需要先看一看不同的返回值会对应什么样的处理。</p>

<p>如果返回值是TCP_TW_SYN，则说明接收到的是一个“合法”的SYN包（也就是说这个SYN包可以接受），这时会首先查找内核中是否有对应的监听套接字，如果存在相应的监听套接字，则会释放TIME_WAIT状态的传输控制结构，跳转到process处开始处理，开始建立一个新的连接。如果没有找到监听套接字会执行到TCP_TW_ACK分支。</p>

<p>如果返回值是TCP_TW_ACK，则会调用tcp_v4_timewait_ack()发送ACK，然后跳转到discard_it标签处，丢掉数据包。</p>

<p>如果返回值是TCP_TW_RST，则会调用tcp_v4_send_reset()给对端发送RST包，然后丢掉数据包。</p>

<p>如果返回值是TCP_TW_SUCCESS，则会直接丢掉数据包。</p>

<p>接下来我们通过tcp_timewait_state_process()函数来看TIME_WAIT状态下的数据包处理。</p>

<p>为了方便讨论，假设数据包中没有时间戳选项，在这个前提下，tcp_timewait_state_process()中的局部变量paws_reject的值为0。</p>

<p>如果需要保持在FIN_WAIT_2状态的时间小于等于TCP_TIMEWAIT_LEN，则会从FIN_WAIT_2状态直接迁移到TIME_WAIT状态，也就是使用描述TIME_WAIT状态的sock结构代替当前的传输控制块。虽然这时的sock结构处于TIME_WAIT结构，但是还要区分内部状态，这个内部状态存储在inet_timewait_sock结构的tw_substate成员中。</p>

<p>如果内部状态为FIN_WAIT_2，tcp_timewait_state_process()中处理的关键代码片段如下所示：</p>

<pre><code>    if (tw-&gt;tw_substate == TCP_FIN_WAIT2) {
        /* Just repeat all the checks of tcp_rcv_state_process() */

        /* Out of window, send ACK */
        if (paws_reject ||
            !tcp_in_window(TCP_SKB_CB(skb)-&gt;seq, TCP_SKB_CB(skb)-&gt;end_seq,
                  tcptw-&gt;tw_rcv_nxt,
                  tcptw-&gt;tw_rcv_nxt + tcptw-&gt;tw_rcv_wnd))
            return TCP_TW_ACK;

        if (th-&gt;rst)
            goto kill;

        if (th-&gt;syn &amp;&amp; !before(TCP_SKB_CB(skb)-&gt;seq, tcptw-&gt;tw_rcv_nxt))
            goto kill_with_rst;

        /* Dup ACK? */
        if (!th-&gt;ack ||
            !after(TCP_SKB_CB(skb)-&gt;end_seq, tcptw-&gt;tw_rcv_nxt) ||
            TCP_SKB_CB(skb)-&gt;end_seq == TCP_SKB_CB(skb)-&gt;seq) {
            inet_twsk_put(tw);
            return TCP_TW_SUCCESS;
        }

        /* New data or FIN. If new data arrive after half-duplex close,
         * reset.
         */
        if (!th-&gt;fin ||
            TCP_SKB_CB(skb)-&gt;end_seq != tcptw-&gt;tw_rcv_nxt + 1) {
    kill_with_rst:
            inet_twsk_deschedule(tw, &amp;tcp_death_row);
            inet_twsk_put(tw);
            return TCP_TW_RST;
        }

        /* FIN arrived, enter true time-wait state. */
        tw-&gt;tw_substate      = TCP_TIME_WAIT;
        tcptw-&gt;tw_rcv_nxt = TCP_SKB_CB(skb)-&gt;end_seq;
        if (tmp_opt.saw_tstamp) {
            tcptw-&gt;tw_ts_recent_stamp = get_seconds();
            tcptw-&gt;tw_ts_recent      = tmp_opt.rcv_tsval;
        }

        /* I am shamed, but failed to make it more elegant.
         * Yes, it is direct reference to IP, which is impossible
         * to generalize to IPv6. Taking into account that IPv6
         * do not understand recycling in any case, it not
         * a big problem in practice. --ANK 
         */
        if (tw-&gt;tw_family == AF_INET &amp;&amp;
            tcp_death_row.sysctl_tw_recycle &amp;&amp; tcptw-&gt;tw_ts_recent_stamp &amp;&amp;
            tcp_v4_tw_remember_stamp(tw))
            inet_twsk_schedule(tw, &amp;tcp_death_row, tw-&gt;tw_timeout,
                      TCP_TIMEWAIT_LEN);
        else
            inet_twsk_schedule(tw, &amp;tcp_death_row, TCP_TIMEWAIT_LEN,
                      TCP_TIMEWAIT_LEN);

        return TCP_TW_ACK;
    }
</code></pre>

<p>如果TCP段序号不完全在接收窗口内，则返回TCP_TW_ACK，表示需要给对端发送ACK。</p>

<p>如果在FIN_WAIT_2状态下接收到的是RST包，则跳转到kill标签处处理，立即释放timewait控制块，并返回TCP_TW_SUCCESS。</p>

<p>如果是SYN包，但是SYN包的序列号在要接收的序列号之前，则表示这是一个过期的SYN包，则跳转到kill_with_rst标签处处理，此时不仅会释放TIME_WAIT传输控制块，还会返回TCP_TW_RST，要给对端发送RST包。</p>

<p>如果接收到DACK，则释放timewait控制块，并返回TCP_TW_SUCCESS。在这种情况下有一个判断条件是看包的结束序列号和起始序列号相同时，会作为DACK处理，所以之后的处理是在数据包中的数据不为空的情况下处理。前面的处理中已经处理了SYN包、RST包的情况，接下来就剩以下三种情况：</p>

<p>1、不带FIN标志的数据包</p>

<p>2、带FIN标志，但是还包含数据</p>

<p>3、FIN包，不包含数据</p>

<p>如果是前两种情况，则会调用inet_twsk_deschedule()释放time_wait控制块。inet_twsk_deschedule()中会调用到inet_twsk_put()减少time_wait控制块的引用，在外层函数中再次调用inet_twsk_put()函数时，就会真正释放time_wait控制块。</p>

<p>如果接收的是对端的FIN包，即第3种情况，则将time_wait控制块的子状态设置为TCP_TIME_WAIT，此时才是进入真正的TIME_WAIT状态。然后根据TIME_WAIT的持续时间的长短来确定是加入到twcal_row队列还是启动一个定时器，最后会返回TCP_TW_ACK，给对端发送TCP连接关闭时最后的ACK包。</p>

<p>到这里，我们看到了对FIN_WAIT_2状态（传输控制块状态为TIME_WAIT状态下，但是子状态为FIN_WAIT_2）的完整处理。</p>

<p>接下来的处理才是对真正的TIME_WAIT状态的处理，即子状态也是TIME_WAIT。</p>

<p>如果在TIME_WAIT状态下，接收到ACK包（不带数据）或RST包，并且包的序列号刚好是下一个要接收的序列号，由以下代码片段处理：</p>

<pre><code>    if (!paws_reject &amp;&amp;
        (TCP_SKB_CB(skb)-&gt;seq == tcptw-&gt;tw_rcv_nxt &amp;&amp;
        (TCP_SKB_CB(skb)-&gt;seq == TCP_SKB_CB(skb)-&gt;end_seq || th-&gt;rst))) {
        /* In window segment, it may be only reset or bare ack. */
        if (th-&gt;rst) {
            /* This is TIME_WAIT assassination, in two flavors.
            * Oh well... nobody has a sufficient solution to this
            * protocol bug yet.
            */
            if (sysctl_tcp_rfc1337 == 0) {
    kill:
                inet_twsk_deschedule(tw, &amp;tcp_death_row);
                inet_twsk_put(tw);
                return TCP_TW_SUCCESS;
            }
        }
        inet_twsk_schedule(tw, &amp;tcp_death_row, TCP_TIMEWAIT_LEN,
                  TCP_TIMEWAIT_LEN);

        if (tmp_opt.saw_tstamp) {
            tcptw-&gt;tw_ts_recent      = tmp_opt.rcv_tsval;
            tcptw-&gt;tw_ts_recent_stamp = get_seconds();
        }

        inet_twsk_put(tw);
        return TCP_TW_SUCCESS;
    }
</code></pre>

<p>如果是RST包的话，并且系统配置sysctl_tcp_rfc1337（默认情况下为0，参见/proc/sys/net/ipv4/tcp_rfc1337）的值为0，这时会立即释放time_wait传输控制块，丢掉接收的RST包。</p>

<p>如果是ACK包，则会启动TIME_WAIT定时器后丢掉接收到的ACK包。</p>

<p>接下来是对SYN包的处理。前面提到了，如果在TIME_WAIT状态下接收到序列号比上一个连接的结束序列号大的SYN包，可以接受，并建立新的连接，下面这段代码就是来处理这样的情况：</p>

<pre><code>    if (th-&gt;syn &amp;&amp; !th-&gt;rst &amp;&amp; !th-&gt;ack &amp;&amp; !paws_reject &amp;&amp;
        (after(TCP_SKB_CB(skb)-&gt;seq, tcptw-&gt;tw_rcv_nxt) ||
        (tmp_opt.saw_tstamp &amp;&amp;
          (s32)(tcptw-&gt;tw_ts_recent - tmp_opt.rcv_tsval) &lt; 0))) {
        u32 isn = tcptw-&gt;tw_snd_nxt + 65535 + 2;
        if (isn == 0)
            isn++;
        TCP_SKB_CB(skb)-&gt;when = isn;
        return TCP_TW_SYN;
    }
</code></pre>

<p>当返回TCP_TW_SYN时，在tcp_v4_rcv()中会立即释放time_wait控制块，并且开始进行正常的连接建立过程。</p>

<p>如果数据包不是上述几种类型的包，可能的情况有：</p>

<p>1、不是有效的SYN包。不考虑时间戳的话，就是序列号在上一次连接的结束序列号之前</p>

<p>2、ACK包，起始序列号不是下一个要接收的序列号</p>

<p>3、RST包，起始序列号不是下一个要接收的序列号</p>

<p>4、带数据的SKB包</p>

<p>这几种情况由以下代码处理：</p>

<pre><code>    if (!th-&gt;rst) {
        /* In this case we must reset the TIMEWAIT timer.
         *
         * If it is ACKless SYN it may be both old duplicate
         * and new good SYN with random sequence number &lt;rcv_nxt.
         * Do not reschedule in the last case.
         */
        if (paws_reject || th-&gt;ack)
            inet_twsk_schedule(tw, &amp;tcp_death_row, TCP_TIMEWAIT_LEN,
                      TCP_TIMEWAIT_LEN);

        /* Send ACK. Note, we do not put the bucket,
         * it will be released by caller.
         */
        return TCP_TW_ACK;
    }
    inet_twsk_put(tw);
    return TCP_TW_SUCCESS;
</code></pre>

<p>如果是RST包，即第3种情况，则直接返回TCP_TW_SUCCESS，丢掉RST包。</p>

<p>如果带有ACK标志的话，则会启动TIME_WAIT定时器，然后给对端发送ACK。我们知道SYN包正常情况下不会设置ACK标志，所以如果是SYN包不会启动TIME_WAIT定时器，只会给对端发送ACK，告诉对端已经收到SYN包，避免重传，但连接应该不会继续建立。</p>

<p>还有一个细节需要提醒下，就是我们看到在返回TCP_TW_ACK时，没有调用inet_twsk_put()释放对time_wait控制块的引用。这时因为在tcp_v4_rcv()中调用tcp_v4_timewait_ack()发送ACK时会用到time_wait控制块，所以需要保持对time_wait控制块的引用。在tcp_v4_timewait_ack()中发送完ACK后，会调用inet_twsk_put()释放对time_wait控制块的引用。</p>

<p>OK，现在我们对TIME_WAIT状态下接收到数据包的情况有了一个了解，知道内核会如何来处理这些包。但是看到的这些更多的是以服务器端的角度来看的，如果客户端主动关闭连接的话，进入TIME_WAIT状态的是客户端。如果客户端在TIME_WAIT状态下重用端口号来和服务器建立连接，内核会如何处理呢？</p>

<p>我编写了一个测试程序：创建一个套接字，设置SO_REUSEADDR选项，建立连接后立即关闭，关闭后立即又重复同样的过程，发现在第二次调用connect()的时候返回EADDRNOTAVAIL错误。这个测试程序很容易理解，写起来也很容易，就不贴出来了。</p>

<p>要找到这个错误是怎么返回的，需要从TCP层的连接函数tcp_4_connect()开始。在tcp_v4_connect()中没有显示返回EADDRNOTAVAIL错误的地方，可能的地方就是在调用inet_hash_connect()返回的。为了确定是不是在inet_hash_connect()中返回的，使用systemtap编写了一个脚本，发现确实是在这个函数中返回的-99错误（EADDRNOTAVAIL的值为99）。其实这个通过代码也可以看出来，在这个函数之前会先查找目的主机的路由缓存项，调用的是ip_route_connect（）函数，跟着这个函数的调用轨迹，没有发现返回EADDRNOTAVAIL错误的地方。</p>

<p>inet_hash_connect()函数只是对<code>__inet_hash_connect()</code>函数进行了简单的封装。在<code>__inet_hash_connect()</code>中如果已绑定了端口号，并且是和其他传输控制块共享绑定的端口号，则会调用check_established参数指向的函数来检查这个绑定的端口号是否可用，代码如下所示：</p>

<pre><code>    int __inet_hash_connect(struct inet_timewait_death_row *death_row,
            struct sock *sk, u32 port_offset,
            int (*check_established)(struct inet_timewait_death_row *,
                struct sock *, __u16, struct inet_timewait_sock **),
            void (*hash)(struct sock *sk))
    {
        struct inet_hashinfo *hinfo = death_row-&gt;hashinfo;
        const unsigned short snum = inet_sk(sk)-&gt;num;
        struct inet_bind_hashbucket *head;
        struct inet_bind_bucket *tb;
        int ret;
        struct net *net = sock_net(sk);

        if (!snum) {
            ......
        }

        head = &amp;hinfo-&gt;bhash[inet_bhashfn(net, snum, hinfo-&gt;bhash_size)];
        tb  = inet_csk(sk)-&gt;icsk_bind_hash;
        spin_lock_bh(&amp;head-&gt;lock);
        if (sk_head(&amp;tb-&gt;owners) == sk &amp;&amp; !sk-&gt;sk_bind_node.next) {
            hash(sk);
            spin_unlock_bh(&amp;head-&gt;lock);
            return 0;
        } else {
            spin_unlock(&amp;head-&gt;lock);
            /* No definite answer... Walk to established hash table */
            ret = check_established(death_row, sk, snum, NULL);
    out:
            local_bh_enable();
            return ret;
        }
    }
</code></pre>

<p>(sk_head(&amp;tb->owners) == sk &amp;&amp; !sk->sk_bind_node.next)这个判断条件就是用来判断是不是只有当前传输控制块在使用已绑定的端口，条件为false时，会执行else分支，检查是否可用。这么看来，调用bind()成功并不意味着这个端口就真的可以用。</p>

<p>check_established参数对应的函数是__inet_check_established()，在inet_hash_connect()中可以看到。在上面的代码中我们还注意到调用check_established()时第三个参数为NULL，这在后面的分析中会用到。</p>

<p><code>__inet_check_established()</code>函数中，会分别在TIME_WAIT传输控制块和除TIME_WIAT、LISTEN状态外的传输控制块中查找是已绑定的端口是否已经使用，代码片段如下所示：</p>

<pre><code>    /* called with local bh disabled */
    static int __inet_check_established(struct inet_timewait_death_row *death_row,
                        struct sock *sk, __u16 lport,
                        struct inet_timewait_sock **twp)
    {
        struct inet_hashinfo *hinfo = death_row-&gt;hashinfo;
        struct inet_sock *inet = inet_sk(sk);
        __be32 daddr = inet-&gt;rcv_saddr;
        __be32 saddr = inet-&gt;daddr;
        int dif = sk-&gt;sk_bound_dev_if;
        INET_ADDR_COOKIE(acookie, saddr, daddr)
        const __portpair ports = INET_COMBINED_PORTS(inet-&gt;dport, lport);
        struct net *net = sock_net(sk);
        unsigned int hash = inet_ehashfn(net, daddr, lport, saddr, inet-&gt;dport);
        struct inet_ehash_bucket *head = inet_ehash_bucket(hinfo, hash);
        spinlock_t *lock = inet_ehash_lockp(hinfo, hash);
        struct sock *sk2;
        const struct hlist_nulls_node *node;
        struct inet_timewait_sock *tw;

        spin_lock(lock);

        /* Check TIME-WAIT sockets first. */
        sk_nulls_for_each(sk2, node, &amp;head-&gt;twchain) {
            tw = inet_twsk(sk2);

        if (INET_TW_MATCH(sk2, net, hash, acookie,
                        saddr, daddr, ports, dif)) {
                if (twsk_unique(sk, sk2, twp))
                    goto unique;
                else
                    goto not_unique;
            }
        }
        tw = NULL;

        /* And established part... */
        sk_nulls_for_each(sk2, node, &amp;head-&gt;chain) {
            if (INET_MATCH(sk2, net, hash, acookie,
                        saddr, daddr, ports, dif))
                goto not_unique;
        }

    unique:
        ......
        return 0;

    not_unique:
        spin_unlock(lock);
        return -EADDRNOTAVAIL;
    }
</code></pre>

<p>可以看到返回EADDRNOTVAIL错误的有两种情况：</p>

<p>1、在TIME_WAIT传输控制块中找到匹配的端口，并且twsk_unique()返回true时</p>

<p>2、在除TIME_WAIT和LISTEN状态外的传输块中存在匹配的端口。</p>

<p>第二种情况很好容易理解了，只要状态在FIN_WAIT_1、ESTABLISHED等的传输控制块使用的端口和要查找的匹配，就会返回EADDRNOTVAIL错误。第一种情况还要取决于twsk_uniqueue()的返回值，所以接下来我们看twsk_uniqueue()中什么情况下会返回true。</p>

<p>如果是TCP套接字，twsk_uniqueue()中会调用tcp_twsk_uniqueue()来判断，返回true的条件如下所示：</p>

<pre><code>    int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp)
    {
        const struct tcp_timewait_sock *tcptw = tcp_twsk(sktw);
        struct tcp_sock *tp = tcp_sk(sk);

        if (tcptw-&gt;tw_ts_recent_stamp &amp;&amp;
            (twp == NULL || (sysctl_tcp_tw_reuse &amp;&amp;
                    get_seconds() - tcptw-&gt;tw_ts_recent_stamp &gt; 1))) {
            ......
            return 1;
        }

        return 0;
    }
</code></pre>

<p>我们前面提到过，<code>__inet_hash_connect()</code>函数调用check_established指向的函数时第三个参数为NULL，所以现在我们只需要关心tcptw->tw_ts_recent_stamp是否非零，只要这个值非零，tcp_twsk_unique()就会返回true， 在上层connect（）函数中就会返回EADDRNOTVAIL错误。tcptw->tw_ts_recent_stamp存储的是最近接收到段的时间戳值，所以正常情况下这个值不会为零。当然也可以通过调整系统的参数，让这个值可以为零，这不是本文讨论的重点，感兴趣的可以参考tcp_v4_connect()中的代码进行修改。</p>

<p>在导致返回EADDRNOTVAIL的两种情况中，第一种情况可以有办法避免，但是如果的第二次建立连接的时间和第一次关闭连接之间的时间间隔太小的话，此时第一个连接可能处在FIN_WAIT_1、FIN_WAIT_2等状态，此时没有系统参数可以用来避免返回EADDRNOTVAIL。如果你还是想无论如何都要在很短的时间内重用客户端的端口，这样也有办法，要么是用kprobe机制，要么用systemtap脚本，改变<code>__inet_check_established()</code>函数的返回值。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[内核处理time_wait状态详解]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/29/kernel-net-timewait/"/>
    <updated>2015-09-29T17:40:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/29/kernel-net-timewait</id>
    <content type="html"><![CDATA[<p><a href="http://simohayha.iteye.com/blog/566980">http://simohayha.iteye.com/blog/566980</a></p>

<p>这次来详细看内核的time_wait状态的实现，在前面介绍定时器的时候，time_wait就简单的介绍了下。这里我们会先介绍tw状态的实现，然后来介绍内核协议栈如何处理tw状态。</p>

<p>首先我们要知道在linux内核中time_wait的处理是由tcp_time_wait这个函数来做得，比如我们在closing状态收到一个fin，就会调用tcp_time_wait.而内核为time_wait状态的socket专门设计了一个结构就是inet_timewait_sock，并且是挂载在inet_ehash_bucket的tw上(这个结构前面也已经介绍过了)。这里要注意，端口号的那个hash链表中也是会保存time_wait状态的socket的。</p>

<pre><code>    struct inet_timewait_sock {  

        //common也就是包含了一些socket的必要信息。  
        struct sock_common  __tw_common;  
    #define tw_family       __tw_common.skc_family  
    #define tw_state        __tw_common.skc_state  
    #define tw_reuse        __tw_common.skc_reuse  
    #define tw_bound_dev_if     __tw_common.skc_bound_dev_if  
    #define tw_node         __tw_common.skc_nulls_node  
    #define tw_bind_node        __tw_common.skc_bind_node  
    #define tw_refcnt       __tw_common.skc_refcnt  
    #define tw_hash         __tw_common.skc_hash  
    #define tw_prot         __tw_common.skc_prot  
    #define tw_net          __tw_common.skc_net  

         //tw状态的超时时间  
        int         tw_timeout;  
        //这个用来标记我们是正常进入tw还是说由于超时等一系列原因进入(比如超时等一系列原因)  
        volatile unsigned char  tw_substate;  
        /* 3 bits hole, try to pack */  
        //和tcp option中的接收窗口比例类似  
        unsigned char       tw_rcv_wscale;  

        //也就是标示sock的4个域，源目的端口和地址  
        __be16          tw_sport;  
        __be32          tw_daddr __attribute__((aligned(INET_TIMEWAIT_ADDRCMP_ALIGN_BYTES)));  
        __be32          tw_rcv_saddr;  
        __be16          tw_dport;  
        //本地端口。  
        __u16           tw_num;  
        kmemcheck_bitfield_begin(flags);  
        /* And these are ours. */  
        //几个标记位。  
        unsigned int        tw_ipv6only     : 1,  
                    tw_transparent  : 1,  
                    tw_pad      : 14,   /* 14 bits hole */  
                    tw_ipv6_offset  : 16;  
        kmemcheck_bitfield_end(flags);  
        unsigned long       tw_ttd;  
        //链接到端口的hash表中。  
        struct inet_bind_bucket *tw_tb;  
        //链接到全局的tw状态hash表中。  
        struct hlist_node   tw_death_node;  
    };  
</code></pre>

<p>然后我们要知道linux有两种方式执行tw状态的socket，一种是等待2×MSL时间（内核中是60秒)，一种是基于RTO来计算超时时间。</p>

<p>基于RTO的超时时间也叫做recycle模式，这里内核会通过sysctl_tw_recycle（也就是说我们能通过sysctl来打开这个值)以及是否我们还保存有从对端接收到的最近的数据包的时间戳来判断是否进入打开recycle模式的处理。如果进入则会调用tcp_v4_remember_stamp来得到是否打开recycle模式。</p>

<p>下面就是这个片断的代码片断：</p>

<pre><code>    //必须要设置sysctl_tw_recycle以及保存有最后一次的时间戳.  
    if (tcp_death_row.sysctl_tw_recycle &amp;&amp; tp-&gt;rx_opt.ts_recent_stamp)  
        //然后调用remember_stamp（在ipv4中被初始化为tcp_v4_remember_stamp）来得到是否要打开recycle模式。  
        recycle_ok = icsk-&gt;icsk_af_ops-&gt;remember_stamp(sk);  
</code></pre>

<p>然后我们来看tcp_v4_remember_stamp，在看这个之前，我们需要理解inet_peer结构，这个结构也就是保存了何当前主机通信的主机的一些信息，我前面在分析ip层的时候有详细分析这个结构，因此可以看我前面的blog：</p>

<p><a href="http://simohayha.iteye.com/blog/437695">http://simohayha.iteye.com/blog/437695</a></p>

<p>tcp_v4_remember_stamp主要用来从全局的inet_peer中得到对应的当前sock的对端的信息(通过ip地址).然后设置相关的时间戳(tcp_ts_stamp和tcp_ts）.这里要特别注意一个东西，那就是inet_peer是ip层的东西，因此它的key是ip地址，它会忽略端口号。所以说这里inet_peer的这两个时间戳是专门为解决tw状态而设置的。</p>

<p>然后我们来看下tcp option的几个关键的域：</p>

<pre><code>    struct tcp_options_received {  
        /*  PAWS/RTTM data  */  

        //这个值为我们本机更新ts_recent的时间  
        long    ts_recent_stamp;  
        //这个表示最近接收的那个数据包的时间戳  
        u32 ts_recent;  
        //这个表示这个数据包发送时的时间戳    
        u32 rcv_tsval;  
        //这个表示当前数据包所回应的数据包的时间戳。  
        u32 rcv_tsecr;  
        //如果上面两个时间戳都有设置，则saw_tstamp设置为1.  
        u16 saw_tstamp : 1,   //TIMESTAMP seen on SYN packet  
            tstamp_ok : 1,    //d-scack标记  
            dsack : 1,        //Wscale seen on SYN packet  
            wscale_ok : 1,    //SACK seen on SYN packet     
            sack_ok : 4,      //下面两个是窗口扩大倍数，主要是为了解决一些特殊网络下大窗口的问题。  
            snd_wscale : 4,   
            rcv_wscale : 4;   
        /*  SACKs data  */  
        u8  num_sacks;    
        u16 user_mss;     
        u16 mss_clamp;    
    };  
</code></pre>

<p>而inet_peer中的两个时间戳与option中的ts_recent和ts_recent_stamp类似。</p>

<p>来看tcp_v4_remember_stamp的实现：</p>

<pre><code>    int tcp_v4_remember_stamp(struct sock *sk)  
    {  
        struct inet_sock *inet = inet_sk(sk);  
        struct tcp_sock *tp = tcp_sk(sk);  
        struct rtable *rt = (struct rtable *)__sk_dst_get(sk);  
        struct inet_peer *peer = NULL;  
        int release_it = 0;  

        //得到对应peer（两种得到的方式)。  
        if (!rt || rt-&gt;rt_dst != inet-&gt;daddr) {  
            peer = inet_getpeer(inet-&gt;daddr, 1);  
            release_it = 1;  
        } else {  
            if (!rt-&gt;peer)  
                rt_bind_peer(rt, 1);  
            peer = rt-&gt;peer;  
        }  

        //如果peer不存在则会返回0,也就是关闭recycle模式。  
        if (peer) {  
        //这里tcp_ts以及tcp_ts_stamp保存的是最新的时间戳，所以这里与当前的sock的时间戳比较小的话就要更新。  
        if ((s32)(peer-&gt;tcp_ts - tp-&gt;rx_opt.ts_recent) &lt;= 0 ||(peer-&gt;tcp_ts_stamp + TCP_PAWS_MSL &lt; get_seconds() &amp;&amp;  
         peer-&gt;tcp_ts_stamp &lt;= tp-&gt;rx_opt.ts_recent_stamp)) {  

        //更新时间戳。  
        peer-&gt;tcp_ts_stamp = tp-&gt;rx_opt.ts_recent_stamp;  
            peer-&gt;tcp_ts = tp-&gt;rx_opt.ts_recent;  
            }  
            if (release_it)  
                inet_putpeer(peer);  
            return 1;  
        }  

        //关闭recycle模式。  
        return 0;  
    }  
</code></pre>

<p>ok,我们来看tcp_time_wait的实现，这里删掉了ipv6以及md5的部分：</p>

<pre><code>    //这里也就是2*MSL=60秒。  
    #define TCP_TIMEWAIT_LEN (60*HZ)   

    //这里的state标记我们是正常进入tw状态，还是由于死在fin-wait-2状态才进入tw状态的。  
    void tcp_time_wait(struct sock *sk, int state, int timeo)  
    {  
        //TW的socket  
        struct inet_timewait_sock *tw = NULL;  
        const struct inet_connection_sock *icsk = inet_csk(sk);  
        const struct tcp_sock *tp = tcp_sk(sk);  

        //recycle模式的标记。  
        int recycle_ok = 0;  

        //上面已经分析过了。  
        if (tcp_death_row.sysctl_tw_recycle &amp;&amp; tp-&gt;rx_opt.ts_recent_stamp)  
        recycle_ok = icsk-&gt;icsk_af_ops-&gt;remember_stamp(sk);  
        //然后判断tw状态的sock数量是否已经超过限制。  
        if (tcp_death_row.tw_count &lt; tcp_death_row.sysctl_max_tw_buckets)  
        //没有的话alloc一个新的。  
            tw = inet_twsk_alloc(sk, state);  

        //如果tw不为空才会进入处理。  
        if (tw != NULL) {  
            struct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw);  
            //计算对应的超时时间，这里可以看到刚好是3.5*rto.  
            const int rto = (icsk-&gt;icsk_rto &lt;&lt; 2) - (icsk-&gt;icsk_rto &gt;&gt; 1);  
            //更新对应的域。  
            tw-&gt;tw_rcv_wscale    = tp-&gt;rx_opt.rcv_wscale;  
            tcptw-&gt;tw_rcv_nxt    = tp-&gt;rcv_nxt;  
            tcptw-&gt;tw_snd_nxt    = tp-&gt;snd_nxt;  
            tcptw-&gt;tw_rcv_wnd    = tcp_receive_window(tp);  
            tcptw-&gt;tw_ts_recent  = tp-&gt;rx_opt.ts_recent;  
            tcptw-&gt;tw_ts_recent_stamp = tp-&gt;rx_opt.ts_recent_stamp;  

            //更新链表(下面会分析)。  
            __inet_twsk_hashdance(tw, sk, &amp;tcp_hashinfo);  
            //如果传递进来的超时时间小于我们计算的，则让他等于我们计算的超时时间。  
            /* Get the TIME_WAIT timeout firing. */  
            if (timeo &lt; rto)  
                timeo = rto;  

            //如果打开recycle模式，则超时时间为我们基于rto计算的时间。  
            if (recycle_ok) {  
                tw-&gt;tw_timeout = rto;  
            } else {  
                //否则为2*MSL=60秒  
                tw-&gt;tw_timeout = TCP_TIMEWAIT_LEN;  
                //如果正常进入则timeo也就是超时时间为2*MSL.  
                if (state == TCP_TIME_WAIT)  
                    timeo = TCP_TIMEWAIT_LEN;  
            }  

            //最关键的一个函数，我们后面会详细分析。  
            inet_twsk_schedule(tw, &amp;tcp_death_row, timeo,  
                       TCP_TIMEWAIT_LEN);  
            //更新引用计数。  
            inet_twsk_put(tw);  
        } else {  
            LIMIT_NETDEBUG(KERN_INFO "TCP: time wait bucket table overflow\n");  
        }  
        tcp_update_metrics(sk);  
        tcp_done(sk);  
    }  
</code></pre>

<p>然后我们来看__inet_twsk_hashdance函数，这个函数主要是用于更新对应的全局hash表。有关这几个hash表的结构可以去看我前面的blog。</p>

<pre><code>    void __inet_twsk_hashdance(struct inet_timewait_sock *tw, struct sock *sk,  
                   struct inet_hashinfo *hashinfo)  
    {  
        const struct inet_sock *inet = inet_sk(sk);  
        const struct inet_connection_sock *icsk = inet_csk(sk);  
        //得到ehash。  
        struct inet_ehash_bucket *ehead = inet_ehash_bucket(hashinfo, sk-&gt;sk_hash);  
        spinlock_t *lock = inet_ehash_lockp(hashinfo, sk-&gt;sk_hash);  
        struct inet_bind_hashbucket *bhead;  

        //下面这几步是将tw sock链接到bhash中。  
        bhead = &amp;hashinfo-&gt;bhash[inet_bhashfn(twsk_net(tw), inet-&gt;num,hashinfo-&gt;bhash_size)];  
        spin_lock(&amp;bhead-&gt;lock);  
        //链接到bhash。这里icsk的icsk_bind_hash也就是bash的一个元素。  
        tw-&gt;tw_tb = icsk-&gt;icsk_bind_hash;  
        WARN_ON(!icsk-&gt;icsk_bind_hash);  
        //将tw加入到bash中。  
        inet_twsk_add_bind_node(tw, &amp;tw-&gt;tw_tb-&gt;owners);  
        spin_unlock(&amp;bhead-&gt;lock);  

        spin_lock(lock);  


        atomic_inc(&amp;tw-&gt;tw_refcnt);  
        //将tw sock加入到ehash的tw chain中。  
        inet_twsk_add_node_rcu(tw, &amp;ehead-&gt;twchain);  

        //然后从全局的establish hash中remove掉这个socket。详见sock的sk_common域。  
        if (__sk_nulls_del_node_init_rcu(sk))  
            sock_prot_inuse_add(sock_net(sk), sk-&gt;sk_prot, -1);  

        spin_unlock(lock);  
    }  
</code></pre>

<p>这里我们要知道还有一个专门的全局的struct inet_timewait_death_row类型的变量tcp_death_row来保存所有的tw状态的socket。而整个tw状态的socket并不是全部加入到定时器中，而是将tcp_death_row加入到定时器中，然后每次定时器超时通过tcp_death_row来查看定时器的超时情况，从而处理tw状态的sock。</p>

<p>而这里定时器分为两种，一种是长时间的定时器，它也就是tw_timer域，一种是短时间的定时器，它也就是twcal_timer域。</p>

<p>而这里还有两个hash表，一个是twcal_row，它对应twcal_timer这个定时器，也就是说当twcal_timer超时，它就会从twcal_row中取得对应的twsock。对应的cells保存的就是tw_timer定时器超时所用的twsock。</p>

<p>还有两个slot，一个是slot域，一个是twcal_hand域，分别表示当前对应的定时器(上面介绍的两个)所正在执行的定时器的slot。</p>

<p>而上面所说的recycle模式也就是指twcal_timer定时器。</p>

<p>来看结构。
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
<span class='line-number'>193</span>
<span class='line-number'>194</span>
<span class='line-number'>195</span>
<span class='line-number'>196</span>
<span class='line-number'>197</span>
<span class='line-number'>198</span>
<span class='line-number'>199</span>
<span class='line-number'>200</span>
<span class='line-number'>201</span>
<span class='line-number'>202</span>
<span class='line-number'>203</span>
<span class='line-number'>204</span>
<span class='line-number'>205</span>
<span class='line-number'>206</span>
<span class='line-number'>207</span>
<span class='line-number'>208</span>
<span class='line-number'>209</span>
<span class='line-number'>210</span>
<span class='line-number'>211</span>
<span class='line-number'>212</span>
<span class='line-number'>213</span>
<span class='line-number'>214</span>
<span class='line-number'>215</span>
<span class='line-number'>216</span>
<span class='line-number'>217</span>
<span class='line-number'>218</span>
<span class='line-number'>219</span>
<span class='line-number'>220</span>
<span class='line-number'>221</span>
<span class='line-number'>222</span>
<span class='line-number'>223</span>
<span class='line-number'>224</span>
<span class='line-number'>225</span>
<span class='line-number'>226</span>
<span class='line-number'>227</span>
<span class='line-number'>228</span>
<span class='line-number'>229</span>
<span class='line-number'>230</span>
<span class='line-number'>231</span>
<span class='line-number'>232</span>
<span class='line-number'>233</span>
<span class='line-number'>234</span>
<span class='line-number'>235</span>
<span class='line-number'>236</span>
<span class='line-number'>237</span>
<span class='line-number'>238</span>
<span class='line-number'>239</span>
<span class='line-number'>240</span>
<span class='line-number'>241</span>
<span class='line-number'>242</span>
<span class='line-number'>243</span>
<span class='line-number'>244</span>
<span class='line-number'>245</span>
<span class='line-number'>246</span>
<span class='line-number'>247</span>
<span class='line-number'>248</span>
<span class='line-number'>249</span>
<span class='line-number'>250</span>
<span class='line-number'>251</span>
<span class='line-number'>252</span>
<span class='line-number'>253</span>
<span class='line-number'>254</span>
<span class='line-number'>255</span>
<span class='line-number'>256</span>
<span class='line-number'>257</span>
<span class='line-number'>258</span>
<span class='line-number'>259</span>
<span class='line-number'>260</span>
<span class='line-number'>261</span>
<span class='line-number'>262</span>
<span class='line-number'>263</span>
<span class='line-number'>264</span>
<span class='line-number'>265</span>
<span class='line-number'>266</span>
<span class='line-number'>267</span>
<span class='line-number'>268</span>
<span class='line-number'>269</span>
<span class='line-number'>270</span>
<span class='line-number'>271</span>
<span class='line-number'>272</span>
<span class='line-number'>273</span>
<span class='line-number'>274</span>
<span class='line-number'>275</span>
<span class='line-number'>276</span>
<span class='line-number'>277</span>
<span class='line-number'>278</span>
<span class='line-number'>279</span>
<span class='line-number'>280</span>
<span class='line-number'>281</span>
<span class='line-number'>282</span>
<span class='line-number'>283</span>
<span class='line-number'>284</span>
<span class='line-number'>285</span>
<span class='line-number'>286</span>
<span class='line-number'>287</span>
<span class='line-number'>288</span>
<span class='line-number'>289</span>
<span class='line-number'>290</span>
<span class='line-number'>291</span>
<span class='line-number'>292</span>
<span class='line-number'>293</span>
<span class='line-number'>294</span>
<span class='line-number'>295</span>
<span class='line-number'>296</span>
<span class='line-number'>297</span>
<span class='line-number'>298</span>
<span class='line-number'>299</span>
<span class='line-number'>300</span>
<span class='line-number'>301</span>
<span class='line-number'>302</span>
<span class='line-number'>303</span>
<span class='line-number'>304</span>
<span class='line-number'>305</span>
<span class='line-number'>306</span>
<span class='line-number'>307</span>
<span class='line-number'>308</span>
<span class='line-number'>309</span>
<span class='line-number'>310</span>
<span class='line-number'>311</span>
<span class='line-number'>312</span>
<span class='line-number'>313</span>
<span class='line-number'>314</span>
<span class='line-number'>315</span>
<span class='line-number'>316</span>
<span class='line-number'>317</span>
<span class='line-number'>318</span>
<span class='line-number'>319</span>
<span class='line-number'>320</span>
<span class='line-number'>321</span>
<span class='line-number'>322</span>
<span class='line-number'>323</span>
<span class='line-number'>324</span>
<span class='line-number'>325</span>
<span class='line-number'>326</span>
<span class='line-number'>327</span>
<span class='line-number'>328</span>
<span class='line-number'>329</span>
<span class='line-number'>330</span>
<span class='line-number'>331</span>
<span class='line-number'>332</span>
<span class='line-number'>333</span>
<span class='line-number'>334</span>
<span class='line-number'>335</span>
<span class='line-number'>336</span>
<span class='line-number'>337</span>
<span class='line-number'>338</span>
<span class='line-number'>339</span>
<span class='line-number'>340</span>
<span class='line-number'>341</span>
<span class='line-number'>342</span>
<span class='line-number'>343</span>
<span class='line-number'>344</span>
<span class='line-number'>345</span>
<span class='line-number'>346</span>
<span class='line-number'>347</span>
<span class='line-number'>348</span>
<span class='line-number'>349</span>
<span class='line-number'>350</span>
<span class='line-number'>351</span>
<span class='line-number'>352</span>
<span class='line-number'>353</span>
<span class='line-number'>354</span>
<span class='line-number'>355</span>
<span class='line-number'>356</span>
<span class='line-number'>357</span>
<span class='line-number'>358</span>
<span class='line-number'>359</span>
<span class='line-number'>360</span>
<span class='line-number'>361</span>
<span class='line-number'>362</span>
<span class='line-number'>363</span>
<span class='line-number'>364</span>
<span class='line-number'>365</span>
<span class='line-number'>366</span>
<span class='line-number'>367</span>
<span class='line-number'>368</span>
<span class='line-number'>369</span>
<span class='line-number'>370</span>
<span class='line-number'>371</span>
<span class='line-number'>372</span>
<span class='line-number'>373</span>
<span class='line-number'>374</span>
<span class='line-number'>375</span>
<span class='line-number'>376</span>
<span class='line-number'>377</span>
<span class='line-number'>378</span>
<span class='line-number'>379</span>
<span class='line-number'>380</span>
<span class='line-number'>381</span>
<span class='line-number'>382</span>
<span class='line-number'>383</span>
<span class='line-number'>384</span>
<span class='line-number'>385</span>
<span class='line-number'>386</span>
<span class='line-number'>387</span>
<span class='line-number'>388</span>
<span class='line-number'>389</span>
<span class='line-number'>390</span>
<span class='line-number'>391</span>
<span class='line-number'>392</span>
<span class='line-number'>393</span>
<span class='line-number'>394</span>
<span class='line-number'>395</span>
<span class='line-number'>396</span>
<span class='line-number'>397</span>
<span class='line-number'>398</span>
<span class='line-number'>399</span>
<span class='line-number'>400</span>
<span class='line-number'>401</span>
<span class='line-number'>402</span>
<span class='line-number'>403</span>
<span class='line-number'>404</span>
<span class='line-number'>405</span>
<span class='line-number'>406</span>
<span class='line-number'>407</span>
<span class='line-number'>408</span>
<span class='line-number'>409</span>
<span class='line-number'>410</span>
<span class='line-number'>411</span>
<span class='line-number'>412</span>
<span class='line-number'>413</span>
<span class='line-number'>414</span>
<span class='line-number'>415</span>
<span class='line-number'>416</span>
<span class='line-number'>417</span>
<span class='line-number'>418</span>
<span class='line-number'>419</span>
<span class='line-number'>420</span>
<span class='line-number'>421</span>
<span class='line-number'>422</span>
<span class='line-number'>423</span>
<span class='line-number'>424</span>
<span class='line-number'>425</span>
<span class='line-number'>426</span>
<span class='line-number'>427</span>
<span class='line-number'>428</span>
<span class='line-number'>429</span>
<span class='line-number'>430</span>
<span class='line-number'>431</span>
<span class='line-number'>432</span>
<span class='line-number'>433</span>
<span class='line-number'>434</span>
<span class='line-number'>435</span>
<span class='line-number'>436</span>
<span class='line-number'>437</span>
<span class='line-number'>438</span>
<span class='line-number'>439</span>
<span class='line-number'>440</span>
<span class='line-number'>441</span>
<span class='line-number'>442</span>
<span class='line-number'>443</span>
<span class='line-number'>444</span>
<span class='line-number'>445</span>
<span class='line-number'>446</span>
<span class='line-number'>447</span>
<span class='line-number'>448</span>
<span class='line-number'>449</span>
<span class='line-number'>450</span>
<span class='line-number'>451</span>
<span class='line-number'>452</span>
<span class='line-number'>453</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>struct inet_timewait_death_row {&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    //这几个域会在tcp_death_row中被初始化。  
</span><span class='line'>int         twcal_hand;  
</span><span class='line'>unsigned long       twcal_jiffie;  
</span><span class='line'>//短时间定时器。  
</span><span class='line'>struct timer_list   twcal_timer;  
</span><span class='line'>//twcal_timer定时器对应的hash表  
</span><span class='line'>struct hlist_head   twcal_row[INET_TWDR_RECYCLE_SLOTS];  
</span><span class='line'>
</span><span class='line'>spinlock_t      death_lock;  
</span><span class='line'>//tw的个数。  
</span><span class='line'>int         tw_count;  
</span><span class='line'>//超时时间。  
</span><span class='line'>int         period;  
</span><span class='line'>u32         thread_slots;  
</span><span class='line'>struct work_struct  twkill_work;  
</span><span class='line'>//长时间定时器  
</span><span class='line'>struct timer_list   tw_timer;  
</span><span class='line'>int         slot;  
</span><span class='line'>
</span><span class='line'>//短时间的定时器对应的hash表  
</span><span class='line'>struct hlist_head   cells[INET_TWDR_TWKILL_SLOTS];  
</span><span class='line'>struct inet_hashinfo    *hashinfo;  
</span><span class='line'>int         sysctl_tw_recycle;  
</span><span class='line'>int         sysctl_max_tw_buckets;  
</span><span class='line'>};  
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>
</span><span class='line'>这里要注意INET_TWDR_TWKILL_SLOTS为8,而INET_TWDR_RECYCLE_SLOTS为32。
</span><span class='line'>
</span><span class='line'>ok我们接着来看tcp_death_row的初始化。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;struct inet_timewait_death_row tcp_death_row = {  
</span><span class='line'>
</span><span class='line'>//最大桶的个数。  
</span><span class='line'>.sysctl_max_tw_buckets = NR_FILE * 2,  
</span><span class='line'>//超时时间，  
</span><span class='line'>.period     = TCP_TIMEWAIT_LEN / INET_TWDR_TWKILL_SLOTS,  
</span><span class='line'>//锁  
</span><span class='line'>.death_lock = __SPIN_LOCK_UNLOCKED(tcp_death_row.death_lock),  
</span><span class='line'>
</span><span class='line'>//可以看到它是链接到全局的inet_hashinfo中的。  
</span><span class='line'>.hashinfo   = &amp;tcp_hashinfo,  
</span><span class='line'>//定时器，这里要注意超时函数。  
</span><span class='line'>.tw_timer   = TIMER_INITIALIZER(inet_twdr_hangman, 0,(unsigned long)&amp;tcp_death_row),  
</span><span class='line'>//工作队列。其实也就是销毁twsock工作的工作队列。  
</span><span class='line'>.twkill_work    = __WORK_INITIALIZER(tcp_death_row.twkill_work,                   inet_twdr_twkill_work),  
</span><span class='line'>/* Short-time timewait calendar */  
</span><span class='line'>
</span><span class='line'>//twcal_hand用来标记twcal_timer定时器是否还在工作。  
</span><span class='line'>.twcal_hand = -1,  
</span><span class='line'>.twcal_timer    = TIMER_INITIALIZER(inet_twdr_twcal_tick, 0,(unsigned long)&amp;tcp_death_row),  
</span><span class='line'>};  
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>然后就是inet_twsk_schedule的实现，这个函数也就是tw状态的处理函数。他主要是用来基于超时时间来计算当前twsock的可用的位置。也就是来判断启动那个定时器，然后加入到那个队列。
</span><span class='line'>
</span><span class='line'>因此这里的关键就是slot的计算。这里slot的计算是根据我们传递进来的timeo来计算的。
</span><span class='line'>
</span><span class='line'>recycle模式下tcp_death_row的超时时间的就为2的INET_TWDR_RECYCLE_TICK幂。
</span><span class='line'>
</span><span class='line'>我们一般桌面的hz为100,来看对应的值：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;#elif HZ &lt;= 128  
</span><span class='line'># define INET_TWDR_RECYCLE_TICK (7 + 2 - INET_TWDR_RECYCLE_SLOTS_LOG)  
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>
</span><span class='line'>可以看到这时它的值就为4.
</span><span class='line'>
</span><span class='line'>而tw_timer的slot也就是长时间定时器的slot的计算是这样的，它也就是用我们传递进来的超时时间timeo/16(可以看到就是2的INET_TWDR_RECYCLE_TICK次方)然后向上取整。
</span><span class='line'>
</span><span class='line'>而这里twdr的period被设置为
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;TCP_TIMEWAIT_LEN / INET_TWDR_TWKILL_SLOTS,  
</span><span class='line'>
</span><span class='line'>//取slot的代码片断。  
</span><span class='line'>slot = DIV_ROUND_UP(timeo, twdr-&gt;period);  
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>
</span><span class='line'>而我们下面取slot的时候也就是会用这个值来散列。可以看到散列表的桶的数目就为INET_TWDR_TWKILL_SLOTS个，因此这里也就是把时间分为INET_TWDR_TWKILL_SLOTS份，每一段时间内的超时twsock都放在一个桶里面，而大于60秒的都放在最后一个桶。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;void inet_twsk_schedule(struct inet_timewait_sock *tw,  
</span><span class='line'>           struct inet_timewait_death_row *twdr,  
</span><span class='line'>           const int timeo, const int timewait_len)  
</span><span class='line'>{  
</span><span class='line'>struct hlist_head *list;  
</span><span class='line'>int slot;  
</span><span class='line'>
</span><span class='line'>//得到slot。  
</span><span class='line'>slot = (timeo + (1 &lt;&lt; INET_TWDR_RECYCLE_TICK) - 1) &gt;&gt; INET_TWDR_RECYCLE_TICK;  
</span><span class='line'>
</span><span class='line'>spin_lock(&amp;twdr-&gt;death_lock);  
</span><span class='line'>
</span><span class='line'>/* Unlink it, if it was scheduled */  
</span><span class='line'>if (inet_twsk_del_dead_node(tw))  
</span><span class='line'>    twdr-&gt;tw_count--;  
</span><span class='line'>else  
</span><span class='line'>    atomic_inc(&amp;tw-&gt;tw_refcnt);  
</span><span class='line'>
</span><span class='line'>//判断该添加到那个定时器。  
</span><span class='line'>if (slot &gt;= INET_TWDR_RECYCLE_SLOTS) {  
</span><span class='line'>    /* Schedule to slow timer */  
</span><span class='line'>    //如果大于timewait_len也就是2*MSL=60秒，则slot为cells的最后一项。  
</span><span class='line'>    if (timeo &gt;= timewait_len) {  
</span><span class='line'>        //设为最后一项。  
</span><span class='line'>        slot = INET_TWDR_TWKILL_SLOTS - 1;  
</span><span class='line'>    } else {  
</span><span class='line'>        //否则timeo除于period然后向上取整。  
</span><span class='line'>        slot = DIV_ROUND_UP(timeo, twdr-&gt;period);  
</span><span class='line'>        //如果大于cells的桶的大小，则也是放到最后一个位置。  
</span><span class='line'>        if (slot &gt;= INET_TWDR_TWKILL_SLOTS)  
</span><span class='line'>            slot = INET_TWDR_TWKILL_SLOTS - 1;  
</span><span class='line'>    }  
</span><span class='line'>    //然后设置超时时间，  
</span><span class='line'>    tw-&gt;tw_ttd = jiffies + timeo;  
</span><span class='line'>    //而twdr的slot为当前正在处理的slot，因此我们需要以这个slot为基准来计算真正的slot  
</span><span class='line'>    slot = (twdr-&gt;slot + slot) &amp; (INET_TWDR_TWKILL_SLOTS - 1);  
</span><span class='line'>    //最后取得对应的链表。  
</span><span class='line'>    list = &amp;twdr-&gt;cells[slot];  
</span><span class='line'>} else {  
</span><span class='line'>    //设置应当超时的时间。  
</span><span class='line'>    tw-&gt;tw_ttd = jiffies + (slot &lt;&lt; INET_TWDR_RECYCLE_TICK);  
</span><span class='line'>    //判断定时器是否还在工作。如果是第一次我们一定会进入下面的处理  
</span><span class='line'>    if (twdr-&gt;twcal_hand &lt; 0) {  
</span><span class='line'>        //如果没有或者第一次进入，则修改定时器然后重新启动定时器  
</span><span class='line'>        twdr-&gt;twcal_hand = 0;  
</span><span class='line'>        twdr-&gt;twcal_jiffie = jiffies;  
</span><span class='line'>        //定时器的超时时间。可以看到时间为我们传进来的timeo(只不过象tick对齐了)  
</span><span class='line'>        twdr-&gt;twcal_timer.expires = twdr-&gt;twcal_jiffie +(slot &lt;&lt; INET_TWDR_RECYCLE_TICK);  
</span><span class='line'>        //重新添加定时器。  
</span><span class='line'>        add_timer(&amp;twdr-&gt;twcal_timer);  
</span><span class='line'>    } else {  
</span><span class='line'>        //如果原本超时时间太小，则修改定时器的超时时间  
</span><span class='line'>        if (time_after(twdr-&gt;twcal_timer.expires,  
</span><span class='line'>                jiffies + (slot &lt;&lt; INET_TWDR_RECYCLE_TICK)))  
</span><span class='line'>            mod_timer(&amp;twdr-&gt;twcal_timer,  
</span><span class='line'>                    jiffies + (slot &lt;&lt; INET_TWDR_RECYCLE_TICK));  
</span><span class='line'>
</span><span class='line'>            //和上面的tw_timer定时器类似，我们要通过当前正在执行的slot也就是twcal_hand来得到真正的slot。  
</span><span class='line'>            slot = (twdr-&gt;twcal_hand + slot) &amp; (INET_TWDR_RECYCLE_SLOTS - 1);  
</span><span class='line'>        }  
</span><span class='line'>    //取得该插入的桶。  
</span><span class='line'>    list = &amp;twdr-&gt;twcal_row[slot];  
</span><span class='line'>}  
</span><span class='line'>
</span><span class='line'>//将tw加入到对应的链表中。  
</span><span class='line'>hlist_add_head(&amp;tw-&gt;tw_death_node, list);  
</span><span class='line'>//如果第一次则启动定时器。  
</span><span class='line'>if (twdr-&gt;tw_count++ == 0)  
</span><span class='line'>    mod_timer(&amp;twdr-&gt;tw_timer, jiffies + twdr-&gt;period);  
</span><span class='line'>spin_unlock(&amp;twdr-&gt;death_lock);  
</span><span class='line'>}  
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>
</span><span class='line'>我们先来总结一下上面的代码。当我们进入tw状态，然后我们会根据计算出来的timeo的不同来加载到不同的hash表中。而对应的定时器一个（tw_timer)是每peroid启动一次，一个是每(slot &lt;&lt; INET_TWDR_RECYCLE_TICK)启动一次。
</span><span class='line'>
</span><span class='line'>下面的两张图很好的表示了recycle模式(twcal定时器)和非recycle模式的区别：
</span><span class='line'>
</span><span class='line'>先是非recycle模式： 
</span><span class='line'>
</span><span class='line'>![](/images/kernel/2015-09-29-1.jpeg)
</span><span class='line'>
</span><span class='line'>然后是recycle模式： 
</span><span class='line'>
</span><span class='line'>![](/images/kernel/2015-09-29-2.jpeg)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>接下来我们来看两个超时函数的实现，这里我只简单的介绍下两个超时函数，一个是inet_twdr_hangman，一个是inet_twdr_twcal_tick。
</span><span class='line'>
</span><span class='line'>在inet_twdr_hangman中，每次只是遍历对应的slot的队列，然后将队列中的所有sock删除，同时也从bind_hash中删除对应的端口信息。这个函数就不详细分析了。
</span><span class='line'>
</span><span class='line'>而在inet_twdr_twcal_tick中，每次遍历所有的twcal_row，然后超时的进行处理(和上面一样),然后没有超时的继续处理).
</span><span class='line'>
</span><span class='line'>这里有一个j的计算要注意，前面我们知道我们的twcal的超时时间可以说都是以INET_TWDR_RECYCLE_SLOTS对齐的，而我们这里在处理超时的同时，有可能上面又有很多sock加入到了tw状态，因此这里我们的超时检测的间隔就是1 &lt;&lt; INET_TWDR_RECYCLE_TICK。
</span><span class='line'>
</span><span class='line'>来看inet_twdr_twcal_tick的实现：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;void inet_twdr_twcal_tick(unsigned long data)  
</span><span class='line'>{  
</span><span class='line'>............................  
</span><span class='line'>if (twdr-&gt;twcal_hand &lt; 0)  
</span><span class='line'>    goto out;  
</span><span class='line'>
</span><span class='line'>//得到slot。  
</span><span class='line'>slot = twdr-&gt;twcal_hand;  
</span><span class='line'>//得到定时器启动时候的jiffes。  
</span><span class='line'>j = twdr-&gt;twcal_jiffie;  
</span><span class='line'>
</span><span class='line'>//遍历所有的twscok。  
</span><span class='line'>for (n = 0; n &lt; INET_TWDR_RECYCLE_SLOTS; n++) {  
</span><span class='line'>    //判断是否超时。  
</span><span class='line'>    if (time_before_eq(j, now)) {  
</span><span class='line'>        //处理超时的socket  
</span><span class='line'>        struct hlist_node *node, *safe;  
</span><span class='line'>        struct inet_timewait_sock *tw;  
</span><span class='line'>        .......................................  
</span><span class='line'>    } else {  
</span><span class='line'>        if (!adv) {  
</span><span class='line'>            adv = 1;  
</span><span class='line'>            twdr-&gt;twcal_jiffie = j;  
</span><span class='line'>            twdr-&gt;twcal_hand = slot;  
</span><span class='line'>        }  
</span><span class='line'>
</span><span class='line'>    //如果不为空，则将重新添加这些定时器  
</span><span class='line'>    if (!hlist_empty(&amp;twdr-&gt;twcal_row[slot])) {  
</span><span class='line'>        mod_timer(&amp;twdr-&gt;twcal_timer, j);  
</span><span class='line'>            goto out;  
</span><span class='line'>        }  
</span><span class='line'>    }  
</span><span class='line'>    //设置间隔  
</span><span class='line'>    j += 1 &lt;&lt; INET_TWDR_RECYCLE_TICK;  
</span><span class='line'>    //更新  
</span><span class='line'>    slot = (slot + 1) &amp; (INET_TWDR_RECYCLE_SLOTS - 1);  
</span><span class='line'>}  
</span><span class='line'>//处理完毕则将twcal_hand设为-1.  
</span><span class='line'>twdr-&gt;twcal_hand = -1;  
</span><span class='line'>
</span><span class='line'>...............................  
</span><span class='line'>}  
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>然后我们来看tcp怎么样进入tw状态。这里分为两种，一种是正常进入也就是在wait2收到一个fin，或者closing收到ack。这种都是比较简单的。我们就不分析了。
</span><span class='line'>
</span><span class='line'>比较特殊的是，我们有可能会直接从wait1进入tw状态，或者是在wait2等待超时也将会直接进入tw状态。这个时候也就是没有收到对端的fin。
</span><span class='line'>
</span><span class='line'>这个主要是为了处理当对端死在close_wait状态的时候，我们需要自己能够恢复到close状态，而不是一直处于wait2状态。
</span><span class='line'>
</span><span class='line'>在看代码之前我们需要知道一个东西，那就是fin超时时间，这个超时时间我们可以通过TCP_LINGER2这个option来设置，并且这个值的最大值是sysctl_tcp_fin_timeout/HZ. 这里可以看到sysctl_tcp_fin_timeout是jiffies数，所以要转成秒。我这里简单的测试了下，linger2的默认值也就是60,刚好是2*MSL.
</span><span class='line'>
</span><span class='line'>这里linger2也就是代表在tcp_wait2的最大生命周期。如果小于0则说明我们要跳过tw状态。
</span><span class='line'>
</span><span class='line'>先来看在tcp_close中的处理，不过这里不理解为什么这么做的原因。
</span><span class='line'>
</span><span class='line'>这里为什么有可能会为wait2状态呢，原因是如果设置了linger，则我们就会休眠掉，而休眠的时间可能我们已经收到ack，此时将会进入wait2的处理。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;if (sk-&gt;sk_state == TCP_FIN_WAIT2) {  
</span><span class='line'>    struct tcp_sock *tp = tcp_sk(sk);  
</span><span class='line'>    //如果小于0,则说明从wait2立即超时此时也就是相当于跳过tw状态，所以我们直接发送rst，然后进入close。  
</span><span class='line'>    if (tp-&gt;linger2 &lt; 0) {  
</span><span class='line'>        tcp_set_state(sk, TCP_CLOSE);  
</span><span class='line'>        tcp_send_active_reset(sk, GFP_ATOMIC);  
</span><span class='line'>        NET_INC_STATS_BH(sock_net(sk),  
</span><span class='line'>                LINUX_MIB_TCPABORTONLINGER);  
</span><span class='line'>    } else {  
</span><span class='line'>        //否则计算fin的时间，这里的超时时间是在linger2和3.5RTO之间取最大值。  
</span><span class='line'>        const int tmo = tcp_fin_time(sk);  
</span><span class='line'>
</span><span class='line'>        //如果超时时间很大，则说明我们需要等待时间很长，因此我们启动keepalive探测对端是否存活。  
</span><span class='line'>        if (tmo &gt; TCP_TIMEWAIT_LEN) {  
</span><span class='line'>        inet_csk_reset_keepalive_timer(sk,  
</span><span class='line'>            tmo - TCP_TIMEWAIT_LEN);  
</span><span class='line'>        } else {  
</span><span class='line'>            //否则我们直接进入tw状态。  
</span><span class='line'>            tcp_time_wait(sk, TCP_FIN_WAIT2, tmo);  
</span><span class='line'>            goto out;  
</span><span class='line'>        }  
</span><span class='line'>    }  
</span><span class='line'>}  
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>
</span><span class='line'>还有从wait1直接进入tw，和上面类似，我就不介绍了。
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>最后我们来看当内核处于tw状态后，再次接收到数据包后如何处理。这里的处理函数就是tcp_timewait_state_process，而他是在tcp_v4_rcv中被调用的，它会先判断是否处于tw状态，如果是的话，进入tw的处理。
</span><span class='line'>
</span><span class='line'>这个函数的返回值分为4种。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;enum tcp_tw_status  
</span><span class='line'>{  
</span><span class='line'>//这个代表我们成功处理了数据包。  
</span><span class='line'>TCP_TW_SUCCESS = 0,  
</span><span class='line'>//我们需要发送给对端一个rst。  
</span><span class='line'>TCP_TW_RST = 1,  
</span><span class='line'>//我们接收到了重传的fin，因此我们需要重传ack。  
</span><span class='line'>TCP_TW_ACK = 2,  
</span><span class='line'>//这个表示我们需要重新建立一个连接。  
</span><span class='line'>TCP_TW_SYN = 3  
</span><span class='line'>};  
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>
</span><span class='line'>这里可能最后一个比较难理解，这里内核注释得很详细，主要是实现了RFC1122:
</span><span class='line'>
</span><span class='line'>引用
</span><span class='line'>
</span><span class='line'>RFC 1122:
</span><span class='line'>
</span><span class='line'>"When a connection is [...] on TIME-WAIT state [...] [a TCP] MAY accept a new SYN from the remote TCP to reopen the connection directly, if it:
</span><span class='line'>(1)  assigns its initial sequence number for the new  connection to be larger than the largest sequence number it used on the previous connection incarnation,and
</span><span class='line'>
</span><span class='line'>(2)  returns to TIME-WAIT state if the SYN turns out
</span><span class='line'>to be an old duplicate".
</span><span class='line'>
</span><span class='line'>来看这段处理代码：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {  
</span><span class='line'>case TCP_TW_SYN: {  
</span><span class='line'>    //取得一个sk。  
</span><span class='line'>    struct sock *sk2 = inet_lookup_listener(dev_net(skb-&gt;dev),&amp;tcp_hashinfo,  
</span><span class='line'>        iph-&gt;daddr, th-&gt;dest,inet_iif(skb));  
</span><span class='line'>    if (sk2) {  
</span><span class='line'>        //从tw中删除，然后继续执行（也就是开始三次握手)。  
</span><span class='line'>        inet_twsk_deschedule(inet_twsk(sk), &amp;tcp_death_row);  
</span><span class='line'>        inet_twsk_put(inet_twsk(sk));  
</span><span class='line'>        sk = sk2;  
</span><span class='line'>        goto process;  
</span><span class='line'>    }  
</span><span class='line'>    /* Fall through to ACK */  
</span><span class='line'>}  
</span><span class='line'>case TCP_TW_ACK:  
</span><span class='line'>    //发送ack  
</span><span class='line'>    tcp_v4_timewait_ack(sk, skb);  
</span><span class='line'>    break;  
</span><span class='line'>//发送给对端rst。  
</span><span class='line'>case TCP_TW_RST:  
</span><span class='line'>    goto no_tcp_socket;  
</span><span class='line'>//处理成功  
</span><span class='line'>case TCP_TW_SUCCESS:;  
</span><span class='line'>}  
</span><span class='line'>goto discard_it;  
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;```&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;tcp_timewait_state_process这个函数具体的实现我就不介绍了，它就是分为两部分，一部分处理tw_substate == TCP_FIN_WAIT2的情况，一部分是正常情况。在前一种情况，我们对于syn的相应是直接rst的。而后一种我们需要判断是否新建连接。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;而对于fin的处理他们也是不一样的，wait2的话，它会将当前的tw重新加入到定时器列表(inet_twsk_schedule).而后一种则只是重新发送ack。&lt;/p&gt;
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[NAPI机制分析]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2015/08/27/kernel-net-napi/"/&gt;
</span><span class='line'>&lt;updated&gt;2015-08-27T01:29:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2015/08/27/kernel-net-napi&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;p&gt;&lt;a href="http://blog.csdn.net/shanshanpt/article/details/20564845"&gt;http://blog.csdn.net/shanshanpt/article/details/20564845&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;NAPI 的核心在于：在一个繁忙网络，每次有网络数据包到达时，不需要都引发中断，因为高频率的中断可能会影响系统的整体效率，假象一个场景，我们此时使用标准的 100M 网卡，可能实际达到的接收速率为 80MBits/s，而此时数据包平均长度为 1500Bytes，则每秒产生的中断数目为：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>80M bits/s / (8 Bits/Byte * 1500 Byte) = 6667 个中断 /s
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;每秒 6667 个中断，对于系统是个很大的压力，此时其实可以转为使用轮询 (polling) 来处理，而不是中断;但轮询在网络流量较小的时没有效率，因此低流量时，基于中断的方式则比较合适，这就是 NAPI 出现的原因，在低流量时候使用中断接收数据包，而在高流量时候则使用基于轮询的方式接收。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;现在内核中 NIC 基本上已经全部支持 NAPI 功能，由前面的叙述可知，NAPI 适合处理高速率数据包的处理，而带来的好处则是：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  1、中断缓和 (Interrupt mitigation)，由上面的例子可以看到，在高流量下，网卡产生的中断可能达到每秒几千次，而如果每次中断都需要系统来处理，是一个很大的压力，而 NAPI 使用轮询时是禁止了网卡的接收中断的，这样会减小系统处理中断的压力；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  2、数据包节流 (Packet throttling)，NAPI 之前的 Linux NIC 驱动总在接收到数据包之后产生一个 IRQ，接着在中断服务例程里将这个 skb 加入本地的 softnet，然后触发本地 NET_RX_SOFTIRQ 软中断后续处理。如果包速过高，因为 IRQ 的优先级高于 SoftIRQ，导致系统的大部分资源都在响应中断，但 softnet 的队列大小有限，接收到的超额数据包也只能丢掉，所以这时这个模型是在用宝贵的系统资源做无用功。而 NAPI 则在这样的情况下，直接把包丢掉，不会继续将需要丢掉的数据包扔给内核去处理，这样，网卡将需要丢掉的数据包尽可能的早丢弃掉，内核将不可见需要丢掉的数据包，这样也减少了内核的压力。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;对NAPI 的使用，一般包括以下的几个步骤：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  1、在中断处理函数中，先禁止接收中断，且告诉网络子系统，将以轮询方式快速收包，其中禁止接收中断完全由硬件功能决定，而告诉内核将以轮询方式处理包则是使用函数 netif_rx_schedule()，也可以使用下面的方式，其中的 netif_rx_schedule_prep 是为了判定现在是否已经进入了轮询模式：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;将网卡预定为轮询模式
</span><span class='line'>&lt;code&gt;
</span><span class='line'>void netif_rx_schedule(struct net_device *dev);
</span><span class='line'>&lt;/code&gt;
</span><span class='line'>或者
</span><span class='line'>&lt;code&gt;
</span><span class='line'>if (netif_rx_schedule_prep(dev))
</span><span class='line'>    __netif_rx_schedule(dev);
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  2、在驱动中创建轮询函数，它的工作是从网卡获取数据包并将其送入到网络子系统，其原型是：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;NAPI 的轮询方法
</span><span class='line'>&lt;code&gt;
</span><span class='line'>int (*poll)(struct net_device *dev, int *budget);
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这里的轮询函数用于在将网卡切换为轮询模式之后，用 poll() 方法处理接收队列中的数据包，如队列为空，则重新切换为中断模式。切换回中断模式需要先关闭轮询模式，使用的是函数 netif_rx_complete ()，接着开启网卡接收中断 .。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;退出轮询模式
</span><span class='line'>&lt;code&gt;
</span><span class='line'>void netif_rx_complete(struct net_device *dev);
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  3、在驱动中创建轮询函数，需要和实际的网络设备 struct net_device 关联起来，这一般在网卡的初始化时候完成，示例代码如下：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;设置网卡支持轮询模式
</span><span class='line'>&lt;code&gt;
</span><span class='line'>dev-&gt;poll = my_poll;
</span><span class='line'>dev-&gt;weight = 64;
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;里面另外一个字段为权重 (weight)，该值并没有一个非常严格的要求，实际上是个经验数据，一般 10Mb 的网卡，我们设置为 16，而更快的网卡，我们则设置为 64。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;NAPI的一些相关Interface&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;下面是 NAPI 功能的一些接口，在前面都基本有涉及，我们简单看看：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    netif_rx_schedule(dev)
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;在网卡的中断处理函数中调用，用于将网卡的接收模式切换为轮询&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    netif_rx_schedule_prep(dev)
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;在网卡是 Up 且运行状态时，将该网卡设置为准备将其加入到轮询列表的状态，可以将该函数看做是 netif_rx_schedule(dev) 的前半部分
</span><span class='line'>&lt;code&gt;
</span><span class='line'>__netif_rx_schedule(dev)
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;将设备加入轮询列表，前提是需要 netif_schedule_prep(dev) 函数已经返回了 1&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    __netif_rx_schedule_prep(dev)
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;与 netif_rx_schedule_prep(dev) 相似，但是没有判断网卡设备是否 Up 及运行，不建议使用&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    netif_rx_complete(dev)
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;用于将网卡接口从轮询列表中移除，一般在轮询函数完成之后调用该函数。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    __netif_rx_complete(dev)
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;Newer newer NAPI&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;其实之前的 NAPI(New API) 这样的命名已经有点让人忍俊不禁了，可见 Linux 的内核极客们对名字的掌控，比对代码的掌控差太多，于是乎，连续的两次对 NAPI 的重构，被戏称为 Newer newer NAPI 了。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;与 netif_rx_complete(dev) 类似，但是需要确保本地中断被禁止&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Newer newer NAPI&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;在最初实现的 NAPI 中，有 2 个字段在结构体 net_device 中，分别为轮询函数 poll() 和权重 weight，而所谓的 Newer newer NAPI，是在 2.6.24 版内核之后，对原有的 NAPI 实现的几次重构，其核心是将 NAPI 相关功能和 net_device 分离，这样减少了耦合，代码更加的灵活，因为 NAPI 的相关信息已经从特定的网络设备剥离了，不再是以前的一对一的关系了。例如有些网络适配器，可能提供了多个 port，但所有的 port 却是共用同一个接受数据包的中断，这时候，分离的 NAPI 信息只用存一份，同时被所有的 port 来共享，这样，代码框架上更好地适应了真实的硬件能力。Newer newer NAPI 的中心结构体是napi_struct:&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;NAPI 结构体</span></code></pre></td></tr></table></div></figure>
    /<em>
     * Structure for NAPI scheduling similar to tasklet but with weighting
    </em>/
    struct napi_struct {
        /<em> The poll_list must only be managed by the entity which
         * changes the state of the NAPI_STATE_SCHED bit.  This means
         * whoever atomically sets that bit can add this napi_struct
         * to the per-cpu poll_list, and whoever clears that bit
         * can remove from the list right before clearing the bit.
         </em>/
        struct list_head      poll_list;</p>

<pre><code>    unsigned long          state; 
    int              weight; 
    int              (*poll)(struct napi_struct *, int); 
 #ifdef CONFIG_NETPOLL 
    spinlock_t          poll_lock; 
    int              poll_owner; 
 #endif 

    unsigned int          gro_count; 

    struct net_device      *dev; 
    struct list_head      dev_list; 
    struct sk_buff          *gro_list; 
    struct sk_buff          *skb; 
};
</code></pre>

<pre><code>
熟悉老的 NAPI 接口实现的话，里面的字段 poll_list、state、weight、poll、dev、没什么好说的，gro_count 和 gro_list 会在后面讲述 GRO 时候会讲述。需要注意的是，与之前的 NAPI 实现的最大的区别是该结构体不再是 net_device 的一部分，事实上，现在希望网卡驱动自己单独分配与管理 napi 实例，通常将其放在了网卡驱动的私有信息，这样最主要的好处在于，如果驱动愿意，可以创建多个 napi_struct，因为现在越来越多的硬件已经开始支持多接收队列 (multiple receive queues)，这样，多个 napi_struct 的实现使得多队列的使用也更加的有效。

与最初的 NAPI 相比较，轮询函数的注册有些变化，现在使用的新接口是：
</code></pre>

<pre><code>void netif_napi_add(struct net_device *dev, struct napi_struct *napi, 
                    int (*poll)(struct napi_struct *, int), int weight)
</code></pre>

<pre><code>
熟悉老的 NAPI 接口的话，这个函数也没什么好说的。

值得注意的是，前面的轮询 poll() 方法原型也开始需要一些小小的改变：
</code></pre>

<pre><code>int (*poll)(struct napi_struct *napi, int budget);
</code></pre>

<pre><code>
大部分 NAPI 相关的函数也需要改变之前的原型，下面是打开轮询功能的 API：
</code></pre>

<pre><code>void netif_rx_schedule(struct net_device *dev, 
                        struct napi_struct *napi); 
/* ...or... */ 
int netif_rx_schedule_prep(struct net_device *dev, 
                        struct napi_struct *napi); 
void __netif_rx_schedule(struct net_device *dev, 
                        struct napi_struct *napi);
</code></pre>

<pre><code>
轮询功能的关闭则需要使用：
</code></pre>

<pre><code>void netif_rx_complete(struct net_device *dev, 
                        struct napi_struct *napi);
</code></pre>

<pre><code>
因为可能存在多个 napi_struct 的实例，要求每个实例能够独立的使能或者禁止，因此，需要驱动作者保证在网卡接口关闭时，禁止所有的 napi_struct 的实例。

函数 netif_poll_enable() 和 netif_poll_disable() 不再需要，因为轮询管理不再和 net_device 直接管理，取而代之的是下面的两个函数：
</code></pre>

<pre><code>void napi_enable(struct napi *napi); 
void napi_disable(struct napi *napi);
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux下ip协议(V4)的实现]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/08/25/kernel-net-ipv4/"/>
    <updated>2015-08-25T23:34:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/08/25/kernel-net-ipv4</id>
    <content type="html"><![CDATA[<p>这次主要介绍的是ip层的切片与组包的实现。</p>

<p>首先来看一下分片好的帧的一些概念：</p>

<p>1 第一个帧的offset位非0并且MF位为1</p>

<p>2 所有的在第一个帧和最后一个帧之间的帧都拥有长度大于0的域</p>

<p>3 最后一个帧MF位为0 并且offset位非0。(这样就能判断是否是最后一个帧了).</p>

<p>这里要注意在linux中，ip头的frag_off域包含了 rfcip头的定义中的nf,df,以及offset域，因此我们每次需要按位与来取得相应的域的值,看下面</p>

<p>ip_local_deliver的代码片段就清楚了：</p>

<pre><code>        // 取出mf位和offset域，从而决定是否要组包。
        if (ip_hdr(skb)-&gt;frag_off &amp; htons(IP_MF | IP_OFFSET)) {
            if (ip_defrag(skb, IP_DEFRAG_LOCAL_DELIVER))
                return 0;
        }
</code></pre>

<p>而fragmentation/defragmentation 子系统的初始化是通过ipfrag_init来实现了，而它是被inet_init来调用的。它主要做的是注册sys文件系统节点，并开启一个定时器，以及初始化一些相关的变量.这个函数的初始化以及相关的数据结构的详细介绍，我们会在后面的组包小节中介绍。现在我们先来看切片的处理。</p>

<p>相对于组包，切片逻辑什么的都比较简单。切片的主要函数是ip_fragment.它的输入包包括下面几种：</p>

<p>1 要被转发的包(没有切片的)。</p>

<p>2 要被转发的包(已经被路由器或者源主机切片了的).</p>

<p>3 被本地函数所创建的buffer，简而言之也就是本地所要传输的数据包(还未加包头)，但是需要被切片的。</p>

<p>而ip_fragment所必须处理下面几种情况：</p>

<p>1 一大块数据需要被分割为更小的部分。</p>

<p>2 一堆数据片段(我的上篇blog有介绍，也就是ip_append_data已经切好的数据包，或者tcp已经切好的数据包)不需要再被切片。</p>

<p>上面的两种情况其实就是看高层(4层)协议有没有做切片工作(按照PMTU）了。如果已经被切片(其实也算不上切片(4层不能处理ip头)，只能说i4层为了ip层更好的处理数据包，从而帮ip层做了一部分工作)，则ip层所做的很简单，就是给每个包加上ip头就可以了。</p>

<p>切片分为两种类型，一种是fast (或者说 efficient)切片，这种也就是4层已经切好片，这里只需要加上ip头就可以了，一种是slow切片，也就是需要现在切片。</p>

<p>下来来看切片的主要任务：</p>

<p>1 将数据包切片为MTU大小(通过ptmu).</p>

<p>2 初始化每一个fragment的ip 头。还要判断一些option的copy位，因为并不是每一种option都要放在所有已切片的fragment 的ip头中的。</p>

<p>3 计算ip层的校验值。</p>

<p>4 通过netfilter过滤。</p>

<p>5 update 一些kernel 域以及snmp 统计值。</p>

<p>接下来来看ip_fragment的具体实现：</p>

<pre><code>    int ip_fragment(struct sk_buff *skb, int (*output)(struct sk_buff*))
</code></pre>

<p>第一个参数skb表示将要被切片的ip包，第二个参数是一个传输切片的输出函数(切片完毕后就交给这个函数处理)。比如ip_finish_output2类似的。</p>

<p>这个函数我们来分段看，首先来看它进行切片前的一些准备工作：</p>

<pre><code>        // 先是取出了一些下面将要使用的变量。
        struct iphdr *iph;
        int raw = 0;
        int ptr;
        struct net_device *dev;
        struct sk_buff *skb2;
        unsigned int mtu, hlen, left, len, ll_rs, pad;
        int offset;
        __be16 not_last_frag;
        // 路由表
        struct rtable *rt = skb-&gt;rtable;
        int err = 0;
        // 网络设备
        dev = rt-&gt;u.dst.dev;

        // ip头
        iph = ip_hdr(skb);
        // 判断DF位，我们知道如果df位被设置了话就表示不要被切片，这时ip_fragment将会发送一个icmp豹纹返回到源主机。这里主要是为forward数据所判断。
        if (unlikely((iph-&gt;frag_off &amp; htons(IP_DF)) &amp;&amp; !skb-&gt;local_df)) {
            IP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGFAILS);
            icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED,
                  htonl(ip_skb_dst_mtu(skb)));
            kfree_skb(skb);
            return -EMSGSIZE;
        }
        // 得到ip头的长度
        hlen = iph-&gt;ihl * 4;
        // 得到mtu的大小。这里要注意，他的大小减去了hlen，也就是ip头的大小。
        mtu = dst_mtu(&amp;rt-&gt;u.dst) - hlen;    /* Size of data space */
        IPCB(skb)-&gt;flags |= IPSKB_FRAG_COMPLETE;
</code></pre>

<p>不管是slow还是fast 被切片的任何一个帧如果传输失败，ip_fragment都会立即返回一个错误给4层，并且紧跟着的帧也不会再被传输，然后将处理方法交给4层去做。</p>

<p>接下来我们来看fast 切片。 一般用fast切片的都是经由4层的ip_append_data和ip_push_pending函数(udp)将数据包已经切片好的，或者是tcp层已经切片好的数据包，才会用fast切片.</p>

<p>这里要主要几个问题：<br/>
1 每一个切片的大小都不能超过PMTU。<br/>
2 只有最后一个切片才会有3层的整个数据包的大小。<br/>
3 每一个切片都必须有足够的大小来允许2层加上自己的头。</p>

<p>我们先看一下skb_pagelen这个函数(下面的处理会用到),这个函数用来得到当前skb的len，首先我们要知道(我前面的blog有介绍)在sk_write_queue的sk_buff队列中，每一个sk_buff的len = x(也就是么一个第一个切片的包的l4 payload的长度) + S1 (这里表示所有的frags域的数据的总大小，也就是data_len的长度)。可以先看下面的图：</p>

<p><img src="/images/kernel/2015-08-25-21.jpg" alt="" /></p>

<p>很容易一目了然。</p>

<pre><code>    static inline int skb_pagelen(const struct sk_buff *skb)
    {
        int i, len = 0;
        // 我们知道如果设备支持S/G IO的话，nr_frags会包含一些L4 payload，因此我们需要先遍历nr_frags.然后加入它的长度。
        for (i = (int)skb_shinfo(skb)-&gt;nr_frags - 1; i &gt;= 0; i--)
            len += skb_shinfo(skb)-&gt;frags[i].size;
        // 最后加上skb_headlen,而skb_headlen = skb-&gt;len - skb-&gt;data_len;因此这里就会返回这个数据包的len。
        return len + skb_headlen(skb);
    }
</code></pre>

<pre><code>        // 通过上一篇blog我们知道，如果4层将数据包分片了，那么就会把这些数据包放到skb的frag_list链表中，因此我们这里首先先判断frag_list链表是否为空，为空的话我们将会进行slow 切片。
        if (skb_shinfo(skb)-&gt;frag_list) {
            struct sk_buff *frag;
            // 取得第一个数据报的len.我们知道当sk_write_queue队列被flush后，除了第一个切好包的另外的包都会加入到frag_list中，而这里我们我们需要得到的第一个包(也就是本身这个sk_buff）的长度。
            int first_len = skb_pagelen(skb);
            int truesizes = 0;
            // 接下来的判断都是为了确定我们能进行fast切片。切片不能被共享，这是因为在fast path 中，我们需要加给每个切片不同的ip头(而并不会复制每个切片)。因此在fast path中是不可接受的。而在slow path中，就算有共享也无所谓，因为他会复制每一个切片，使用一个新的buff。

            // 判断第一个包长度是否符合一些限制(包括mtu，mf位等一些限制).如果第一个数据报的len没有包含mtu的大小这里之所以要把第一个切好片的数据包单独拿出来检测，是因为一些域是第一个包所独有的(比如IP_MF要为1）。这里由于这个mtu是不包括hlen的mtu，因此我们需要减去一个hlen。
            if (first_len - hlen &gt; mtu ||
                ((first_len - hlen) &amp; 7) ||
                (iph-&gt;frag_off &amp; htons(IP_MF|IP_OFFSET)) ||
                skb_cloned(skb))
                goto slow_path;
            // 遍历剩余的frag。
            for (frag = skb_shinfo(skb)-&gt;frag_list; frag; frag = frag-&gt;next) {
                /* Correct geometry. */
                // 判断每个帧的mtu，以及相关的东西，如果不符合条件则要进行slow path,基本和上面的第一个skb的判断类似。
                if (frag-&gt;len &gt; mtu ||
                    ((frag-&gt;len &amp; 7) &amp;&amp; frag-&gt;next) ||
                    skb_headroom(frag) &lt; hlen)
                    goto slow_path;
                // 判断是否共享。
                /* Partially cloned skb? */
                if (skb_shared(frag))
                    goto slow_path;

                BUG_ON(frag-&gt;sk);
                // 进行socket的一些操作。
                if (skb-&gt;sk) {
                    sock_hold(skb-&gt;sk);
                    frag-&gt;sk = skb-&gt;sk;
                    frag-&gt;destructor = sock_wfree;
                    truesizes += frag-&gt;truesize;
                }
            }

            // 通过上面的检测，都通过了，因此我们可以进行fast path切片了。

            // 先是设置一些将要处理的变量的值。
            err = 0;
            offset = 0;
            // 取得frag_list列表
            frag = skb_shinfo(skb)-&gt;frag_list;
            skb_shinfo(skb)-&gt;frag_list = NULL;

            // 得到数据(不包括头)的大小。
            skb-&gt;data_len = first_len - skb_headlen(skb);
            skb-&gt;truesize -= truesizes;
            // 得到
            skb-&gt;len = first_len;
            iph-&gt;tot_len = htons(first_len);
            // 设置mf位
            iph-&gt;frag_off = htons(IP_MF);
            // 执行校验
            ip_send_check(iph);

            for (;;) {
                // 开始进行发送。
                if (frag) {
                    // 设置校验位
                    frag-&gt;ip_summed = CHECKSUM_NONE;
                    // 设置相应的头部。
                    skb_reset_transport_header(frag);
                    __skb_push(frag, hlen);
                    skb_reset_network_header(frag);
                    // 复制ip头。
                    memcpy(skb_network_header(frag), iph, hlen);
                    // 修改每个切片的ip头的一些属性。
                    iph = ip_hdr(frag);
                    iph-&gt;tot_len = htons(frag-&gt;len);
                    // 将当前skb的一些属性付给将要传递的切片好的帧。
                    ip_copy_metadata(frag, skb);
                    if (offset == 0)
                    // 处理ip_option
                        ip_options_fragment(frag);
                    offset += skb-&gt;len - hlen;
                    // 设置位移。
                    iph-&gt;frag_off = htons(offset&gt;&gt;3);
                    if (frag-&gt;next != NULL)
                        iph-&gt;frag_off |= htons(IP_MF);
                    /* Ready, complete checksum */
                    ip_send_check(iph);
                }
                // 调用输出函数。
                err = output(skb);

                if (!err)
                    IP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGCREATES);
                if (err || !frag)
                    break;
                // 处理链表中下一个buf。
                skb = frag;
                frag = skb-&gt;next;
                skb-&gt;next = NULL;
            }

            if (err == 0) {
                IP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGOKS);
                return 0;
            }
            // 释放内存。
            while (frag) {
                skb = frag-&gt;next;
                kfree_skb(frag);
                frag = skb;
            }
            IP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGFAILS);
            return err;
        }
</code></pre>

<p>再接下来我们来看slow fragmentation：</p>

<pre><code>        // 切片开始的位移
        left = skb-&gt;len - hlen;      /* Space per frame */
        // 而ptr就是切片开始的指针。
        ptr = raw + hlen;       /* Where to start from */

        /* for bridged IP traffic encapsulated inside f.e. a vlan header,
         * we need to make room for the encapsulating header
         */
        // 处理桥接的相关操作。
        pad = nf_bridge_pad(skb);
        ll_rs = LL_RESERVED_SPACE_EXTRA(rt-&gt;u.dst.dev, pad);
        mtu -= pad;

        // 其实也就是取出取出ip offset域。
        offset = (ntohs(iph-&gt;frag_off) &amp; IP_OFFSET) &lt;&lt; 3;
        // not_last_frag，顾名思义，其实也就是表明这个帧是否是最后一个切片。
        not_last_frag = iph-&gt;frag_off &amp; htons(IP_MF);


        // 开始为循环处理，每一个切片创建一个skb buffer。
        while (left &gt; 0) {
            len = left;
            // 如果len大于mtu，我们设置当前的将要切片的数据大小为mtu。
            if (len &gt; mtu)
                len = mtu;
            // 长度也必须位对齐。
            if (len &lt; left)  {
                len &amp;= ~7;
            }
            // malloc一个新的buff。它的大小包括ip payload,ip head,以及L2 head.
            if ((skb2 = alloc_skb(len+hlen+ll_rs, GFP_ATOMIC)) == NULL) {
                NETDEBUG(KERN_INFO "IP: frag: no memory for new fragment!\n");
                err = -ENOMEM;
                goto fail;
            }
            // 调用ip_copy_metadata复制一些相同的值的域。
            ip_copy_metadata(skb2, skb);
            // 进行skb的相关操作。为了加上ip头。
            skb_reserve(skb2, ll_rs);
            skb_put(skb2, len + hlen);
            skb_reset_network_header(skb2);
            skb2-&gt;transport_header = skb2-&gt;network_header + hlen;
            // 将每一个分片的ip包都关联到源包的socket上。
            if (skb-&gt;sk)
                skb_set_owner_w(skb2, skb-&gt;sk);
            // 开始填充新的ip包的数据。

            // 先拷贝包头。
            skb_copy_from_linear_data(skb, skb_network_header(skb2), hlen);
            // 拷贝数据部分，这个函数实现的比较复杂。
            if (skb_copy_bits(skb, ptr, skb_transport_header(skb2), len))
                BUG();
            left -= len;
            // 填充相应的ip头。
            iph = ip_hdr(skb2);
            iph-&gt;frag_off = htons((offset &gt;&gt; 3));

            // 第一个包，因此进行ip_option处理。
            if (offset == 0)
                ip_options_fragment(skb);
            // 不是最后一个包，因此设置mf位。
            if (left &gt; 0 || not_last_frag)
                iph-&gt;frag_off |= htons(IP_MF);
            // 移动指针以及更改位移大小。
            ptr += len;
            offset += len;
            // update包头的大小。
            iph-&gt;tot_len = htons(len + hlen);
            // 重新计算校验。
            ip_send_check(iph);
            //最终输出。
            err = output(skb2);
            if (err)
                goto fail;

            IP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGCREATES);
        }
        kfree_skb(skb);
        IP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGOKS);
        return err;
</code></pre>

<p>接下来来看ip组包的实现。首先要知道每一个切片(属于同一个源包的)的ip包 id都是相同的。</p>

<p>首先来看相应的数据结构。在内核中，每一个ip包(切片好的)都是一个struct ipq链表。而不同的数据包(这里指不是属于同一个源包的数据包)都保</p>

<p>存在一个hash表中。也就是ip4_frags这个变量：</p>

<pre><code>    static struct inet_frags ip4_frags;

    #define INETFRAGS_HASHSZ        64

    struct inet_frags {
        struct hlist_head   hash[INETFRAGS_HASHSZ];
        rwlock_t        lock;
        // 随机值，它被用在计算hash值上面，下面会介绍到，过一段时间，内核就会更新这个值。
        u32         rnd;
        int         qsize;
        int         secret_interval;
        struct timer_list   secret_timer;
        // hash函数
        unsigned int        (*hashfn)(struct inet_frag_queue *);
        void            (*constructor)(struct inet_frag_queue *q,
                            void *arg);
        void            (*destructor)(struct inet_frag_queue *);
        void            (*skb_free)(struct sk_buff *);
        int         (*match)(struct inet_frag_queue *q,
                            void *arg);
        void            (*frag_expire)(unsigned long data);
    };

    struct ipq {
        struct inet_frag_queue q;
        u32     user;
        // 都是ip头相关的一些域。
        __be32      saddr;
        __be32      daddr;
        __be16      id;
        u8      protocol;
        int             iif;
        unsigned int    rid;
        struct inet_peer *peer;
    };

    struct inet_frag_queue {
        struct hlist_node   list;
        struct netns_frags  *net;
        // 基于LRU算法，主要用在GC上。
        struct list_head    lru_list;   /* lru list member */
        spinlock_t      lock;
        atomic_t        refcnt;
        // 属于同一个源的数据包的定时器，当定时器到期，切片还没到达，此时就会drop掉所有的数据切片。
        struct timer_list   timer;      /* when will this queue expire? */
        // 保存有所有的切片链表(从属于同一个ip包)
        struct sk_buff      *fragments; /* list of received fragments */
        ktime_t         stamp;
        int         len;        /* total length of orig datagram */
        // 表示从源ip包已经接收的字节数。
        int         meat;
        // 这个域主要可以设置为下面的3种值。
        __u8            last_in;    /* first/last segment arrived? */

    // 完成，第一个帧以及最后一个帧。
    #define INET_FRAG_COMPLETE  4
    #define INET_FRAG_FIRST_IN  2
    #define INET_FRAG_LAST_IN   1
    };
</code></pre>

<p>看下面的图就一目了然了：</p>

<p><img src="/images/kernel/2015-08-25-22.jpg" alt="" /></p>

<p>首先来看组包要解决的一些问题：</p>

<p>1 fragment必须存储在内存中，知道他们全部都被网络子系统处理。才会释放，因此内存会是个巨大的浪费。</p>

<p>2 这里虽然使用了hash表，可是假设恶意攻击者得到散列算法并且伪造数据包来尝试着降低一些hash表中的元素的比重，从而使执行变得缓慢。这里linux使用一个定时器通过制造的随机数来使hash值的生成不可预测。</p>

<p>这个定时器的初始化是通过ipfrag_init(它会初始化上面提到的ip4_frags全局变量)调用inet_frags_init进行的：</p>

<pre><code>    void inet_frags_init(struct inet_frags *f)
    {
        int i;

        for (i = 0; i &lt; INETFRAGS_HASHSZ; i++)
            INIT_HLIST_HEAD(&amp;f-&gt;hash[i]);

        rwlock_init(&amp;f-&gt;lock);

        f-&gt;rnd = (u32) ((num_physpages ^ (num_physpages&gt;&gt;7)) ^
                       (jiffies ^ (jiffies &gt;&gt; 6)));
        // 安装定时器，当定时器到期就会调用inet_frag_secret_rebuild方法。
        setup_timer(&amp;f-&gt;secret_timer, inet_frag_secret_rebuild,
                (unsigned long)f);
        f-&gt;secret_timer.expires = jiffies + f-&gt;secret_interval;
        add_timer(&amp;f-&gt;secret_timer);
    }

    static void inet_frag_secret_rebuild(unsigned long dummy)
    {
    ................................................

        write_lock(&amp;f-&gt;lock);
        // 得到随机值
        get_random_bytes(&amp;f-&gt;rnd, sizeof(u32));

        // 然后通过这个随机值重新计算整个hash表的hash值。
        for (i = 0; i &lt; INETFRAGS_HASHSZ; i++) {
            struct inet_frag_queue *q;
            struct hlist_node *p, *n;

            hlist_for_each_entry_safe(q, p, n, &amp;f-&gt;hash[i], list) {
                unsigned int hval = f-&gt;hashfn(q);

                if (hval != i) {
                    hlist_del(&amp;q-&gt;list);

                    /* Relink to new hash chain. */
                    hlist_add_head(&amp;q-&gt;list, &amp;f-&gt;hash[hval]);
                }
            }
        }
    ..............................................
    }
</code></pre>

<p>3 ip协议是不可靠的，因此切片有可能被丢失。内核处理这个，是使用了一个定时器(每个数据包(也就是这个切片从属于的那个数据包)).当定时器到期，而切片没有到达，就会丢弃这个包。</p>

<p>4 由于ip协议是无连接的，因此当高层决定重传数据包的时候，组包时有可能会出现多个重复分片的情况。这是因为ip包是由4个域来判断的，源和目的地址，包id以及4层的协议类型。而最主要的是包id。可是包id只有16位，因此一个gigabit网卡几乎在半秒时间就能用完这个id一次。而第二次重传的数据包有可能走的和第一个第一次时不同的路径，因此内核必须每个切片都要检测和前面接受的切片的重叠情况的发生。</p>

<p>先来看ip_defrag用到的几个函数：</p>

<p>inet_frag_create: 创建一个新的ipq实例</p>

<p>ip_evitor: remove掉所有的未完成的数据包。它每次都会update一个LRU链表。每次都会把一个新的ipq数据结构加到ipq_lru_list的结尾。</p>

<p>ip_find: 发现切片所从属的数据包的切片链表。</p>

<p>ip_frag_queue: 排队一个给定的切片刀一个切片列表。这个经常和上一个方法一起使用。</p>

<p>ip_frag_reasm: 当所有的切片都到达后，build一个ip数据包。</p>

<p>ip_frag_destroy: remove掉传进来的ipq数据结构。包括和他有联系的所有的ip切片。</p>

<p>ipq_put: 将引用计数减一，如果为0，则直接调用ip_frag_destroy.</p>

<pre><code>    static inline void inet_frag_put(struct inet_frag_queue *q, struct inet_frags *f)
    {
        if (atomic_dec_and_test(&amp;q-&gt;refcnt))
            inet_frag_destroy(q, f, NULL);
    }
</code></pre>

<p>ipq_kill: 主要用在gc上，标记一个ipq数据结构可以被remove，由于一些帧没有按时到达。</p>

<p>接下来来看ip_defrag的实现。</p>

<pre><code>    int ip_defrag(struct sk_buff *skb, u32 user)
    {
        struct ipq *qp;
        struct net *net;

        net = skb-&gt;dev ? dev_net(skb-&gt;dev) : dev_net(skb-&gt;dst-&gt;dev);
        IP_INC_STATS_BH(net, IPSTATS_MIB_REASMREQDS);

        // 如果内存不够，则依据lru算法进行清理。
        if (atomic_read(&amp;net-&gt;ipv4.frags.mem) &gt; net-&gt;ipv4.frags.high_thresh)
            ip_evictor(net);

        // 查找相应的iqp，如果不存在则会新创建一个(这些都在ip_find里面实现)
        if ((qp = ip_find(net, ip_hdr(skb), user)) != NULL) {
            int ret;

            spin_lock(&amp;qp-&gt;q.lock);
            // 排队进队列。
            ret = ip_frag_queue(qp, skb);

            spin_unlock(&amp;qp-&gt;q.lock);
            ipq_put(qp);
            return ret;
        }

        IP_INC_STATS_BH(net, IPSTATS_MIB_REASMFAILS);
        kfree_skb(skb);
        return -ENOMEM;
    }
</code></pre>

<p>我们可以看到这里最重要的一个函数其实是ip_frag_queue,它主要任务是：</p>

<p>1 发现输入帧在源包的位置。<br/>
2 基于blog刚开始所描述的，判断是否是最后一个切片。<br/>
3 插入切片到切片列表(从属于相同的ip包)<br/>
4 update 垃圾回收所用到的ipq的一些相关域。<br/>
5 校验l4层的校验值(在硬件计算).</p>

<pre><code>    // 其中qp是源ip包的所有切片链表，而skb是将要加进来切片。
    static int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)
    {
        .............................
        //  INET_FRAG_COMPLETE表示所有的切片包都已经抵达，这个时侯就不需要再组包了，因此这里就是校验函数有没有被错误的调用。
        if (qp-&gt;q.last_in &amp; INET_FRAG_COMPLETE)
            goto err;
        .................................................
        // 将offset 8字节对齐、
        offset = ntohs(ip_hdr(skb)-&gt;frag_off);
        flags = offset &amp; ~IP_OFFSET;
        offset &amp;= IP_OFFSET;
        offset &lt;&lt;= 3;     /* offset is in 8-byte chunks */
        ihl = ip_hdrlen(skb);

        // 计算这个新的切片包的结束位置。
        end = offset + skb-&gt;len - ihl;
        err = -EINVAL;

        // MF没有设置，表明这个帧是最后一个帧。进入相关处理。
        if ((flags &amp; IP_MF) == 0) {
            /* If we already have some bits beyond end
             * or have different end, the segment is corrrupted.
             */
        // 设置相应的len位置，以及last_in域。
            if (end &lt; qp-&gt;q.len ||
                ((qp-&gt;q.last_in &amp; INET_FRAG_LAST_IN) &amp;&amp; end != qp-&gt;q.len))
                goto err;
            qp-&gt;q.last_in |= INET_FRAG_LAST_IN;
            qp-&gt;q.len = end;
        } else {
            // 除了最后一个切片，每个切片都必须是8字节的倍数。
            if (end&amp;7) {
                // 不是8字节的倍数，kernel截断这个切片。此时就需要l4层的校验重新计算，因此设置ip_summed为 CHECKSUM_NONE
                end &amp;= ~7;
                if (skb-&gt;ip_summed != CHECKSUM_UNNECESSARY)
                    skb-&gt;ip_summed = CHECKSUM_NONE;
            }
            if (end &gt; qp-&gt;q.len) {
                // 数据包太大，并且是最后一个包，则表明这个数据包出错，因此drop它。
                /* Some bits beyond end -&gt; corruption. */
                if (qp-&gt;q.last_in &amp; INET_FRAG_LAST_IN)
                    goto err;
                qp-&gt;q.len = end;
            }
        }
        // ip头不能被切片，因此end肯定会大于offset。
        if (end == offset)
            goto err;

        err = -ENOMEM;
        // remove掉ip头。
        if (pskb_pull(skb, ihl) == NULL)
            goto err;
        // trim掉一些padding，然后重新计算checksum。
        err = pskb_trim_rcsum(skb, end - offset);
        if (err)
            goto err;

        // 接下来遍历并将切片(为了找出当前将要插入的切片的位置)，是以offset为基准。这里要合租要FRAG_CB宏是用来提取sk_buff-&gt;cb域。
        prev = NULL;
        for (next = qp-&gt;q.fragments; next != NULL; next = next-&gt;next) {
            if (FRAG_CB(next)-&gt;offset &gt;= offset)
                break;  /* bingo! */
            prev = next;
        }
        // 当prev!=NULL时，说明这个切片要插入到列表当中。
        if (prev) {
            // 计算有没有重叠。
            int i = (FRAG_CB(prev)-&gt;offset + prev-&gt;len) - offset;
            // 大于0.证明有重叠，因此进行相关处理
            if (i &gt; 0) {
                // 将重叠部分用新的切片覆盖。
                offset += i;
                err = -EINVAL;
                if (end &lt;= offset)
                    goto err;
                err = -ENOMEM;
                //移动i个位置。
                if (!pskb_pull(skb, i))
                    goto err;
                // 需要重新计算L4的校验。
                if (skb-&gt;ip_summed != CHECKSUM_UNNECESSARY)
                    skb-&gt;ip_summed = CHECKSUM_NONE;
            }
        }

        err = -ENOMEM;

        while (next &amp;&amp; FRAG_CB(next)-&gt;offset &lt; end) {
            // 和上面的判断很类似，也是先计算重叠数。这里要注意重叠分为两种情况：1；一个或多个切片被新的切片完全覆盖。2；被部分覆盖，因此这里我们需要分两种情况进行处理。
            int i = end - FRAG_CB(next)-&gt;offset; /* overlap is 'i' bytes */

            if (i &lt; next-&gt;len) {
                // 被部分覆盖的情况。将新的切片offset移动i字节，然后remove掉老的切片中的i个字节。
                /* Eat head of the next overlapped fragment
                 * and leave the loop. The next ones cannot overlap.
                 */
                if (!pskb_pull(next, i))
                    goto err;
                FRAG_CB(next)-&gt;offset += i;
                // 将接收到的源数据报的大小减去i，也就是remove掉不完全覆盖的那一部分。
                qp-&gt;q.meat -= i;
                // 重新计算l4层的校验。
                if (next-&gt;ip_summed != CHECKSUM_UNNECESSARY)
                    next-&gt;ip_summed = CHECKSUM_NONE;
                break;
            } else {
                // 老的切片完全被新的切片覆盖，此时只需要remove掉老的切片就可以了。
                struct sk_buff *free_it = next;
                next = next-&gt;next;

                if (prev)
                    prev-&gt;next = next;
                else
                    qp-&gt;q.fragments = next;
                // 将qp的接受字节数更新。
                qp-&gt;q.meat -= free_it-&gt;len;
                frag_kfree_skb(qp-&gt;q.net, free_it, NULL);
            }
        }

        FRAG_CB(skb)-&gt;offset = offset;

    ....................................................
        atomic_add(skb-&gt;truesize, &amp;qp-&gt;q.net-&gt;mem);
        // offset为0说明是第一个切片，因此设置相应的位。
        if (offset == 0)
            qp-&gt;q.last_in |= INET_FRAG_FIRST_IN;

        if (qp-&gt;q.last_in == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &amp;&amp;
            qp-&gt;q.meat == qp-&gt;q.len)
            // 所有条件的满足了，就开始buildip包。
            return ip_frag_reasm(qp, prev, dev);
        write_lock(&amp;ip4_frags.lock);
        // 从将此切片加入到lry链表中。
        list_move_tail(&amp;qp-&gt;q.lru_list, &amp;qp-&gt;q.net-&gt;lru_list);
        write_unlock(&amp;ip4_frags.lock);
        return -EINPROGRESS;

    err:
        kfree_skb(skb);
        return err;
    }
</code></pre>

<p>如果网络设备提供L4层的硬件校验的话，输入ip帧还会进行L4的校验计算。当帧通过ip_frag_reasm组合好，它会进行校验的重新计算。我们这里通过设置skb->ip_summed到CHECKSUM_NONE，来表示需要娇艳的标志。</p>

<p>最后来看下GC。</p>

<p>内核为ip切片数据包实现了两种类型的垃圾回收。</p>

<p>1 系统内存使用限制。</p>

<p>2 组包的定时器</p>

<p>这里有一个全局的ip_frag_mem变量，来表示当前被切片所占用的内存数。每次一个新的切片被加入，这个值都会更新。而所能使用的最大内存可以在运行时改变，是通过/proc的sysctl_ipfrag_high_thresh来改变的，因此我们能看到当ip_defrag时，一开始会先判断内存的限制：</p>

<pre><code>    if (atomic_read(&amp;net-&gt;ipv4.frags.mem) &gt; net-&gt;ipv4.frags.high_thresh)
            ip_evictor(net);
</code></pre>

<p>当一个切片数据包到达后，内核会启动一个组包定时器，他是为了避免一个数据包占据ipq_hash太长时间，因此当定时器到期后，它就会清理掉在hash表中的相应的qp结构(也就是所有的未完成切片包).这个处理函数就是ip_expire,它的初始化是在ipfrag_init进行的。:</p>

<pre><code>    static void ip_expire(unsigned long arg)
    {
        struct ipq *qp;
        struct net *net;
        // 取出相应的qp，以及net域。
        qp = container_of((struct inet_frag_queue *) arg, struct ipq, q);
        net = container_of(qp-&gt;q.net, struct net, ipv4.frags);

        spin_lock(&amp;qp-&gt;q.lock);
        // 如果数据包已经传输完毕，则不进行任何处理，直接退出。
        if (qp-&gt;q.last_in &amp; INET_FRAG_COMPLETE)
            goto out;
        // 调用ipq_kill，这个函数主要是减少qp的引用计数，并从相关链表(比如LRU_LIST)中移除它。
        ipq_kill(qp);

        IP_INC_STATS_BH(net, IPSTATS_MIB_REASMTIMEOUT);
        IP_INC_STATS_BH(net, IPSTATS_MIB_REASMFAILS);

        // 如果是第一个切片，则发送一个ICMP给源主机。
        if ((qp-&gt;q.last_in &amp; INET_FRAG_FIRST_IN) &amp;&amp; qp-&gt;q.fragments != NULL) {
            struct sk_buff *head = qp-&gt;q.fragments;

            /* Send an ICMP "Fragment Reassembly Timeout" message. */
            if ((head-&gt;dev = dev_get_by_index(net, qp-&gt;iif)) != NULL) {
                icmp_send(head, ICMP_TIME_EXCEEDED, ICMP_EXC_FRAGTIME, 0);
                dev_put(head-&gt;dev);
            }
        }
    out:
        spin_unlock(&amp;qp-&gt;q.lock);
        ipq_put(qp);
    }
</code></pre>
]]></content>
  </entry>
  
</feed>
