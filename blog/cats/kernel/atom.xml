<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2016-01-12T16:14:03+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[linux 实时时钟（RTC）驱动]]></title>
    <link href="http://abcdxyzk.github.io/blog/2016/01/12/kernel-base-rtc/"/>
    <updated>2016-01-12T15:53:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2016/01/12/kernel-base-rtc</id>
    <content type="html"><![CDATA[<p><a href="/download/kernel/rtc.txt">Documentation/rtc.txt</a></p>

<hr />

<p><a href="http://blog.csdn.net/yaozhenguo2006/article/details/6820218">http://blog.csdn.net/yaozhenguo2006/article/details/6820218</a></p>

<p>这个是linux内核文档关于rtc实时时钟部分的说明，此文档主要描述了rtc实时时钟的作用和编程接口，分别介绍了老的rtc接口和新的rtc类架构。并给出了一个测试rtc驱动的程序。</p>

<h3>linux 实时时钟（RTC）驱动</h3>

<p>翻译：窗外云天<a href="&#109;&#97;&#x69;&#108;&#116;&#111;&#x3a;&#121;&#97;&#x6f;&#x7a;&#104;&#x65;&#110;&#x67;&#x75;&#111;&#50;&#48;&#x30;&#54;&#64;&#49;&#50;&#54;&#46;&#x63;&#111;&#109;">&#x79;&#97;&#111;&#x7a;&#x68;&#101;&#x6e;&#x67;&#x75;&#111;&#x32;&#48;&#x30;&#x36;&#x40;&#x31;&#50;&#54;&#46;&#99;&#111;&#x6d;</a>  最后矫正时间：2011.9.25</p>

<p>当linux开发者提到“实时时钟”的时候，他们通常所指的就是墙钟时间，这个时间是电池供电的，所以在系统掉电的情况下还能正常工作。除非在MS-Windows启动的时候设置，否则这个时钟不会同步于本地时区和夏令时间。事实上，他被设置成格林威治时间。</p>

<p>最新的非pc体系的硬件趋向于记秒数，比如time(2)系统调用的输出，但是实时时钟用公历和24小时表示日期与时间，比如gmtime(3)的输出。</p>

<p>linux提供两类的rtc兼容性很高的用户空间系统调用接口，如下所示：<br/>
（1） /dev/rtc &hellip; 这个RTC适合pc体系的系统，而并不适合非x86体系的系统<br/>
（2） /dev/rtc0,/dev/rtc1 &hellip; 他们依赖一种架构，这种架构在所有的系统上被RTC芯片广泛的支持。</p>

<p>程序员必须知道，PC/AT的功能不总是有效，其他的系统可能会有另外的实现。这种情况下，如果在相同的系统结构上使用同样的RTC API，那么硬件会有不同的反应。例如，不是每一个RTC都提供IRQ，所以这些不能处理报警中断；标准的PC系统RTC只能处理未来24小时以内的闹钟，而其他系统的RTC可能处理未来一个世纪的任何时间。</p>

<h4>老的PC/AT驱动：/dev/rtc</h4>

<p>所有基于PC的系统（甚至Alpha体系的机器）都有一个集成的实时时钟。通常他们集成在计算机的芯片组内，但是也有一些系统是在主板上焊接着摩托罗拉MC146818（或者类似的芯片），他们给系统提供时间和日期，这个时间和日期在系统掉电后仍然会保存。</p>

<p>ACPT(高级配置与电源管理接口)对MC146818的功能进行了标准化，并且在一些方面进行了功能扩展（提供了更长的定时周期，睡眠唤醒功能）。然而这些扩展的功能不适合老的驱动程序。</p>

<p>这个RTC还可以产生频率从 2HZ 到 8192HZ 的信号，以2的乘方增长。这些信号通过中断信号线8报告给系统。这个RTC还可以用作定时限制为24小时的闹钟，当定时时间到时产生8号中断。这个闹钟可以设置成任意一个可编程值的子集，这意味着可以设置成任意小时的任意分钟任意秒，例如，可以将这个时钟设置成在每个clk产生中断，从而产生1hz的信号。</p>

<p>这些中断通过/dev/rtc报告给系统（主设备号10,次设备号135，只读字符设备），中断传回一个无符号整数类型的数据。最低的位包含中断的类型（更新，闹钟，或者期），其他的字节代表了最后一次读到现在中断发生的次数。状态信息由虚拟文件/proc/driver/rtc产生，前提条件是使能了/proc文件系统。驱动应该提供锁机制，保证在同一时刻只有一个进程访问/dev/rtc。</p>

<p>用户进程通过系统调用read(2)或者select(2)读取/dev/rtc来获取这些中断。当调用这两个系统调用的时候，进程会阻塞或者退出直到下一个中断到来。这个功能用在需要不频繁的获取数据而又不希望通过轮询当前时间而占用CPU时间的情况下。</p>

<p>在高频率中断或者高系统负载下，用户进程应该检查从上次读取到现在发生中断的次数以判断是否有未处理的中断。例如，一个典型的 486-33 对/dev/rtc以大于1024hz的频率进行循环读，偶尔会产生中断积累（从上次读取到现在发生大于一次的中断）。鉴于此你应该检查读取数据的高字节，特别是在频率高于普通定时器中断&ndash;100hz的情况下。</p>

<p>中断频率是可编程的或可以让他超过64hz，但是只有root权限的用户可以这样做。这样做可能有点保守，但是我们不希望有恶意的用户在一个较慢的386sx-16机器上产生很多中断，这样会严重影响系统的性能。我们可以通过向/proc/sys/dev/rtc/max-user-freq写入值来修改这个64hz的限制。但是注意你一定要这样做，减少中断处理程序的代码才会亡羊补牢，使对系统性能的影响降到最小。</p>

<p>如果内核时间是和外部时钟源同步的，那么内核将每隔11分钟就会将时间写回CMOS时钟。在这个过程中，内核会关闭rtc周期中断，如果你的程序在做一些关键的工作一定要注意到。如果你的内核不和外部时钟源同步，那么内核会一直处理rtc中断，处理方式根据你具体的应用。</p>

<p>闹钟和中断频率可以通过系统调用ioctl(2)来设置，ioctl的命令定义在./include/linux/rtc.h。与其长篇大论的介绍怎么样使用这个系统调用，还不如写一个实例程序来的方便，这个程序用来演示驱动的功能，对很多人来说用驱动程序提供的功能来进行应用编程他们会更感兴趣。在这个文档的最后有这段程序。</p>

<h4>新接口 “RTC类” 驱动：/dev/rtcn</h4>

<p>因为linux支持许多非ACPI非PC平台，其中一些平台有不只一个RTC，所以需要更多可移植性的设计，而不是仅仅在每个系统都实现类似MC146818的接口。在这种情况下，新的“RTC类”构架产生了。他提供不同的用户空间接口：
（1） /dev/rtcn 和老的接口一样
（2）/dev/class/rtc/rtcn   sysfs 属性，一些属性是只读的
（3） /dev/driver/rtc 第一个rtc会使用procfs接口。更多的信息会显示在这里而不是sysfs。</p>

<p>RTC类构架支持很多类型的RTC，从集成在嵌入式SOC处理器内的RTC到通过I2C，SPI和其他总线连接到CPU的芯片。这个架构甚至还支持PC系统的RTC，包括使用ACPI，PC的一些新特性。</p>

<p>新架构也打破了“每个系统只有一个RTC”的限制。例如，一个低功耗电池供电的RTC是一个分离的I2C接口的芯片，但是系统可能还集成了一个多功能的RTC。系统可能从分离的RTC读取系统时钟，但是对于其他任务用集成的RTC，因为这个RTC提供更多的功能。</p>

<h4>SYSFS 接口</h4>

<p>在/sys/class/rtc/rtcn下面的sysfs接口提供了操作rtc属性的方法，而不用通过Ioclt系统调用。所有的日期时间都是墙钟时间，而不是系统时间。
<code>
    date:           RTC提供的日期
    hctosys:        如果在内核配置选项中配置了CONFIG_RTC_HCTOSYS，RTC会在系统启动的时候提供系统时间，这种情况下这个位就是1,否则为0
    max_user_freq:  非特权用户可以从RTC得到的最大中断频率
    name:           RTC的名字，与sysfs目录相关
    since_epoch:    从纪元开始所经历的秒数
    time:           RTC提供的时间
    wakealarm:      唤醒时间的时间事件。 这是一种单次的唤醒事件，所以如果还需要唤醒，在唤醒发生后必须复位。这个域的数据结构或者是从纪元开始经历的妙数，或者是相对的秒数
</code></p>

<h4>IOCTL 接口</h4>

<p>/dev/rtc支持的Ioctl系统调用，RTC类构架也支持。然而，因为芯片和系统没有一个统一的标准，一些PC/AT功能可能没有提供。以相同方式工作的一些新特性，&ndash;包括ACPI提供的，&ndash;在RTC类构架中表现出的，在老的驱动上不会工作。</p>

<p>（1） RTC_RD_TIME,RTC_SET_TIME .. 每一个RTC都至少支持读时间这个命令，时间格式为公历和24小时制墙钟时间。最有用的特性是，这个时间可以更新。<br/>
（2） RTC_ATE_ON,RTC_ATE_OFF,RTC_ALM_SET,RTC_ALM_READ &hellip; 当RTC连接了一条IRQ线，他还能处理在未来24小时的报警中断。<br/>
（3） RTC_WKALM_SET，RTC_WKALM_RD 。。。 RTCs 使用一个功能更强大的api,他可以处理超过24小时的报警时间。这个API支持设置更长的报警时间，支持单次请求的IRQ中断。<br/>
（4） RTC_UIE_ON,RTC_UIE_OFF &hellip; 如果RTC提供IRQ，他可能也提供每秒更新的IRQ中断。如果需要，RTC结构可以模仿这个机制。</p>

<p>（5） RTC_PIE_ON,RTC_PIE_OFF,RTC_IRQP_SET,RTC_IRQP_READ &hellip; 如果一个IRQ是周期中断，那么这个IRQ还有可设置频率的特性（频率通常是2的n次方）</p>

<p>很多情况下，RTC报警时钟通常是一个系统唤醒事件，用于将Linux从低功耗睡眠模式唤醒到正常的工作模式。例如，系统会处于低功耗的模式下，直到时间到了去执行一些任务。注意这些ioctl的一些功能不必在你的驱动程序中实现。如果一个ioctl调用，你的驱动返回ENOIOCTLCMD，那么这个Ioctl就由通用RTC设备接口处理。下面是一些通用的例子：<br/>
（6） RTC_RD_TIME, RTC_SET_TIME: read_time/set_time 函数会被调用。<br/>
（7） RTC_ALM_SET, RTC_ALM_READ, RTC_WKALM_SET, RTC_WKALM_RD: set_alarm/read_alarm 函数将会被调用.<br/>
（8） RTC_IRQP_SET, RTC_IRQP_READ: irq_set_freq 函数将会调用，用来设置频率，RTC类构架会处理读请求，而频率保存在RTC设备结构中的irq_freq域。你的驱动需要在模块初始化的时候初始化irq_freq，你必须在irq_set_freq函数里检查设置的频率是否在硬件允许的范围。如果不是那么驱动应该返回-EINVAL。如果你不需要改变这个频率，那么不要定义irq_set_freq这个函数。<br/>
（7） RTC_PIE_ON, RTC_PIE_OFF: irq_set_state 函数会被调用。</p>

<p>  如果所有的ioctl都失败了，用下面的rtc-test.c检查一下你的驱动吧！</p>

<pre><code>    /*
     *      Real Time Clock Driver Test/Example Program
     *
     *      Compile with:
     *           gcc -s -Wall -Wstrict-prototypes rtctest.c -o rtctest
     *
     *      Copyright (C) 1996, Paul Gortmaker.
     *
     *      Released under the GNU General Public License, version 2,
     *      included herein by reference.
     *
     */

    #include &lt;stdio.h&gt;
    #include &lt;linux/rtc.h&gt;
    #include &lt;sys/ioctl.h&gt;
    #include &lt;sys/time.h&gt;
    #include &lt;sys/types.h&gt;
    #include &lt;fcntl.h&gt;
    #include &lt;unistd.h&gt;
    #include &lt;stdlib.h&gt;
    #include &lt;errno.h&gt;


    /*
     * This expects the new RTC class driver framework, working with
     * clocks that will often not be clones of what the PC-AT had.
     * Use the command line to specify another RTC if you need one.
     */
    static const char default_rtc[] = "/dev/rtc0";


    int main(int argc, char **argv)
    {
        int i, fd, retval, irqcount = 0;
        unsigned long tmp, data;
        struct rtc_time rtc_tm;
        const char *rtc = default_rtc;

        switch (argc) {
        case 2:
            rtc = argv[1];
            /* FALLTHROUGH */
        case 1:
            break;
        default:
            fprintf(stderr, "usage:  rtctest [rtcdev]\n");
            return 1;
        }

        fd = open(rtc, O_RDONLY);

        if (fd ==  -1) {
            perror(rtc);
            exit(errno);
        }

        fprintf(stderr, "\n\t\t\tRTC Driver Test Example.\n\n");

        /* Turn on update interrupts (one per second) */
        retval = ioctl(fd, RTC_UIE_ON, 0);
        if (retval == -1) {
            if (errno == ENOTTY) {
                fprintf(stderr,
                    "\n...Update IRQs not supported.\n");
                goto test_READ;
            }
            perror("RTC_UIE_ON ioctl");
            exit(errno);
        }

        fprintf(stderr, "Counting 5 update (1/sec) interrupts from reading %s:",
                rtc);
        fflush(stderr);
        for (i=1; i&lt;6; i++) {
            /* This read will block */
            retval = read(fd, &amp;data, sizeof(unsigned long));
            if (retval == -1) {
                perror("read");
                exit(errno);
            }
            fprintf(stderr, " %d",i);
            fflush(stderr);
            irqcount++;
        }

        fprintf(stderr, "\nAgain, from using select(2) on /dev/rtc:");
        fflush(stderr);
        for (i=1; i&lt;6; i++) {
            struct timeval tv = {5, 0};     /* 5 second timeout on select */
            fd_set readfds;

            FD_ZERO(&amp;readfds);
            FD_SET(fd, &amp;readfds);
            /* The select will wait until an RTC interrupt happens. */
            retval = select(fd+1, &amp;readfds, NULL, NULL, &amp;tv);
            if (retval == -1) {
                    perror("select");
                    exit(errno);
            }
            /* This read won't block unlike the select-less case above. */
            retval = read(fd, &amp;data, sizeof(unsigned long));
            if (retval == -1) {
                    perror("read");
                    exit(errno);
            }
            fprintf(stderr, " %d",i);
            fflush(stderr);
            irqcount++;
        }

        /* Turn off update interrupts */
        retval = ioctl(fd, RTC_UIE_OFF, 0);
        if (retval == -1) {
            perror("RTC_UIE_OFF ioctl");
            exit(errno);
        }

    test_READ:
        /* Read the RTC time/date */
        retval = ioctl(fd, RTC_RD_TIME, &amp;rtc_tm);
        if (retval == -1) {
            perror("RTC_RD_TIME ioctl");
            exit(errno);
        }

        fprintf(stderr, "\n\nCurrent RTC date/time is %d-%d-%d, %02d:%02d:%02d.\n",
            rtc_tm.tm_mday, rtc_tm.tm_mon + 1, rtc_tm.tm_year + 1900,
            rtc_tm.tm_hour, rtc_tm.tm_min, rtc_tm.tm_sec);

        /* Set the alarm to 5 sec in the future, and check for rollover */
        rtc_tm.tm_sec += 5;
        if (rtc_tm.tm_sec &gt;= 60) {
            rtc_tm.tm_sec %= 60;
            rtc_tm.tm_min++;
        }
        if (rtc_tm.tm_min == 60) {
            rtc_tm.tm_min = 0;
            rtc_tm.tm_hour++;
        }
        if (rtc_tm.tm_hour == 24)
            rtc_tm.tm_hour = 0;

        retval = ioctl(fd, RTC_ALM_SET, &amp;rtc_tm);
        if (retval == -1) {
            if (errno == ENOTTY) {
                fprintf(stderr,
                    "\n...Alarm IRQs not supported.\n");
                goto test_PIE;
            }
            perror("RTC_ALM_SET ioctl");
            exit(errno);
        }

        /* Read the current alarm settings */
        retval = ioctl(fd, RTC_ALM_READ, &amp;rtc_tm);
        if (retval == -1) {
            perror("RTC_ALM_READ ioctl");
            exit(errno);
        }

        fprintf(stderr, "Alarm time now set to %02d:%02d:%02d.\n",
            rtc_tm.tm_hour, rtc_tm.tm_min, rtc_tm.tm_sec);

        /* Enable alarm interrupts */
        retval = ioctl(fd, RTC_AIE_ON, 0);
        if (retval == -1) {
            perror("RTC_AIE_ON ioctl");
            exit(errno);
        }

        fprintf(stderr, "Waiting 5 seconds for alarm...");
        fflush(stderr);
        /* This blocks until the alarm ring causes an interrupt */
        retval = read(fd, &amp;data, sizeof(unsigned long));
        if (retval == -1) {
            perror("read");
            exit(errno);
        }
        irqcount++;
        fprintf(stderr, " okay. Alarm rang.\n");

        /* Disable alarm interrupts */
        retval = ioctl(fd, RTC_AIE_OFF, 0);
        if (retval == -1) {
            perror("RTC_AIE_OFF ioctl");
            exit(errno);
        }

    test_PIE:
        /* Read periodic IRQ rate */
        retval = ioctl(fd, RTC_IRQP_READ, &amp;tmp);
        if (retval == -1) {
            /* not all RTCs support periodic IRQs */
            if (errno == ENOTTY) {
                fprintf(stderr, "\nNo periodic IRQ support\n");
                goto done;
            }
            perror("RTC_IRQP_READ ioctl");
            exit(errno);
        }
        fprintf(stderr, "\nPeriodic IRQ rate is %ldHz.\n", tmp);

        fprintf(stderr, "Counting 20 interrupts at:");
        fflush(stderr);

        /* The frequencies 128Hz, 256Hz, ... 8192Hz are only allowed for root. */
        for (tmp=2; tmp&lt;=64; tmp*=2) {

            retval = ioctl(fd, RTC_IRQP_SET, tmp);
            if (retval == -1) {
                /* not all RTCs can change their periodic IRQ rate */
                if (errno == ENOTTY) {
                    fprintf(stderr,
                        "\n...Periodic IRQ rate is fixed\n");
                    goto done;
                }
                perror("RTC_IRQP_SET ioctl");
                exit(errno);
            }

            fprintf(stderr, "\n%ldHz:\t", tmp);
            fflush(stderr);

            /* Enable periodic interrupts */
            retval = ioctl(fd, RTC_PIE_ON, 0);
            if (retval == -1) {
                perror("RTC_PIE_ON ioctl");
                exit(errno);
            }

            for (i=1; i&lt;21; i++) {
                /* This blocks */
                retval = read(fd, &amp;data, sizeof(unsigned long));
                if (retval == -1) {
                    perror("read");
                    exit(errno);
                }
                fprintf(stderr, " %d",i);
                fflush(stderr);
                irqcount++;
            }

            /* Disable periodic interrupts */
            retval = ioctl(fd, RTC_PIE_OFF, 0);
            if (retval == -1) {
                perror("RTC_PIE_OFF ioctl");
                exit(errno);
            }
        }

    done:
        fprintf(stderr, "\n\n\t\t\t *** Test complete ***\n");

        close(fd);

        return 0;
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Web压力测试工具]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/12/27/kernel-net-test-tool/"/>
    <updated>2015-12-27T02:51:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/12/27/kernel-net-test-tool</id>
    <content type="html"><![CDATA[<p><a href="http://297020555.blog.51cto.com/1396304/592386">http://297020555.blog.51cto.com/1396304/592386</a></p>

<h4>一、http_load</h4>

<p>http_load以并行复用的方式运行，用以测试web服务器的吞吐量与负载。但是它不同于大多数压力测试工具，它可以以一个单一的进程运行，一般不会把客户机搞死。还可以测试HTTPS类的网站请求。</p>

<p>下载地址：<a href="http://www.acme.com/software/http_load/">http://www.acme.com/software/http_load/</a></p>

<pre><code>    ./http_load -verbose -proxy 192.168.99.6:80 -parallel 24 -seconds 1000 url.txt
</code></pre>

<h4>二、webbench</h4>

<p>webbench是Linux下的一个网站压力测试工具，最多可以模拟3万个并发连接去测试网站的负载能力。
<code>
    用法：webbench -c 并发数 -t 运行测试时间 URL
    如：webbench -c 5000 -t 120 http://www.163.com
</code></p>

<h4>三、ab</h4>

<p>ab是apache自带的一款功能强大的测试工具。安装了apache一般就自带了，用法可以查看它的说明</p>

<p>参数众多，一般我们用到的是-n 和-c</p>

<p>例如：
<code>
    ./ab -c 1000 -n 100 http://www.vpser.net/index.php
</code>
这个表示同时处理1000个请求并运行100次index.php文件.</p>

<h4>四、Siege</h4>

<p>一款开源的压力测试工具，可以根据配置对一个WEB站点进行多用户的并发访问，记录每个用户所有请求过程的相应时间，并在一定数量的并发访问下重复进行。
官方：<a href="http://www.joedog.org/">http://www.joedog.org/</a></p>

<p>使用
<code>
    siege -c 200 -r 10 -f example.url
</code></p>

<p>-c是并发量，-r是重复次数。 url文件就是一个文本，每行都是一个url，它会从里面随机访问的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP拥塞控制窗口有效性验证机制]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/12/08/kernel-net-cwnd-test/"/>
    <updated>2015-12-08T15:49:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/12/08/kernel-net-cwnd-test</id>
    <content type="html"><![CDATA[<p>blog.csdn.net/zhangskd/article/details/7609465</p>

<h4>概述</h4>

<p>问题1：当发送方长时间受到应用程序的限制，不能发送数据时，会使拥塞窗口无效。TCP是根据拥塞窗口来动态地估计网络带宽的。发送方受到应用程序的限制后，没有数据可以发送。那么此时的拥塞窗口就不能准确的反应网络状况，因为这个拥塞窗口是很早之前的。</p>

<p>问题2：当发送方受到应用程序限制，不能利用完拥塞窗口，会使拥塞窗口的增长无效。TCP不断调整cwnd来测试网络带宽。如果不能完全使用掉cwnd，就不知道网络能否承受得了cwnd的数据量，这种情况下的cwnd增长是无效的。</p>

<h4>原理</h4>

<p>TCP sender受到的两种限制</p>

<p>(1) application-limited ：when the sender sends less than is allowed by the congestion or receiver window.</p>

<p>(2) network-limited：when the sender is limited by the TCP window. More precisely, we define a network-limited period as any period when the sender is sending a full window of data.</p>

<h5>问题1描述</h5>

<p>TCP&rsquo;s congestion window controls the number of packets a TCP flow may have in the
network at any time. However, long periods when the sender is idle or application-limited
can lead to the invalidation of the congestion window, in that the congestion window no longer
reflects current information about the state of the network.</p>

<p>The congestion window is set using an Additive-Increase, Multiplicative-Decrease(AIMD) mechanism
that probes for available bandwidth, dynamically adapting to changing network conditions. This AIMD
works well when the sender continually has data to send, as is typically the case for TCP used for
bulk-data transfer. In contrast, for TCP used with telnet applications, the data sender often has little
or no data to send, and the sending rate is often determined by the rate at which data is generated
by the user.</p>

<h5>问题2描述</h5>

<p>An invalid congestion window also results when the congestion window is increased (i.e.,
in TCP&rsquo;s slow-start or congestion avoidance phases) during application-limited periods, when the
previous value of the congestion window might never have been fully utilized. As far as we know, all
current TCP implementations increase the congestion window when an acknowledgement arrives,
if allowed by the receiver&rsquo;s advertised window and the slow-start or congestion avoidance window
increase algorithm, without checking to see if the previous value of the congestion window has in
fact been used.</p>

<p>This document proposes that the window increase algorithm not be invoked during application-
limited periods. This restriction prevents the congestion window from growing arbitrarily large,
in the absence of evidence that the congestion window can be supported by the network.</p>

<h4>实现(1)</h4>

<p>发送方在发送数据包时，如果发送的数据包有负载，则会检测拥塞窗口是否超时。如果超时，则会使拥塞窗口失效并重新计算拥塞窗口。然后根据最近接收段的时间，确定是否进入pingpong模式。
```
    /<em> Congestion state accounting after a packet has been sent. </em>/<br/>
    static void tcp_event_data_sent (struct tcp_sock <em>tp, struct sock </em>sk)<br/>
    {<br/>
        struct inet_connection_sock *icsk = inet_csk(sk);<br/>
        const u32 now = tcp_time_stamp;</p>

<pre><code>    if (sysctl_tcp_slow_start_after_idle &amp;&amp;   
        (!tp-&gt;packets_out &amp;&amp; (s32) (now - tp-&gt;lsndtime) &gt; icsk-&gt;icsk_rto))  
        tcp_cwnd_restart(sk, __sk_dst_get(sk)); /* 重置cnwd */  

    tp-&gt;lsndtime = now; /* 更新最近发包的时间*/  

    /* If it is a reply for ato after last received packets,  
     * enter pingpong mode. */  
    if ((u32)(now - icsk-&gt;icsk_ack.lrcvtime) &lt; icsk.icsk_ack.ato)  
        icsk-&gt;icsk_ack.pingpong = 1;  
}  
</code></pre>

<pre><code>
tcp_event_data_sent()中，符合三个条件才重置cwnd：

（1）tcp_slow_start_after_idle选项设置，这个内核默认置为1
（2）tp-&gt;packets_out == 0，表示网络中没有未确认数据包
（3）now - tp-&gt;lsndtime &gt; icsk-&gt;icsk_rto，距离上次发送数据包的时间超过了RTO
</code></pre>

<pre><code>/* RFC2861. Reset CWND after idle period longer RTO to "restart window". 
 * This is the first part of cnwd validation mechanism. 
 */  
static void tcp_cwnd_restart (struct sock *sk, const struct dst_entry *dst)  
{  
    struct tcp_sock *tp = tcp_sk(sk);  
    s32 delta = tcp_time_stamp - tp-&gt;lsndtime;  

    /* 关于tcp_init_cwnd()可见上一篇blog.*/  
    u32 restart_cwnd = tcp_init_cwnd(tp, dst);  
    u32 cwnd = tp-&gt;snd_cwnd;  

    /* 触发拥塞窗口重置事件*/  
    tcp_ca_event(sk, CA_EVENT_CWND_RESTART);  

    /* 阈值保存下来，并没有重置。*/  
    tp-&gt;snd_ssthresh = tcp_current_ssthresh(sk);  
    restart_cwnd = min(restart_cwnd, cwnd);  

    /* 闲置时间每超过一个RTO且cwnd比重置后的大时，cwnd减半。*/  
    while((delta -= inet_csk(sk)-&gt;icsk_rto) &gt; 0 &amp;&amp; cwnd &gt; restart_cwnd)  
        cwnd &gt;&gt; 1;  

    tp-&gt;snd_cwnd = max(cwnd, restart_cwnd); /* 取其大者！*/  
    tp-&gt;snd_cwnd_stamp = tcp_time_stamp;  
    tp-&gt;snd_cwnd_used = 0;  
}  
</code></pre>

<pre><code>
那么调用tcp_cwnd_restart()后，tp-&gt;snd_cwnd是多少呢？这个是不确定的，要看闲置时间delta、闲置前的cwnd、路由器中设置的initcwnd。当然，最大概率的是：拥塞窗口降为闲置前cwnd的一半。

#### 实现(2)

在发送方成功发送一个数据包后，会检查从发送队列发出而未确认的数据包是否用完拥塞窗口。
如果拥塞窗口被用完了，说明发送方收到网络限制；
如果拥塞窗口没被用完，且距离上次检查时间超过了RTO，说明发送方收到应用程序限制。
</code></pre>

<pre><code>/* Congestion window validation.(RFC2861) */  
static void tcp_cwnd_validate(struct sock *sk) {  
    struct tcp_sock *tp = tcp_sk(sk);  

    if (tp-&gt;packets_out &gt;= tp-&gt;snd_cwnd) {  
        /* Network is feed fully. */  
        tp-&gt;snd_cwnd_used = 0; /*不用这个变量*/  
        tp-&gt;snd_cwnd_stamp = tcp_time_stamp; /* 更新检测时间*/  

    } else {  
        /* Network starves. */  
        if (tp-&gt;packets_out &gt; tp-&gt;snd_cwnd_used)  
            tp-&gt;snd_cwnd_used = tp-&gt;packets_out; /* 更新已使用窗口*/  

            /* 如果距离上次检测的时间，即距离上次发包时间已经超过RTO*/  
            if (sysctl_tcp_slow_start_after_idle &amp;&amp;  
                (s32) (tcp_time_stamp - tp-&gt;snd_cwnd_stamp) &gt;= inet_csk(sk)-&gt;icsk_rto)  
                tcp_cwnd_application_limited(sk);  
    }  
}  
</code></pre>

<pre><code>
在发送方收到应用程序的限制期间，每隔RTO时间，都会调用tcp_cwnd_application_limited()来重新设置sshresh和cwnd，具体如下：
</code></pre>

<pre><code>/* RFC2861, slow part. Adjust cwnd, after it was not full during one rto. 
 * As additional protections, we do not touch cwnd in retransmission phases, 
 * and if application hit its sndbuf limit recently. 
 */  
void tcp_cwnd_application_limited(struct sock *sk)  
{  
    struct tcp_sock *tp = tcp_sk(sk);  

    /* 只有处于Open态，应用程序没受到sndbuf限制时，才进行 
     * ssthresh和cwnd的重置。 
     */  
    if (inet_csk(sk)-&gt;icsk_ca_state == TCP_CA_Open &amp;&amp;   
        sk-&gt;sk_socket &amp;&amp; !test_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags)) {  

        /* Limited by application or receiver window. */  
        u32 init_win = tcp_init_cwnd(tp, __sk_dst_get(sk));  
        u32 win_used = max(tp-&gt;snd_cwnd_used, init_win);  

        /* 没用完拥塞窗口*/  
        if (win_used &lt; tp-&gt;snd_cwnd) {  
            /* 并没有减小ssthresh，反而增大，保留了过去的信息，以便之后有数据发送 
              * 时能快速增大到接近此时的窗口。 
              */  
            tp-&gt;snd_ssthresh = tcp_current_ssthresh(sk);   
            /* 减小了snd_cwnd */  
            tp-&gt;snd_cwnd = (tp-&gt;snd_cwnd + win_used) &gt;&gt; 1;  
        }  
        tp-&gt;snd_cwnd_used = 0;  
    }  
    tp-&gt;snd_cwnd_stamp = tcp_time_stamp; /* 更新最近的数据包发送时间*/  
}  
</code></pre>

<p>```</p>

<p>发送方受到应用程序限制，且限制的时间每经过RTO后，就会调用以上函数来处理snd_ssthresh和snd_cwnd：</p>

<p>（1）snd_ssthresh = max(snd_ssthresh, &frac34; cwnd)</p>

<p>慢启动阈值并没有减小，相反，如果此时cwnd较大，ssthresh会相应的增大。ssthresh是一个很重要的参数，它保留了旧的信息。这样一来，如果应用程序产生了大量的数据，发送方不再受到限制后，经过慢启动阶段，拥塞窗口就能快速恢复到接近以前的值了。</p>

<p>（2）snd_cwnd = (snd_cwnd + snd_cwnd_used) / 2</p>

<p>因为snd_cwnd_used &lt; snd_cwnd，所以snd_cwnd是减小了的。减小snd_cwnd是为了不让它盲目的增长。因为发送方没有利用完拥塞窗口，并不能检测到网络是否能承受该拥塞窗口，这时的增长是无根据的。</p>

<h4>结论</h4>

<p>在发送完数据包后，通过对拥塞窗口有效性的检验，能够避免使用不合理的拥塞窗口。</p>

<p>拥塞窗口代表着网络的状况，通过避免使用不合理的拥塞窗口，就能得到正确的网络状况，而不会采取一些不恰当的措施。</p>

<p>在上文的两种情况下，通过TCP的拥塞窗口有效性验证机制（TCP congestion window validationmechanism），能够更合理的利用网络、避免丢包，从而提高传输效率。</p>

<h4>Reference</h4>

<p>RFC2861</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ixgbe]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-ixgbe/"/>
    <updated>2015-11-17T15:16:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-ixgbe</id>
    <content type="html"><![CDATA[<p><a href="http://www.pagefault.info/?p=403">http://www.pagefault.info/?p=403</a></p>

<p>这里分析的驱动代码是给予linux kernel 3.4.4</p>

<p>对应的文件在drivers/net/ethernet/intel 目录下，这个分析不涉及到很细节的地方，主要目的是理解下数据在协议栈和驱动之间是如何交互的。</p>

<p>首先我们知道网卡都是pci设备，因此这里每个网卡驱动其实就是一个pci驱动。并且intel这里是把好几个万兆网卡(82599/82598/x540)的驱动做在一起的。</p>

<p>首先我们来看对应的pci_driver的结构体，这里每个pci驱动都是一个pci_driver的结构体，而这里是多个万兆网卡共用这个结构体ixgbe_driver.</p>

<pre><code>    static struct pci_driver ixgbe_driver = {
        .name     = ixgbe_driver_name,
        .id_table = ixgbe_pci_tbl,
        .probe    = ixgbe_probe,
        .remove   = __devexit_p(ixgbe_remove),
    #ifdef CONFIG_PM
        .suspend  = ixgbe_suspend,
        .resume   = ixgbe_resume,
    #endif
        .shutdown = ixgbe_shutdown,
        .err_handler = &amp;ixgbe_err_handler
    };
</code></pre>

<p>然后是模块初始化方法,这里其实很简单，就是调用pci的驱动注册方法，把ixgbe挂载到pci设备链中。 这里不对pci设备的初始化做太多介绍，我以前的blog有这方面的介绍，想了解的可以去看看。这里我们只需要知道最终内核会调用probe回调来初始化ixgbe。</p>

<pre><code>    char ixgbe_driver_name[] = "ixgbe";
    static const char ixgbe_driver_string[] =
                    "Intel(R) 10 Gigabit PCI Express Network Driver";

    static int __init ixgbe_init_module(void)
    {
        int ret;
        pr_info("%s - version %s\n", ixgbe_driver_string, ixgbe_driver_version);
        pr_info("%s\n", ixgbe_copyright);

    #ifdef CONFIG_IXGBE_DCA
        dca_register_notify(&amp;dca_notifier);
    #endif

        ret = pci_register_driver(&amp;ixgbe_driver);
        return ret;
    }
</code></pre>

<p>这里不去追究具体如何调用probe的细节，我们直接来看probe函数，这个函数中通过硬件的信息来确定需要初始化那个驱动(82598/82599/x540),然后核心的驱动结构就放在下面的这个数组中。</p>

<pre><code>    static const struct ixgbe_info *ixgbe_info_tbl[] = {
        [board_82598] = &amp;ixgbe_82598_info,
        [board_82599] = &amp;ixgbe_82599_info,
        [board_X540] = &amp;ixgbe_X540_info,
    };
</code></pre>

<p>ixgbe_probe函数很长，我们这里就不详细分析了，因为这部分就是对网卡进行初始化。不过我们关注下面几个代码片段。</p>

<p>首先是根据硬件的参数来取得对应的驱动值:</p>

<pre><code>    const struct ixgbe_info *ii = ixgbe_info_tbl[ent-&gt;driver_data];
</code></pre>

<p>然后就是如何将不同的网卡驱动挂载到对应的回调中，这里做的很简单，就是通过对应的netdev的结构取得adapter，然后所有的核心操作都是保存在adapter中的，最后将ii的所有回调拷贝给adapter就可以了。我们来看代码：</p>

<pre><code>        struct net_device *netdev;
        struct ixgbe_adapter *adapter = NULL;
        struct ixgbe_hw *hw;
        .....................................

        adapter = netdev_priv(netdev);
        pci_set_drvdata(pdev, adapter);

        adapter-&gt;netdev = netdev;
        adapter-&gt;pdev = pdev;
        hw = &amp;adapter-&gt;hw;
        hw-&gt;back = adapter;
        .......................................
        memcpy(&amp;hw-&gt;mac.ops, ii-&gt;mac_ops, sizeof(hw-&gt;mac.ops));
        hw-&gt;mac.type  = ii-&gt;mac;

        /* EEPROM */
        memcpy(&amp;hw-&gt;eeprom.ops, ii-&gt;eeprom_ops, sizeof(hw-&gt;eeprom.ops));
        .....................................
</code></pre>

<p>最后需要关注的就是设置网卡属性，这些属性一般来说都是通过ethtool 可以设置的属性(比如tso/checksum等),这里我们就截取一部分:</p>

<pre><code>        netdev-&gt;features = NETIF_F_SG |
                   NETIF_F_IP_CSUM |
                   NETIF_F_IPV6_CSUM |
                   NETIF_F_HW_VLAN_TX |
                   NETIF_F_HW_VLAN_RX |
                   NETIF_F_HW_VLAN_FILTER |
                   NETIF_F_TSO |
                   NETIF_F_TSO6 |
                   NETIF_F_RXHASH |
                   NETIF_F_RXCSUM;

        netdev-&gt;hw_features = netdev-&gt;features;

        switch (adapter-&gt;hw.mac.type) {
        case ixgbe_mac_82599EB:
        case ixgbe_mac_X540:
            netdev-&gt;features |= NETIF_F_SCTP_CSUM;
            netdev-&gt;hw_features |= NETIF_F_SCTP_CSUM |
                           NETIF_F_NTUPLE;
            break;
        default:
            break;
        }

        netdev-&gt;hw_features |= NETIF_F_RXALL;
        ..................................................

        netdev-&gt;priv_flags |= IFF_UNICAST_FLT;
        netdev-&gt;priv_flags |= IFF_SUPP_NOFCS;

        if (adapter-&gt;flags &amp; IXGBE_FLAG_SRIOV_ENABLED)
            adapter-&gt;flags &amp;= ~(IXGBE_FLAG_RSS_ENABLED |
                        IXGBE_FLAG_DCB_ENABLED);
        ...................................................................
        if (pci_using_dac) {
            netdev-&gt;features |= NETIF_F_HIGHDMA;
            netdev-&gt;vlan_features |= NETIF_F_HIGHDMA;
        }

        if (adapter-&gt;flags2 &amp; IXGBE_FLAG2_RSC_CAPABLE)
            netdev-&gt;hw_features |= NETIF_F_LRO;
        if (adapter-&gt;flags2 &amp; IXGBE_FLAG2_RSC_ENABLED)
            netdev-&gt;features |= NETIF_F_LRO;
</code></pre>

<p>然后我们来看下中断的注册，因为万兆网卡大部分都是多对列网卡(配合msix)，因此对于上层软件来说，就好像有多个网卡一样，它们之间的数据是相互独立的，这里读的话主要是napi驱动的poll方法，后面我们会分析这个.</p>

<p>到了这里或许要问那么网卡是如何挂载回调给上层，从而上层来发送数据呢，这里是这样子的，每个网络设备都有一个回调函数表(比如ndo_start_xmit)来供上层调用，而在ixgbe中的话，就是ixgbe_netdev_ops，下面就是这个结构，不过只是截取了我们很感兴趣的几个地方.</p>

<p>不过这里注意，读回调并不在里面，这是因为写是软件主动的，而读则是硬件主动的。现在ixgbe是NAPI的，因此它的poll回调是ixgbe_poll，是中断注册时候通过netif_napi_add添加进去的。</p>

<pre><code>    static const struct net_device_ops ixgbe_netdev_ops = {
        .ndo_open       = ixgbe_open,
        .ndo_stop       = ixgbe_close,
        .ndo_start_xmit     = ixgbe_xmit_frame,
        .ndo_select_queue   = ixgbe_select_queue,
        .ndo_set_rx_mode    = ixgbe_set_rx_mode,
        .ndo_validate_addr  = eth_validate_addr,
        .ndo_set_mac_address    = ixgbe_set_mac,
        .ndo_change_mtu     = ixgbe_change_mtu,
        .ndo_tx_timeout     = ixgbe_tx_timeout,
        .................................................
        .ndo_set_features = ixgbe_set_features,
        .ndo_fix_features = ixgbe_fix_features,
    };
</code></pre>

<p>这里我们最关注的其实就是ndo_start_xmit回调，这个回调就是驱动提供给协议栈的发送回调接口。我们来看这个函数.</p>

<p>它的实现很简单，就是选取对应的队列，然后调用ixgbe_xmit_frame_ring来发送数据。</p>

<pre><code>    static netdev_tx_t ixgbe_xmit_frame(struct sk_buff *skb,
                        struct net_device *netdev)
    {
        struct ixgbe_adapter *adapter = netdev_priv(netdev);
        struct ixgbe_ring *tx_ring;

        if (skb-&gt;len &lt;= 0) {
            dev_kfree_skb_any(skb);
            return NETDEV_TX_OK;
        }

        /*
         * The minimum packet size for olinfo paylen is 17 so pad the skb
         * in order to meet this minimum size requirement.
         */
        if (skb-&gt;len &lt; 17) {
            if (skb_padto(skb, 17))
                return NETDEV_TX_OK;
            skb-&gt;len = 17;
        }
        //取得对应的队列
        tx_ring = adapter-&gt;tx_ring[skb-&gt;queue_mapping];
        //发送数据
        return ixgbe_xmit_frame_ring(skb, adapter, tx_ring);
    }
</code></pre>

<p>而在ixgbe_xmit_frame_ring中，我们就关注两个地方，一个是tso(什么是TSO，请自行google)，一个是如何发送.</p>

<pre><code>        tso = ixgbe_tso(tx_ring, first, &amp;hdr_len);
        if (tso &lt; 0)
            goto out_drop;
        else if (!tso)
            ixgbe_tx_csum(tx_ring, first);

        /* add the ATR filter if ATR is on */
        if (test_bit(__IXGBE_TX_FDIR_INIT_DONE, &amp;tx_ring-&gt;state))
            ixgbe_atr(tx_ring, first);

    #ifdef IXGBE_FCOE
    xmit_fcoe:
    #endif /* IXGBE_FCOE */
        ixgbe_tx_map(tx_ring, first, hdr_len);
</code></pre>

<p>调用ixgbe_tso处理完tso之后，就会调用ixgbe_tx_map来发送数据。而ixgbe_tx_map所做的最主要是两步，第一步请求DMA，第二步写寄存器，通知网卡发送数据.</p>

<pre><code>        dma = dma_map_single(tx_ring-&gt;dev, skb-&gt;data, size, DMA_TO_DEVICE);
        if (dma_mapping_error(tx_ring-&gt;dev, dma))
            goto dma_error;

        /* record length, and DMA address */
        dma_unmap_len_set(first, len, size);
        dma_unmap_addr_set(first, dma, dma);

        tx_desc-&gt;read.buffer_addr = cpu_to_le64(dma);

        for (;;) {
            while (unlikely(size &gt; IXGBE_MAX_DATA_PER_TXD)) {
                tx_desc-&gt;read.cmd_type_len =
                    cmd_type | cpu_to_le32(IXGBE_MAX_DATA_PER_TXD);

                i++;
                tx_desc++;
                if (i == tx_ring-&gt;count) {
                    tx_desc = IXGBE_TX_DESC(tx_ring, 0);
                    i = 0;
                }

                dma += IXGBE_MAX_DATA_PER_TXD;
                size -= IXGBE_MAX_DATA_PER_TXD;

                tx_desc-&gt;read.buffer_addr = cpu_to_le64(dma);
                tx_desc-&gt;read.olinfo_status = 0;
            }

            ...................................................
            data_len -= size;

            dma = skb_frag_dma_map(tx_ring-&gt;dev, frag, 0, size,
                           DMA_TO_DEVICE);
            ..........................................................

            frag++;
        }
        .................................
        tx_ring-&gt;next_to_use = i;

        /* notify HW of packet */
        writel(i, tx_ring-&gt;tail);
        .................
</code></pre>

<p>上面的操作是异步的，也就是说此时内核还不能释放SKB，而是网卡硬件发送完数据之后，会再次产生中断通知内核，然后内核才能释放内存.接下来我们来看这部分代码。</p>

<p>首先来看的是中断注册的代码，这里我们假设启用了MSIX,那么网卡的中断注册回调就是ixgbe_request_msix_irqs函数，这里我们可以看到调用request_irq函数来注册回调，并且每个队列都有自己的中断号。</p>

<pre><code>    static int ixgbe_request_msix_irqs(struct ixgbe_adapter *adapter)
    {
        struct net_device *netdev = adapter-&gt;netdev;
        int q_vectors = adapter-&gt;num_msix_vectors - NON_Q_VECTORS;
        int vector, err;
        int ri = 0, ti = 0;

        for (vector = 0; vector &lt; q_vectors; vector++) {
            struct ixgbe_q_vector *q_vector = adapter-&gt;q_vector[vector];
            struct msix_entry *entry = &amp;adapter-&gt;msix_entries[vector];
            .......................................................................
            err = request_irq(entry-&gt;vector, &amp;ixgbe_msix_clean_rings, 0,
                      q_vector-&gt;name, q_vector);
            if (err) {
                e_err(probe, "request_irq failed for MSIX interrupt "
                      "Error: %d\n", err);
                goto free_queue_irqs;
            }
            /* If Flow Director is enabled, set interrupt affinity */
            if (adapter-&gt;flags &amp; IXGBE_FLAG_FDIR_HASH_CAPABLE) {
                /* assign the mask for this irq */
                irq_set_affinity_hint(entry-&gt;vector,
                              &amp;q_vector-&gt;affinity_mask);
            }
        }

        ..............................................

        return 0;

    free_queue_irqs:
        ...............................
        return err;
    }
</code></pre>

<p>而对应的中断回调是ixgbe_msix_clean_rings,而这个函数呢，做的事情很简单(需要熟悉NAPI的原理，我以前的blog有介绍),就是调用napi_schedule来重新加入软中断处理.</p>

<pre><code>    static irqreturn_t ixgbe_msix_clean_rings(int irq, void *data)
    {
        struct ixgbe_q_vector *q_vector = data;

        /* EIAM disabled interrupts (on this vector) for us */

        if (q_vector-&gt;rx.ring || q_vector-&gt;tx.ring)
            napi_schedule(&amp;q_vector-&gt;napi);

        return IRQ_HANDLED;
    }
</code></pre>

<p>而NAPI驱动我们知道，最终是会调用网卡驱动挂载的poll回调，在ixgbe中，对应的回调就是ixgbe_poll，那么也就是说这个函数要做两个工作，一个是处理读，一个是处理写完之后的清理.</p>

<pre><code>    int ixgbe_poll(struct napi_struct *napi, int budget)
    {
        struct ixgbe_q_vector *q_vector =
                    container_of(napi, struct ixgbe_q_vector, napi);
        struct ixgbe_adapter *adapter = q_vector-&gt;adapter;
        struct ixgbe_ring *ring;
        int per_ring_budget;
        bool clean_complete = true;

    #ifdef CONFIG_IXGBE_DCA
        if (adapter-&gt;flags &amp; IXGBE_FLAG_DCA_ENABLED)
            ixgbe_update_dca(q_vector);
    #endif
        //清理写
        ixgbe_for_each_ring(ring, q_vector-&gt;tx)
            clean_complete &amp;= !!ixgbe_clean_tx_irq(q_vector, ring);

        /* attempt to distribute budget to each queue fairly, but don't allow
         * the budget to go below 1 because we'll exit polling */
        if (q_vector-&gt;rx.count &gt; 1)
            per_ring_budget = max(budget/q_vector-&gt;rx.count, 1);
        else
            per_ring_budget = budget;
        //读数据，并清理已完成的
        ixgbe_for_each_ring(ring, q_vector-&gt;rx)
            clean_complete &amp;= ixgbe_clean_rx_irq(q_vector, ring,
                                 per_ring_budget);

        /* If all work not completed, return budget and keep polling */
        if (!clean_complete)
            return budget;

        /* all work done, exit the polling mode */
        napi_complete(napi);
        if (adapter-&gt;rx_itr_setting &amp; 1)
            ixgbe_set_itr(q_vector);
        if (!test_bit(__IXGBE_DOWN, &amp;adapter-&gt;state))
            ixgbe_irq_enable_queues(adapter, ((u64)1 &lt;&lt; q_vector-&gt;v_idx));

        return 0;
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[cubic]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-cubic/"/>
    <updated>2015-11-17T15:08:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-cubic</id>
    <content type="html"><![CDATA[<p><a href="http://www.pagefault.info/?p=145">http://www.pagefault.info/?p=145</a></p>

<p>这次主要来看一下内核拥塞控制算法cubic的实现，在linux kernel中实现了很多种拥塞控制算法，不过新的内核(2.6.19之后)默认是cubic(想得到当前内核使用的拥塞控制算法可以察看/proc/sys/net/ipv4/tcp_congestion_control这个值).下面是最新的redhat 6的拥塞控制算法(rh5还是bic算法):
<code>
    [root@rhel6 ~]# cat /proc/sys/net/ipv4/tcp_congestion_control
    cubic
</code>
这个算法的paper在这里：</p>

<p><a href="http://netsrv.csc.ncsu.edu/export/cubic_a_new_tcp_2008.pdf">http://netsrv.csc.ncsu.edu/export/cubic_a_new_tcp_2008.pdf</a></p>

<p>拥塞控制算法会在tcp_ack中被调用，如果是正常的ack(比如不是重复的，不是sack等等)就会进入拥塞控制算法。</p>

<p>cubic会调用tcp_slow_start这个方法(基本上每种拥塞控制算法都会调用它)，这个方法主要是处理slow start，而内核中的slow start是这样子的，接收一个ack，snd_cwnd就会加1，然后当cwnd大于设置的拥塞窗口阀值snd_ssthresh的时候，就会进入拥塞避免状态。而在发送数据包的时候，会判断in_flight(可以认为是发送还没确认的数据包，它等于发送未确认的数据包－sack的数据段－丢失的数据段＋重传的数据段，我的前面的blog有详细解释这个数据段)是否大于snd_cwnd,如果大于等于则不会发送数据，如果小于才会继续发送数据。</p>

<p>而进入拥塞避免状态之后，窗口的增长速度将会减缓，</p>

<p>来看一下我用jprobe hook tcp_slow_start(slow start处理函数) 和 tcp_cong_avoid_ai (拥塞避免处理)的数据。</p>

<p>在下面的数据中sk表示当前socket的地址， in_flight packet表示发送还未接收的包, snd_cwnd表示发送拥塞窗口。
然后详细解释下count后面的两个值，其中第一个是snd_cwnd_cnt，表示在当前的拥塞窗口中已经发送的数据段的个数，而第二个是struct bictcp的一个域cnt，它是cubic拥塞算法的核心，主要用来控制在拥塞避免状态的时候，什么时候才能增大拥塞窗口，具体实现是通过比较它和snd_cwnd_cnt，来决定是否增大拥塞窗口，而这个值的计算，我这里不会分析，想了解的，可以去看cubic的paper。</p>

<p>还有一个需要注意的地方就是ssthresh，可以看到这个值在一开始初始化为一个最大的值，然后在进入拥塞避免状态的时候被设置为前一次拥塞窗口的大小.这个处理可以看rfc2581的这段：</p>

<p>  The initial value of ssthresh may be arbitrarily high (i.e., the size of the advertised window), but it may be reduced in response to congestion. When cwnd &lt; ssthresh, the slow-start algorithm is used and when cwnd > ssthresh, the congestion avoidance algorithm is used. When cwnd and ssthresh are equal, the sender may use either of them.</p>

<p>我们后面会看到这个值在cubic中是如何被设置的。
<code>
    //进入slow start，可以看到拥塞窗口默认初始值是3，然后每次接收到ack，都会加1.
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 2, snd_cwnd is 3, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 3, snd_cwnd is 4, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 4, snd_cwnd is 5, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 5, snd_cwnd is 6, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 4, snd_cwnd is 7, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 7, snd_cwnd is 8, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 6, snd_cwnd is 9, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 9, snd_cwnd is 10, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 10, snd_cwnd is 11, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 11, snd_cwnd is 12, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 10, snd_cwnd is 13, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 9, snd_cwnd is 14, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 14, snd_cwnd is 15, ssthresh is 2147483647, count is [0:0]
    //ssthresh被更新为当前拥塞窗口的大小，后面会看到为什么是16
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 13, snd_cwnd is 16, ssthresh is 16, count is [0:0]
    //进入拥塞避免，可以清楚的看到，此时拥塞窗口大于对应的阀值ssthresh.
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 16, snd_cwnd is 17, ssthresh is 16, count is [0:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 15, snd_cwnd is 17, ssthresh is 16, count is [1:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 16, snd_cwnd is 17, ssthresh is 16, count is [2:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 15, snd_cwnd is 17, ssthresh is 16, count is [3:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 14, snd_cwnd is 17, ssthresh is 16, count is [4:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 13, snd_cwnd is 17, ssthresh is 16, count is [5:877]
    //这里注意，其中count的第一个值是一直线性增长的，也就是说下面省略了大概80条log，而在这80几次中拥塞窗口一直维持在17没有变化
    ....................................................................................................................
    //可以看到cnt变为3，也就是说明当执行完拥塞避免就会增加窗口了。
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 13, snd_cwnd is 17, ssthresh is 16, count is [91:3]
    //增加窗口的大小，然后将snd_cwnd_cnt reset为0.
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 11, snd_cwnd is 18, ssthresh is 16, count is [0:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 16, snd_cwnd is 18, ssthresh is 16, count is [1:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 14, snd_cwnd is 18, ssthresh is 16, count is [2:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 12, snd_cwnd is 18, ssthresh is 16, count is [3:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 12, snd_cwnd is 18, ssthresh is 16, count is [4:6]
</code></p>

<p>可以看到在slow start的状态，发送拥塞窗口就是很简单的每次加1，而当进入拥塞避免之后，明显的拥塞窗口的增大速度变慢很多。</p>

<p>接下来来看具体的代码是如何实现的.</p>

<p>首先来看bictcp_cong_avoid，也就是cubic拥塞控制算法的handler(一般来说在tcp_ack中被调用)，它有3个参数，第一个是对应的sock，第二个是对应的ack序列号，而第三个就是比较重要的一个变量，表示发送还没有被ack的数据包(在linux 内核tcp拥塞处理一中详细介绍过内核中这些变量)，这个变量是拥塞控制的核心。</p>

<pre><code>    static void bictcp_cong_avoid(struct sock *sk, u32 ack, u32 in_flight)
    {
        struct tcp_sock *tp = tcp_sk(sk);
        struct bictcp *ca = inet_csk_ca(sk);
        //判断发送拥塞窗口是否到达限制，如果到达限制则直接返回。
        if (!tcp_is_cwnd_limited(sk, in_flight))
            return;
        //开始决定进入slow start还是拥塞控制状态
        if (tp-&gt;snd_cwnd &lt;= tp-&gt;snd_ssthresh) {
            //是否需要reset对应的bictcp的值
            if (hystart &amp;&amp; after(ack, ca-&gt;end_seq))
                bictcp_hystart_reset(sk);
            //进入slow start状态
            tcp_slow_start(tp);
        } else {
            //进入拥塞避免状态，首先会更新ca-&gt;cnt.
            bictcp_update(ca, tp-&gt;snd_cwnd);
            //然后进入拥塞避免
            tcp_cong_avoid_ai(tp, ca-&gt;cnt);
        }
    }
</code></pre>

<p>接下来就是看tcp_is_cwnd_limited，这个函数主要是实现RFC2861中对拥塞窗口的检测。它返回1说明拥塞窗口被限制，我们需要增加拥塞窗口，否则的话，就不需要增加拥塞窗口。</p>

<p>然后这里还有两个判断，先来看第一个 gso的概念，gso是Generic Segmentation Offload的简写，他的主要功能就是尽量的延迟数据包的传输，以便与在最恰当的时机传输数据包，这个机制是处于数据包离开协议栈与进入驱动之间。比如如果驱动支持TSO的话，gso就会将多个unsegmented的数据段传递给驱动。而TSO是TCP Segmentation Offload的缩写，它表示驱动支持协议栈发送大的MTU的数据段，然后硬件负责来切包，然后将数据发送出去，这样子的话，就能提高系统的吞吐。这几个东西(还有GRO)，以后我会详细分析，现在只需要大概知道他们是干什么的。</p>

<p>而在这里如果支持gso，就有可能是tso defer住了数据包，因此这里会进行几个相关的判断，来看需不需要增加拥塞窗口。。</p>

<p>然后是burst的概念，主要用来控制网络流量的突发性增大，也就是说当left数据(还能发送的数据段数)大于burst值的时候，我们需要暂时停止增加窗口，因为此时有可能我们这边数据发送过快。</p>

<pre><code>    int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
    {
        const struct tcp_sock *tp = tcp_sk(sk);
        u32 left;
        //比较发送未确认和发送拥塞窗口的大小
        if (in_flight &gt;= tp-&gt;snd_cwnd)
            return 1;
        //得到还能发送的数据包的段数
        left = tp-&gt;snd_cwnd - in_flight;
        if (sk_can_gso(sk) &amp;&amp;
            left * sysctl_tcp_tso_win_divisor &lt; tp-&gt;snd_cwnd &amp;&amp;
            left * tp-&gt;mss_cache &lt; sk-&gt;sk_gso_max_size)
            return 1;
        //看是否还能发送的数据包是否小于等于burst
        return left &lt;= tcp_max_burst(tp);
    }
</code></pre>

<p>接下来来看snd_ssthresh是如何被设置的，这个值在加载cubic模块的时候可以传递一个我们制定的值给它，不过，默认是很大的值，我这里是2147483647,然后在接收ack期间(slow start)期间会调整这个值，在cubic中，默认是16（一般来说说当拥塞窗口到达16的时候，snd_ssthresh会被设置为16).</p>

<p>在cubic中有两个可以设置snd_ssthresh的地方一个是hystart_update，一个是bictcp_recalc_ssthresh，后一个我这里就不介绍了，以后介绍拥塞状态机的时候会详细介绍，现在只需要知道，只有遇到拥塞的时候，需要调整snd_ssthres的时候，我们才需要调用bictcp_recalc_ssthresh。</p>

<p>而hystart_update是在bictcp_acked中被调用，而bictcp_acked则是基本每次收到ack都会调用这个函数，我们来看在bictcp_acked中什么情况就会调用hystart_update：</p>

<pre><code>    /* hystart triggers when cwnd is larger than some threshold */
    if (hystart &amp;&amp; tp-&gt;snd_cwnd &lt;= tp-&gt;snd_ssthresh &amp;&amp;
        tp-&gt;snd_cwnd &gt;= hystart_low_window)
        hystart_update(sk, delay);
</code></pre>

<p>其中hystart是hybrid slow start打开的标志，默认是开启，hystart_low_window是设置snd_ssthresh的最小拥塞窗口值，默认是16。而tp->snd_ssthresh默认是一个很大的值，因此这里就知道了，当拥塞窗口增大到16的时候我们就会进去hystart_update来更新snd_ssthresh.因此hystart_updat换句话来说也就是主要用于是否退出slow start。</p>

<pre><code>    static void hystart_update(struct sock *sk, u32 delay)
    {
        struct tcp_sock *tp = tcp_sk(sk);
        struct bictcp *ca = inet_csk_ca(sk);

        if (!(ca-&gt;found &amp; hystart_detect)) {
            .................................................................
            /*
             * Either one of two conditions are met,
             * we exit from slow start immediately.
             */
            //found是一个是否退出slow start的标记
            if (ca-&gt;found &amp; hystart_detect)
                //设置snd_ssthresh
                tp-&gt;snd_ssthresh = tp-&gt;snd_cwnd;
        }
    }
</code></pre>

<p>然后是slow start的处理,这里有关abc的处理，注释都很详细了，这里就不解释了，我们主要看abc关闭的部分。这里使用cnt，也是主要为了打开abc之后的slow start。</p>

<p>这是abc（Appropriate Byte Counting）相关的rfc：</p>

<p><a href="http://www.faqs.org/rfcs/rfc3465.html">http://www.faqs.org/rfcs/rfc3465.html</a></p>

<p>Appropriate Byte Countin会导致拥塞控制算法很激进，比如打开它之后就不一定每次ack都会执行slow start，而且窗口也会增加的快很多。</p>

<pre><code>    void tcp_slow_start(struct tcp_sock *tp)
    {
        int cnt; /* increase in packets */

        /* RFC3465: ABC Slow start
         * Increase only after a full MSS of bytes is acked
         *
         * TCP sender SHOULD increase cwnd by the number of
         * previously unacknowledged bytes ACKed by each incoming
         * acknowledgment, provided the increase is not more than L
         */
        if (sysctl_tcp_abc &amp;&amp; tp-&gt;bytes_acked &lt; tp-&gt;mss_cache)
            return;
        //限制slow start的cnt
        if (sysctl_tcp_max_ssthresh &gt; 0 &amp;&amp; tp-&gt;snd_cwnd &gt; sysctl_tcp_max_ssthresh)
            cnt = sysctl_tcp_max_ssthresh &gt;&gt; 1; /* limited slow start */
        else
            cnt = tp-&gt;snd_cwnd;         /* exponential increase */

        /* RFC3465: ABC
         * We MAY increase by 2 if discovered delayed ack
         */
        if (sysctl_tcp_abc &gt; 1 &amp;&amp; tp-&gt;bytes_acked &gt;= 2*tp-&gt;mss_cache)
            cnt &lt;&lt;= 1;
        tp-&gt;bytes_acked = 0;
        //更新cnt，也就是当前拥塞窗口接受的段的个数.
        tp-&gt;snd_cwnd_cnt += cnt;
        while (tp-&gt;snd_cwnd_cnt &gt;= tp-&gt;snd_cwnd) {
            //这里snd_cwnd_cnt是snd_cwnd的几倍，拥塞窗口就增加几。
            tp-&gt;snd_cwnd_cnt -= tp-&gt;snd_cwnd;
            //如果拥塞窗口没有超过最大值，则加一
            if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp)
                tp-&gt;snd_cwnd++;
        }
    }
</code></pre>

<p>最后是拥塞避免的处理。这里主要的步骤就是通过判断当前的拥塞窗口下已经发送的数据段的个数是否大于算法计算出来的值w，如果大于我们才能增加拥塞窗口值，否则之需要增加snd_cwnd_cnt。</p>

<pre><code>    void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w)
    {
        //判断是否大于我们的标记值
        if (tp-&gt;snd_cwnd_cnt &gt;= w) {
            if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp)
                tp-&gt;snd_cwnd++;
            tp-&gt;snd_cwnd_cnt = 0;
        } else {
            //增加计数值
            tp-&gt;snd_cwnd_cnt++;
        }
    }
</code></pre>
]]></content>
  </entry>
  
</feed>
