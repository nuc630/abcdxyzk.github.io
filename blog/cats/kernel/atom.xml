<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-09-30T16:01:54+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[kernel 3.10内核源码分析--Out of Memory(OOM)处理流程]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/30/kernel-mm-oom/"/>
    <updated>2015-09-30T15:56:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/30/kernel-mm-oom</id>
    <content type="html"><![CDATA[<p><a href="http://blog.chinaunix.net/uid-20671208-id-4440249.html">http://blog.chinaunix.net/uid-20671208-id-4440249.html</a></p>

<p>Out Of Memory(OOM)，即内存耗尽，当系统中内存耗尽时，如果不做处理，将处于崩溃的边缘，因为无内核资源可用，而系统运行时刻都可能需要申请内存。这时，内核需要采取一定的措施来防止系统崩溃，这就是我们熟知的OOM流程，其实就是要回收一些内存，而走到OOM流程，已经基本说明其它的回收内存的手段都已经尝试过了(比如回收cache)，这里通常只能通过kill进程来回收内存了，而选择被kill进程的标准就比较简单直接了，总体就是：谁用的多，就kill谁。</p>

<p>OOM处理的基本流程简单描述如下：</p>

<p>1、检查是否配置了/proc/sys/kernel/panic_on_oom，如果是则直接触发panic。</p>

<p>2、检查是否配置了oom_kill_allocating_task，即是否需要kill current进程来回收内存，如果是，且current进程是killable的，则kill current进程。</p>

<p>3、根据既定策略选择需要kill的process，基本策略为：通过进程的内存占用情况计算“点数”，点数最高者被选中。</p>

<p>4、如果没有选出来可kill的进程，那么直接panic(通常不会走到这个流程，但也有例外，比如，当被选中的进程处于D状态，或者正在被kill)</p>

<p>5、kill掉被选中的进程，以释放内存。</p>

<p>代码注释如下：</p>

<pre><code>    /*
      * OOM处理的主流程，上面的注释应该比较清楚了。
      */
    void out_of_memory(struct zonelist *zonelist, gfp_t gfp_mask,
            int order, nodemask_t *nodemask, bool force_kill)
    {
        const nodemask_t *mpol_mask;
        struct task_struct *p;
        unsigned long totalpages;
        unsigned long freed = 0;
        unsigned int uninitialized_var(points);
        enum oom_constraint constraint = CONSTRAINT_NONE;
        int killed = 0;

        // 调用block通知链oom_nofify_list中的函数
        blocking_notifier_call_chain(&amp;oom_notify_list, 0, &amp;freed);

        if (freed &gt; 0)
            /* Got some memory back in the last second. */
            return;

        /*
         * If current has a pending SIGKILL or is exiting, then automatically
         * select it. The goal is to allow it to allocate so that it may
         * quickly exit and free its memory.
         */
        /*
         * 如果当前进程有pending的SIGKILL(9)信号，或者正在退出，则选择当前进程来kill,
         * 这样可以最快的达到释放内存的目的。
         */
        if (fatal_signal_pending(current) || current-&gt;flags &amp; PF_EXITING) {
            set_thread_flag(TIF_MEMDIE);
            return;
        }

        /*
         * Check if there were limitations on the allocation (only relevant for
         * NUMA) that may require different handling.
         */
        /*
         * 检查是否有限制，有几种不同的限制策略，仅用于NUMA场景
         */
        constraint = constrained_alloc(zonelist, gfp_mask, nodemask,
                            &amp;totalpages);
        mpol_mask = (constraint == CONSTRAINT_MEMORY_POLICY) ? nodemask : NULL;
        // 检查是否配置了/proc/sys/kernel/panic_on_oom，如果是则直接触发panic
        check_panic_on_oom(constraint, gfp_mask, order, mpol_mask);

        /*
         * 检查是否配置了oom_kill_allocating_task，即是否需要kill current进程来
         * 回收内存，如果是，且current进程是killable的，则kill current进程。
         */
        if (sysctl_oom_kill_allocating_task &amp;&amp; current-&gt;mm &amp;&amp;
         !oom_unkillable_task(current, NULL, nodemask) &amp;&amp;
         current-&gt;signal-&gt;oom_score_adj != OOM_SCORE_ADJ_MIN) {
            get_task_struct(current);
            // kill被选中的进程。
            oom_kill_process(current, gfp_mask, order, 0, totalpages, NULL,
                     nodemask,
                     "Out of memory (oom_kill_allocating_task)");
            goto out;
        }

        // 根据既定策略选择需要kill的process。
        p = select_bad_process(&amp;points, totalpages, mpol_mask, force_kill);
        /* Found nothing?!?! Either we hang forever, or we panic. */
        /*
         * 如果没有选出来，即没有可kill的进程，那么直接panic
         * 通常不会走到这个流程，但也有例外，比如，当被选中的进程处于D状态，或者正在被kill
         */
        if (!p) {
            dump_header(NULL, gfp_mask, order, NULL, mpol_mask);
            panic("Out of memory and no killable processes...\n");
        }
        // kill掉被选中的进程，以释放内存。
        if (PTR_ERR(p) != -1UL) {
            oom_kill_process(p, gfp_mask, order, points, totalpages, NULL,
                     nodemask, "Out of memory");
            killed = 1;
        }
    out:
        /*
         * Give the killed threads a good chance of exiting before trying to
         * allocate memory again.
         */
        /*
         * 在重新分配内存之前，给被kill的进程1s的时间完成exit相关处理，通常情况
         * 下，1s应该够了。
         */
        if (killed)
            schedule_timeout_killable(1);
    }
</code></pre>

<p>out_of_memory->select_bad_process</p>

<p>通过select_bad_process函数选择被kill的进程，其基本流程为：</p>

<p>1、遍历系统中的所有进程，进行"点数"计算</p>

<p>2、进行一些特殊情况的处理，比如: 优先选择触发OOM的进程、不处理正在exit的进程等。</p>

<p>3、计算"点数"，选择点数最大的进程。通过函数oom_badness()</p>

<p>代码注释和分析如下：</p>

<pre><code>    /*
      * OOM流程中，用来选择被kill的进程的函数
      * @ppoints:点数，用来计算每个进程被"选中"可能性，点数越高，越可能被"选中"
      */
    static struct task_struct *select_bad_process(unsigned int *ppoints,
            unsigned long totalpages, const nodemask_t *nodemask,
            bool force_kill)
    {
        struct task_struct *g, *p;
        struct task_struct *chosen = NULL;
        unsigned long chosen_points = 0;

        rcu_read_lock();
        // 遍历系统中的所有进程，进行"点数"计算
        do_each_thread(g, p) {
            unsigned int points;

            /*
             * 进行一些特殊情况的处理，比如: 优先选择触发OOM的进程、不处理
             * 正在exit的进程等。
             */        
            switch (oom_scan_process_thread(p, totalpages, nodemask,
                            force_kill)) {
            case OOM_SCAN_SELECT:
                chosen = p;
                chosen_points = ULONG_MAX;
                /* fall through */
            case OOM_SCAN_CONTINUE:
                continue;
            case OOM_SCAN_ABORT:
                rcu_read_unlock();
                return ERR_PTR(-1UL);
            case OOM_SCAN_OK:
                break;
            };
            // 计算"点数"，选择点数最大的进程。
            points = oom_badness(p, NULL, nodemask, totalpages);
            if (points &gt; chosen_points) {
                chosen = p;
                chosen_points = points;
            }
        } while_each_thread(g, p);
        if (chosen)
            get_task_struct(chosen);
        rcu_read_unlock();

        *ppoints = chosen_points * 1000 / totalpages;
        return chosen;
    }
</code></pre>

<p>out_of_memory->select_bad_process->oom_scan_process_thread</p>

<p>oom_scan_process_thread函数的分析和注释如下：</p>

<pre><code>    enum oom_scan_t oom_scan_process_thread(struct task_struct *task,
            unsigned long totalpages, const nodemask_t *nodemask,
            bool force_kill)
    {
        // 如果进程正在exit
        if (task-&gt;exit_state)
            return OOM_SCAN_CONTINUE;
        /*
         * 如果进程不能被kill，比如: init进程或进程在nodemask对应的节点上，
         * 没有可以释放的内存。
         */
        if (oom_unkillable_task(task, NULL, nodemask))
            return OOM_SCAN_CONTINUE;

        /*
         * This task already has access to memory reserves and is being killed.
         * Don't allow any other task to have access to the reserves.
         */
        /*
         * 如果有进程正在被OOM流程kill，那么应该有内存可以释放了，就不需要再kill
         * 其它进程了，此时返回abort，结束oom kill流程。
         */
        if (test_tsk_thread_flag(task, TIF_MEMDIE)) {
            if (unlikely(frozen(task)))
                __thaw_task(task);
            if (!force_kill)
                return OOM_SCAN_ABORT;
        }
        // 如果不存在mm了(可能进程刚退出了)
        if (!task-&gt;mm)
            return OOM_SCAN_CONTINUE;

        /*
         * If task is allocating a lot of memory and has been marked to be
         * killed first if it triggers an oom, then select it.
         */
        // 优先选择触发OOM的进程。
        if (oom_task_origin(task))
            return OOM_SCAN_SELECT;

        if (task-&gt;flags &amp; PF_EXITING &amp;&amp; !force_kill) {
            /*
             * If this task is not being ptraced on exit, then wait for it
             * to finish before killing some other task unnecessarily.
             */
            if (!(task-&gt;group_leader-&gt;ptrace &amp; PT_TRACE_EXIT))
                return OOM_SCAN_ABORT;
        }
        return OOM_SCAN_OK;
    }
</code></pre>

<p>out_of_memory->select_bad_process->oom_badness</p>

<p>oom_badness用于计算进程的“点数”，点数最高者被选中，代码注释和分析如下：</p>

<pre><code>    /*
     * 计算进程"点数"(代表进程被选中的可能性)的函数，点数根据进程占用的物理内存来计算
     * 物理内存占用越多，被选中的可能性越大。root processes有3%的bonus。
     */
    unsigned long oom_badness(struct task_struct *p, struct mem_cgroup *memcg,
                 const nodemask_t *nodemask, unsigned long totalpages)
    {
        long points;
        long adj;

        if (oom_unkillable_task(p, memcg, nodemask))
            return 0;
        // 确认进程是否还存在
        p = find_lock_task_mm(p);
        if (!p)
            return 0;

        adj = (long)p-&gt;signal-&gt;oom_score_adj;
        if (adj == OOM_SCORE_ADJ_MIN) {
            task_unlock(p);
            return 0;
        }

        /*
         * The baseline for the badness score is the proportion of RAM that each
         * task's rss, pagetable and swap space use.
         */
        // 点数=rss(驻留内存/占用物理内存)+pte数+交换分区用量
        points = get_mm_rss(p-&gt;mm) + p-&gt;mm-&gt;nr_ptes +
             get_mm_counter(p-&gt;mm, MM_SWAPENTS);
        task_unlock(p);

        /*
         * Root processes get 3% bonus, just like the __vm_enough_memory()
         * implementation used by LSMs.
         */
        /*
         * root用户启动的进程，有总 内存*3% 的bonus，就是说可以使用比其它进程多3%的内存
         * 3%=30/1000
         */
        if (has_capability_noaudit(p, CAP_SYS_ADMIN))
            adj -= 30;

        /* Normalize to oom_score_adj units */
        // 归一化"点数"单位
        adj *= totalpages / 1000;
        points += adj;

        /*
         * Never return 0 for an eligible task regardless of the root bonus and
         * oom_score_adj (oom_score_adj can't be OOM_SCORE_ADJ_MIN here).
         */
        return points &gt; 0 ? points : 1;
    }
</code></pre>

<p>out_of_memory->oom_kill_process</p>

<p>oom_kill_process()函数用于：kill被选中的进程，其实就是给指定进程发送SIGKILL信号，待被选中进程返回用户态时，进行信号处理。</p>

<p>相关代码注释和分析如下：</p>

<pre><code>    /*
      * kill被选中的进程，在OOM流程中被调用
      */
    void oom_kill_process(struct task_struct *p, gfp_t gfp_mask, int order,
             unsigned int points, unsigned long totalpages,
             struct mem_cgroup *memcg, nodemask_t *nodemask,
             const char *message)
    {
        struct task_struct *victim = p;
        struct task_struct *child;
        struct task_struct *t = p;
        struct mm_struct *mm;
        unsigned int victim_points = 0;
        static DEFINE_RATELIMIT_STATE(oom_rs, DEFAULT_RATELIMIT_INTERVAL,
                         DEFAULT_RATELIMIT_BURST);

        /*
         * If the task is already exiting, don't alarm the sysadmin or kill
         * its children or threads, just set TIF_MEMDIE so it can die quickly
         */
        /*
         * 如果进程正在exiting，就没有必要再kill它了，直接设置TIF_MEMDIE，然后返回。
        */
        if (p-&gt;flags &amp; PF_EXITING) {
            set_tsk_thread_flag(p, TIF_MEMDIE);
            put_task_struct(p);
            return;
        }

        if (__ratelimit(&amp;oom_rs))
            dump_header(p, gfp_mask, order, memcg, nodemask);

        task_lock(p);
        pr_err("%s: Kill process %d (%s) score %d or sacrifice child\n",
            message, task_pid_nr(p), p-&gt;comm, points);
        task_unlock(p);

        /*
         * If any of p's children has a different mm and is eligible for kill,
         * the one with the highest oom_badness() score is sacrificed for its
         * parent. This attempts to lose the minimal amount of work done while
         * still freeing memory.
         */
        /*
         * 如果被选中的进程的子进程，不跟其共享mm(通常是这样)，且膐om_badness的
         * 得分更高，那么重新选择该子进程为被kill的进程。
         */
        read_lock(&amp;tasklist_lock);
        do {
            // 遍历被选中进程的所有子进程
            list_for_each_entry(child, &amp;t-&gt;children, sibling) {
                unsigned int child_points;

                // 如果不共享mm
                if (child-&gt;mm == p-&gt;mm)
                    continue;
                /*
                 * oom_badness() returns 0 if the thread is unkillable
                 */
                // 计算child?om_badness得分
                child_points = oom_badness(child, memcg, nodemask,
                                    totalpages);
                // 如果child得分更高，则将被选中进程换成child
                if (child_points &gt; victim_points) {
                    put_task_struct(victim);
                    victim = child;
                    victim_points = child_points;
                    get_task_struct(victim);
                }
            }
        } while_each_thread(p, t);
        read_unlock(&amp;tasklist_lock);

        rcu_read_lock();
        /*
         * 遍历确认被选中进程的线程组，判断是否还存在task_struct-&gt;mm，如果不存在
         * (有可能这个时候进程退出了，或释放了mm),就没必要再kill了。
         * 如果存在则选择线程组中的进程。
         */
        p = find_lock_task_mm(victim);
        if (!p) {
            rcu_read_unlock();
            put_task_struct(victim);
            return;
        // 如果新选择的进程跟之前的不是同一个，那么更新victim。
        } else if (victim != p) {
            get_task_struct(p);
            put_task_struct(victim);
            victim = p;
        }

        /* mm cannot safely be dereferenced after task_unlock(victim) */
        mm = victim-&gt;mm;
        pr_err("Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB\n",
            task_pid_nr(victim), victim-&gt;comm, K(victim-&gt;mm-&gt;total_vm),
            K(get_mm_counter(victim-&gt;mm, MM_ANONPAGES)),
            K(get_mm_counter(victim-&gt;mm, MM_FILEPAGES)));
        task_unlock(victim);

        /*
         * Kill all user processes sharing victim-&gt;mm in other thread groups, if
         * any. They don't get access to memory reserves, though, to avoid
         * depletion of all memory. This prevents mm-&gt;mmap_sem livelock when an
         * oom killed thread cannot exit because it requires the semaphore and
         * its contended by another thread trying to allocate memory itself.
         * That thread will now get access to memory reserves since it has a
         * pending fatal signal.
         */
        /*
         * 遍历系统中的所有进程，寻找在其它线程组中，跟被选中进程(victim)共享mm结构
         * 的进程(内核线程除外)，共享mm结构即共享进程地址空间，比如fork后exec之前，
         * 父子进程是共享mm的，回收内存必须要将共享mm的所有进程都kill掉。
         */
        for_each_process(p)
            if (p-&gt;mm == mm &amp;&amp; !same_thread_group(p, victim) &amp;&amp;
             !(p-&gt;flags &amp; PF_KTHREAD)) {
                if (p-&gt;signal-&gt;oom_score_adj == OOM_SCORE_ADJ_MIN)
                    continue;

                // 进行task_struct相关操作时，通常需要获取该锁。
                task_lock(p);    /* Protect -&gt;comm from prctl() */
                pr_err("Kill process %d (%s) sharing same memory\n",
                    task_pid_nr(p), p-&gt;comm);
                task_unlock(p);
                // 通过向被选中的进程发送kill信号，来kill进程。
                do_send_sig_info(SIGKILL, SEND_SIG_FORCED, p, true);
            }
        rcu_read_unlock();

        // 进程设置TIF_MEMDIE标记，表示进程正在被oom killer终止中。
        set_tsk_thread_flag(victim, TIF_MEMDIE);
        /*
         * 最终通过向被选中的进程发送kill信号，来kill进程，被kill的进程在从内核态
         * 返回用户态时，进行信号处理。
         * 被选中的进程可以是自己(current)，则current进程会在oom流程执行完成后，返回
         * 用户态时，处理信号。
         */
        do_send_sig_info(SIGKILL, SEND_SIG_FORCED, victim, true);
        put_task_struct(victim);
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kernel 3.10内核源码分析--内核页表创建]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/30/kernel-mm-init/"/>
    <updated>2015-09-30T15:53:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/30/kernel-mm-init</id>
    <content type="html"><![CDATA[<p><a href="http://blog.chinaunix.net/uid-20671208-id-4440253.html">http://blog.chinaunix.net/uid-20671208-id-4440253.html</a></p>

<p>内核页表创建基本流程：
<code>
    start_kernel
        setup_arch
            init_mem_mapping
                init_range_memory_mapping
                    init_memory_mapping
                        kernel_physical_mapping_init  
</code></p>

<pre><code>    /*
      * 创建内核页表，将内核页表中能线性映射的部分(0-896M，还要刨去ISA等区域)
      * 进行映射，创建相应的页表项，在内核初始化的时候(setup_arch())完成。
      */
    unsigned long __init
    kernel_physical_mapping_init(unsigned long start,
                 unsigned long end,
                 unsigned long page_size_mask)
    {
        int use_pse = page_size_mask == (1&lt;&lt;PG_LEVEL_2M);
        unsigned long last_map_addr = end;
        unsigned long start_pfn, end_pfn;
         /*内核页表页目录所在的位置，其所占的内存是在head_32.S中预先分配好的*/
        pgd_t *pgd_base = swapper_pg_dir;
        int pgd_idx, pmd_idx, pte_ofs;
        unsigned long pfn;
        pgd_t *pgd;
        pmd_t *pmd;
        pte_t *pte;
        unsigned pages_2m, pages_4k;
        int mapping_iter;
        /*计算欲映射区域的起始和结束pfn*/
        start_pfn = start &gt;&gt; PAGE_SHIFT;
        end_pfn = end &gt;&gt; PAGE_SHIFT;

        /*
         * First iteration will setup identity mapping using large/small pages
         * based on use_pse, with other attributes same as set by
         * the early code in head_32.S
         *
         * Second iteration will setup the appropriate attributes (NX, GLOBAL..)
         * as desired for the kernel identity mapping.
         *
         * This two pass mechanism conforms to the TLB app note which says:
         *
         * "Software should not write to a paging-structure entry in a way
         * that would change, for any linear address, both the page size
         * and either the page frame or attributes."
         */
        mapping_iter = 1;

        if (!cpu_has_pse)
            use_pse = 0;

    repeat:
        pages_2m = pages_4k = 0;
        pfn = start_pfn;
        pgd_idx = pgd_index((pfn&lt;&lt;PAGE_SHIFT) + PAGE_OFFSET);
        /*
         * pgd、pmd等存放的是本级页表中对应index项的虚拟地址，页表项的内容中存放的是
         * 下一级页表的起始物理地址
         */
        pgd = pgd_base + pgd_idx;
        for (; pgd_idx &lt; PTRS_PER_PGD; pgd++, pgd_idx++) {
            //创建pmd，如果没有pmd，则返回pgd。实际通过get_free_page接口分配，此时buddy系统已经可用?
            pmd = one_md_table_init(pgd);

            if (pfn &gt;= end_pfn)
                continue;
    #ifdef CONFIG_X86_PAE
            pmd_idx = pmd_index((pfn&lt;&lt;PAGE_SHIFT) + PAGE_OFFSET);
            pmd += pmd_idx;
    #else
            pmd_idx = 0;
    #endif
            for (; pmd_idx &lt; PTRS_PER_PMD &amp;&amp; pfn &lt; end_pfn;
             pmd++, pmd_idx++) {
                /*
                 * 页框虚拟地址，就是物理地址(pfn * PAGE_SIZE)+固定偏移
                 * 这就是线性映射的实质。
                */
                unsigned int addr = pfn * PAGE_SIZE + PAGE_OFFSET;

                /*
                 * Map with big pages if possible, otherwise
                 * create normal page tables:
                 */
                if (use_pse) {
                    unsigned int addr2;
                    pgprot_t prot = PAGE_KERNEL_LARGE;
                    /*
                     * first pass will use the same initial
                     * identity mapping attribute + _PAGE_PSE.
                     */
                    pgprot_t init_prot =
                        __pgprot(PTE_IDENT_ATTR |
                            _PAGE_PSE);

                    pfn &amp;= PMD_MASK &gt;&gt; PAGE_SHIFT;
                    addr2 = (pfn + PTRS_PER_PTE-1) * PAGE_SIZE +
                        PAGE_OFFSET + PAGE_SIZE-1;

                    if (is_kernel_text(addr) ||
                     is_kernel_text(addr2))
                        prot = PAGE_KERNEL_LARGE_EXEC;

                    pages_2m++;
                    if (mapping_iter == 1)
                        set_pmd(pmd, pfn_pmd(pfn, init_prot));
                    else
                        set_pmd(pmd, pfn_pmd(pfn, prot));

                    pfn += PTRS_PER_PTE;
                    continue;
                }
                // 创建页表
                pte = one_page_table_init(pmd);

                pte_ofs = pte_index((pfn&lt;&lt;PAGE_SHIFT) + PAGE_OFFSET);
                pte += pte_ofs;
                // 填写每项页表的内容。
                for (; pte_ofs &lt; PTRS_PER_PTE &amp;&amp; pfn &lt; end_pfn;
                 pte++, pfn++, pte_ofs++, addr += PAGE_SIZE) {
                    pgprot_t prot = PAGE_KERNEL;
                    /*
                     * first pass will use the same initial
                     * identity mapping attribute.
                     */
                    pgprot_t init_prot = __pgprot(PTE_IDENT_ATTR);

                    if (is_kernel_text(addr))
                        prot = PAGE_KERNEL_EXEC;

                    pages_4k++;
                    if (mapping_iter == 1) {
                        // 将pfn(页框号)和相关属性转换为物理地址，然后写入pte中
                        set_pte(pte, pfn_pte(pfn, init_prot));
                        last_map_addr = (pfn &lt;&lt; PAGE_SHIFT) + PAGE_SIZE;
                    } else
                        set_pte(pte, pfn_pte(pfn, prot));
                }
            }
        }
        if (mapping_iter == 1) {
            /*
             * update direct mapping page count only in the first
             * iteration.
             */
            update_page_count(PG_LEVEL_2M, pages_2m);
            update_page_count(PG_LEVEL_4K, pages_4k);

            /*
             * local global flush tlb, which will flush the previous
             * mappings present in both small and large page TLB's.
             */
            __flush_tlb_all();

            /*
             * Second iteration will set the actual desired PTE attributes.
             */
            mapping_iter = 2;
            goto repeat;
        }
        return last_map_addr;
</code></pre>

<p>swapper_pg_dir为内核页表页目录所在的位置，其所占的内存是在head_32.S中预先分配好的，从下面的汇编代码看，预先分配了1024*4=4k的空间，可以容纳1024个entry。</p>

<pre><code>    ENTRY(swapper_pg_dir)
        .fill 1024,4,0
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP的定时器系列 — 保活定时器]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/30/kernel-net-keepalive/"/>
    <updated>2015-09-30T15:32:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/30/kernel-net-keepalive</id>
    <content type="html"><![CDATA[<p><a href="http://blog.csdn.net/zhangskd/article/details/44177475">http://blog.csdn.net/zhangskd/article/details/44177475</a></p>

<p>主要内容：保活定时器的实现，TCP_USER_TIMEOUT选项的实现。<br/>
内核版本：3.15.2</p>

<h4>原理</h4>

<p>HTTP有Keepalive功能，TCP也有Keepalive功能，虽然都叫Keepalive，但是它们的目的却是不一样的。为了说明这一点，先来看下长连接和短连接的定义。</p>

<p>连接的“长短”是什么？<br/>
短连接：建立一条连接，传输一个请求，马上关闭连接。<br/>
长连接：建立一条连接，传输一个请求，过会儿，又传输若干个请求，最后再关闭连接。</p>

<p>长连接的好处是显而易见的，多个请求可以复用一条连接，省去连接建立和释放的时间开销和系统调用，但也意味着服务器的一部分资源会被长时间占用着。</p>

<p>HTTP的Keepalive，顾名思义，目的在于延长连接的时间，以便在同一条连接中传输多个HTTP请求。</p>

<p>HTTP服务器一般会提供Keepalive Timeout参数，用来决定连接保持多久，什么时候关闭连接。</p>

<p>当连接使用了Keepalive功能时，对于客户端发送过来的一个请求，服务器端会发送一个响应，然后开始计时，如果经过Timeout时间后，客户端没有再发送请求过来，服务器端就把连接关了，不再保持连接了。</p>

<p>TCP的Keepalive，是挂羊头卖狗肉的，目的在于看看对方有没有发生异常，如果有异常就及时关闭连接。</p>

<p>当传输双方不主动关闭连接时，就算双方没有交换任何数据，连接也是一直有效的。</p>

<p>如果这个时候对端、中间网络出现异常而导致连接不可用，本端如何得知这一信息呢？</p>

<p>答案就是保活定时器。它每隔一段时间会超时，超时后会检查连接是否空闲太久了，如果空闲的时间超过了设置时间，就会发送探测报文。然后通过对端是否响应、响应是否符合预期，来判断对端是否正常，如果不正常，就主动关闭连接，而不用等待HTTP层的关闭了。</p>

<p>当服务器发送探测报文时，客户端可能处于4种不同的情况：仍然正常运行、已经崩溃、已经崩溃并重启了、由于中间链路问题不可达。在不同的情况下，服务器会得到不一样的反馈。</p>

<p>(1) 客户主机依然正常运行，并且从服务器端可达</p>

<p>客户端的TCP响应正常，从而服务器端知道对方是正常的。保活定时器会在两小时以后继续触发。</p>

<p>(2) 客户主机已经崩溃，并且关闭或者正在重新启动</p>

<p>客户端的TCP没有响应，服务器没有收到对探测包的响应，此后每隔75s发送探测报文，一共发送9次。</p>

<p>socket函数会返回-1，errno设置为ETIMEDOUT，表示连接超时。</p>

<p>(3) 客户主机已经崩溃，并且重新启动了</p>

<p>客户端的TCP发送RST，服务器端收到后关闭此连接。</p>

<p>socket函数会返回-1，errno设置为ECONNRESET，表示连接被对端复位了。</p>

<p>(4) 客户主机依然正常运行，但是从服务器不可达</p>

<p>双方的反应和第二种是一样的，因为服务器不能区分对端异常与中间链路异常。</p>

<p>socket函数会返回-1，errno设置为EHOSTUNREACH，表示对端不可达。</p>

<h4>选项</h4>

<p>内核默认并不使用TCP Keepalive功能，除非用户设置了SO_KEEPALIVE选项。</p>

<p>有两种方式可以自行调整保活定时器的参数：一种是修改TCP参数，一种是使用TCP层选项。</p>

<p>(1) TCP参数</p>

<p>tcp_keepalive_time</p>

<p>最后一次数据交换到TCP发送第一个保活探测报文的时间，即允许连接空闲的时间，默认为7200s。</p>

<p>tcp_keepalive_intvl</p>

<p>保活探测报文的重传时间，默认为75s。</p>

<p>tcp_keepalive_probes</p>

<p>保活探测报文的发送次数，默认为9次。</p>

<p>Q：一次完整的保活探测需要花费多长时间？</p>

<p>A：tcp_keepalive_time + tcp_keepalive_intvl * tcp_keepalive_probes，默认值为7875s。如果觉得两个多小时太长了，可以自行调整上述参数。</p>

<p>(2) TCP层选项</p>

<p>TCP_KEEPIDLE：含义同tcp_keepalive_time。</p>

<p>TCP_KEEPINTVL：含义同tcp_keepalive_intvl。</p>

<p>TCP_KEEPCNT：含义同tcp_keepalive_probes。</p>

<p>Q：既然有了TCP参数可供调整，为什么还增加了上述的TCP层选项？</p>

<p>A：TCP参数是面向本机的所有TCP连接，一旦调整了，对所有的连接都有效。而TCP层选项是面向一条连接的，一旦调整了，只对本条连接有效。</p>

<h4>激活</h4>

<p>在连接建立后，可以通过设置SO_KEEPALIVE选项，来激活保活定时器。</p>

<pre><code>    int keepalive = 1;
    setsockopt(fd, SOL_SOCKET, SO_KEEPALIVE, &amp;keepalive, sizeof(keepalive));
</code></pre>

<pre><code>    int sock_setsockopt(struct socket *sock, int level, int optname, char __user *optval,   
        unsigned int optlen)  
    {  
        ...  
        case SO_KEEPALIVE:  
    #ifdef CONFIG_INET  
            if (sk-&gt;sk_protocol == IPPROTO_TCP &amp;&amp; sk-&gt;sk_type == SOCK_STREAM)  
                tcp_set_keepalive(sk, valbool); /* 激活或删除保活定时器 */  
    #endif  
            sock_valbool_flag(sk, SOCK_KEEPOPEN, valbool); /* 设置或取消SOCK_KEEPOPEN标志位 */  
            break;  
        ...  
    }  

    static inline void sock_valbool_flag (struct sock *sk, int bit, int valbool)  
    {  
        if (valbool)  
            sock_set_flag(sk, bit);  
        else  
            sock_reset_flag(sk, bit);  
    }  
</code></pre>

<pre><code>    void tcp_set_keepalive(struct sock *sk, int val)  
    {  
        /* 不在以下两个状态设置保活定时器： 
         * TCP_CLOSE：sk_timer用作FIN_WAIT2定时器 
         * TCP_LISTEN：sk_timer用作SYNACK重传定时器 
         */  
        if ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN))  
            return;  

        /* 如果SO_KEEPALIVE选项值为1，且此前没有设置SOCK_KEEPOPEN标志， 
         * 则激活sk_timer，用作保活定时器。 
         */  
        if (val &amp;&amp; !sock_flag(sk, SOCK_KEEPOPEN))  
            inet_csk_reset_keepalive_timer(sk, keepalive_time_when(tcp_sk(sk)));  
        else if (!val)  
            /* 如果SO_KEEPALIVE选项值为0，则删除保活定时器 */  
            inet_csk_delete_keepalive_timer(sk);  
    }  

    /* 保活定时器的超时时间 */  
    static inline int keepalive_time_when(const struct tcp_sock *tp)  
    {  
        return tp-&gt;keepalive_time ? : sysctl_tcp_keepalive_time;  
    }  

    void inet_csk_reset_keepalive_timer (struc sock *sk, unsigned long len)  
    {  
        sk_reset_timer(sk, &amp;sk-&gt;sk_timer, jiffies + len);  
    }  
</code></pre>

<p>可以使用TCP层选项来动态调整保活定时器的参数。</p>

<pre><code>    int keepidle = 600;
    int keepintvl = 10;
    int keepcnt = 6;

    setsockopt(fd, SOL_TCP, TCP_KEEPIDLE, &amp;keepidle, sizeof(keepidle));
    setsockopt(fd, SOL_TCP, TCP_KEEPINTVL, &amp;keepintvl, sizeof(keepintvl));
    setsockopt(fd, SOL_TCP, TCP_KEEPCNT, &amp;keepcnt, sizeof(keepcnt));
</code></pre>

<pre><code>    struct tcp_sock {  
        ...  
        /* 最后一次接收到ACK的时间 */  
        u32 rcv_tstamp; /* timestamp of last received ACK (for keepalives) */  
        ...  
        /* time before keep alive takes place, 空闲多久后才发送探测报文 */  
        unsigned int keepalive_time;  
        /* time iterval between keep alive probes */  
        unsigned int keepalive_intvl; /* 探测报文之间的时间间隔 */  
        /* num of allowed keep alive probes */  
        u8 keepalive_probes; /* 探测报文的发送次数 */  
        ...  
        struct {  
            ...  
            /* 最后一次接收到带负荷的报文的时间 */  
            __u32 lrcvtime; /* timestamp of last received data packet */  
            ...  
        } icsk_ack;  
        ...  
    };  

    #define TCP_KEEPIDLE 4 /* Start Keepalives after this period */  
    #define TCP_KEEPINTVL 5 /* Interval between keepalives */  
    #define TCP_KEEPCNT 6 /* Number of keepalives before death */  

    #define MAX_TCP_KEEPIDLE 32767  
    #define MAX_TCP_KEEPINTVL 32767  
    #define MAX_TCP_KEEPCNT 127  
</code></pre>

<pre><code>    static int do_tcp_setsockopt(struct sock *sk, int level, int optname, char __user *optval,  
        unsigned int optlen)  
    {  
        ...  
        case TCP_KEEPIDLE:  
           if (val &lt; 1 || val &gt; MAX_TCP_KEEPIDLE)  
               err = -EINVAL;  
            else {  
                tp-&gt;keepalive_time = val * HZ; /* 设置新的空闲时间 */  

                /* 如果有使用SO_KEEPALIVE选项，连接处于非监听非结束的状态。 
                 * 这个时候保活定时器已经在计时了，这里设置新的超时时间。 
                 */  
                if (sock_flag(sk, SOCK_KEEPOPEN) &amp;&amp;   
                    !((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN))) {  
                    u32 elapsed = keepalive_time_elapsed(tp); /* 连接已经经历的空闲时间 */  

                    if (tp-&gt;keepalive_time &gt; elapsed)  
                        elapsed = tp-&gt;keepalive_time - elapsed; /* 接着等待的时间，然后超时 */  
                    else  
                        elapsed = 0; /* 会导致马上超时 */  
                    inet_csk_reset_keepalive_timer(sk, elapsed);  
                }  
            }  
            break;  

        case TCP_KEEPINTVL:  
            if (val &lt; 1 || val &gt; MAX_TCP_KEEPINTVL)  
                err = -EINVAL;  
            else  
                tp-&gt;keepalive_intvl = val * HZ; /* 设置新的探测报文间隔 */  
            break;  

        case TCP_KEEPCNT:  
            if (val &lt; 1 || val &gt; MAX_TCP_KEEPCNT)  
                err = -EINVAL;  
            else  
                tp-&gt;keepalive_probes = val; /* 设置新的探测次数 */  
            break;  
        ...  
    }  
</code></pre>

<p>到目前为止，连接已经经历的空闲时间，即最后一次接收到报文至今的时间。</p>

<pre><code>    static inline u32 keepalive_time_elapsed (const struct tcp_sock *tp)  
    {  
        const struct inet_connection_sock *icsk = &amp;tp-&gt;inet_conn;  

        /* lrcvtime是最后一次接收到数据报的时间 
         * rcv_tstamp是最后一次接收到ACK的时间 
         * 返回值就是最后一次接收到报文，到现在的时间，即经历的空闲时间。 
         */  
        return min_t(u32, tcp_time_stamp - icsk-&gt;icsk_ack.lrcvtime,  
            tcp_time_stamp - tp-&gt;rcv_tstamp);  
    }  
</code></pre>

<h4>超时处理函数</h4>

<p>我们知道保活定时器、SYNACK重传定时器、FIN_WAIT2定时器是共用一个定时器实例sk->sk_timer，所以它们的超时处理函数也是一样的，都为tcp_keepalive_timer()。而在函数内部，可以根据此时连接所处的状态，来判断是哪个定时器触发了超时。</p>

<p>Q：什么时候判断对端为异常并关闭连接？</p>

<p>A：分两种情况。</p>

<ol>
<li><p>用户使用了TCP_USER_TIMEOUT选项。当连接的空闲时间超过了用户设置的时间，且有发送过探测报文。</p></li>
<li><p>用户没有使用TCP_USER_TIMEOUT选项。当发送保活探测包的次数达到了保活探测的最大次数时。</p></li>
</ol>


<pre><code>    static void tcp_keepalive_timer (unsigned long data)  
    {  
        struct sock *sk = (struct sock *) data;  
        struct inet_connection_sock *icsk = inet_csk(sk);  
        struct tcp_sock *tp = tcp_sk(sk);  
        u32 elapsed;  

        /* Only process if socket is not in use. */  
        bh_lock_sock(sk);  

        /* 加锁以保证在此期间，连接状态不会被用户进程修改。 
         * 如果用户进程正在使用此sock，那么过50ms再来看看。 
         */  
        if (sock_owned_by_user(sk)) {  
            /* Try again later. */  
            inet_csk_reset_keepalive_timer(sk, HZ/20);  
            goto out;  
        }  

        /* 三次握手期间，用作SYNACK定时器 */  
        if (sk-&gt;sk_state == TCP_LISTEN) {  
            tcp_synack_timer(sk);  
            goto out;  
        }      

        /* 连接释放期间，用作FIN_WAIT2定时器 */  
        if (sk-&gt;sk_state == TCP_FIN_WAIT2 &amp;&amp; sock_flag(sk, SOCK_DEAD)) {  
            ...  
        }  

        /* 接下来就是用作保活定时器了 */  
        if (!sock_flag(sk, SOCK_KEEPOPEN) || sk-&gt;sk_state == TCP_CLOSE)  
            goto out;  

        elapsed = keepalive_time_when(tp); /* 连接的空闲时间超过此值，就发送保活探测报文 */  

        /* It is alive without keepalive. 
         * 如果网络中有发送且未确认的数据包，或者发送队列不为空，说明连接不是idle的？ 
         * 既然连接不是idle的，就没有必要探测对端是否正常。 
         * 保活定时器重新开始计时即可。 
         *  
         * 而实际上当网络中有发送且未确认的数据包时，对端也可能会发生异常而没有响应。 
         * 这个时候会导致数据包的不断重传，只能依靠重传超过了允许的最大时间，来判断连接超时。 
         * 为了解决这一问题，引入了TCP_USER_TIMEOUT，允许用户指定超时时间，可见下文：） 
         */  
        if (tp-&gt;packets_out || tcp_send_head(sk))  
            goto resched; /* 保活定时器重新开始计时 */  

        /* 连接经历的空闲时间，即上次收到报文至今的时间 */  
        elapsed = keepalive_time_elapsed(tp);  

        /* 如果连接空闲的时间超过了设置的时间值 */  
        if (elapsed &gt;= keepalive_time_when(tp)) {  

            /* 什么时候关闭连接？ 
             * 1. 使用了TCP_USER_TIMEOUT选项。当连接空闲时间超过了用户设置的时间，且有发送过探测报文。 
             * 2. 用户没有使用选项。当发送的保活探测包达到了保活探测的最大次数。 
             */  
            if (icsk-&gt;icsk_user_timeout != 0 &amp;&amp; elapsed &gt;= icsk-&gt;icsk_user_timeout &amp;&amp;  
                icsk-&gt;icsk_probes_out &gt; 0) || (icsk-&gt;icsk_user_timeout == 0 &amp;&amp;  
                icsk-&gt;icsk_probes_out &gt;= keepalive_probes(tp))) {  
                tcp_send_active_reset(sk, GFP_ATOMIC); /* 构造一个RST包并发送 */  
                tcp_write_err(sk); /* 报告错误，关闭连接 */  
                goto out;  
            }  

            /* 如果还不到关闭连接的时候，就继续发送保活探测包 */  
            if (tcp_write_wakeup(sk) &lt;= 0) {  
                icsk-&gt;icsk_probes_out++; /* 已发送的保活探测包个数 */  
                elapsed = keepalive_intvl_when(tp); /* 下次超时的时间，默认为75s */  
            } else {  
                /* If keepalive was lost due to local congestion, try harder. */  
                elapsd = TCP_RESOURCE_PROBE_INTERVAL; /* 默认为500ms，会使超时更加频繁 */  
            }  

        } else {  
            /* 如果连接的空闲时间，还没有超过设定值，则接着等待 */  
            elapsed = keepalive_time_when(tp) - elapsed;  
        }   

        sk_mem_reclaim(sk);  

    resched: /* 重设保活定时器 */  
        inet_csk_reset_keepalive_timer(sk, elapsed);  
        goto out;   

    out:  
        bh_unlock_sock(sk);  
        sock_put(sk);  
    }  
</code></pre>

<p>Q：TCP是如何发送Keepalive探测报文的？</p>

<p>A：分两种情况。</p>

<ol>
<li><p>有新的数据段可供发送，且对端接收窗口还没被塞满。发送新的数据段，来作为探测包。</p></li>
<li><p>没有新的数据段可供发送，或者对端的接收窗口满了。发送序号为snd_una - 1、长度为0的ACK包作为探测包。</p></li>
</ol>


<pre><code>    /* Initiate keepalive or window probe from timer. */  

    int tcp_write_wakeup (struct sock *sk)  
    {  
        struct tcp_sock *tp = tcp_sk(sk);  
        struct sk_buff *skb;  

        if (sk-&gt;sk_state == TCP_CLOSE)  
            return -1;  

        /* 如果还有未发送过的数据包，并且对端的接收窗口还没有满 */  
        if ((skb = tcp_send_head(sk)) != NULL &amp;&amp; before(TCP_SKB_CB(skb)-&gt;seq, tcp_wnd_end(tp))) {  
            int err;  
            unsigned int mss = tcp_current_mss(sk); /* 当前的MSS */  
            /* 对端接收窗口所允许的最大报文长度 */  
            unsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq;  

            /* pushed_seq记录发送出去的最后一个字节的序号 */  
            if (before(tp-&gt;pushed_seq, TCP_SKB_CB(skb)-&gt;end_seq))  
                tp-&gt;pushed_seq = TCP_SKB_CB(skb)-&gt;end_seq;  

            /* 如果对端接收窗口小于此数据段的长度，或者此数据段的长度超过了MSS，那么就要进行分段 */  
            if (seg_size &lt; TCP_SKB_CB(skb)-&gt;end_seq - TCP_SKB_CB(skb)-&gt;seq || skb-&gt;len &gt; mss) {  
                seg_size = min(seg_size, mss);  
                TCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH; /* 设置PSH标志，让对端马上把数据提交给程序 */  
                if (tcp_fragment(sk, skb, seg_size, mss)) /* 进行分段 */  
                    return -1;  
            } else if (! tcp_skb_pcount(skb)) /* 进行TSO分片 */  
                tcp_set_skb_tso_segs(sk, skb, mss); /* 初始化分片相关变量 */  

            TCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;  
            TCP_SKB_CB(skb)-&gt;when = tcp_time_stamp;  
            err = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC); /* 发送此数据段 */  
            if (!err)  
                tcp_event_new_data_sent(sk, skb); /* 发送了新的数据，更新相关参数 */  

        } else { /* 如果没有新的数据段可用作探测报文发送，或者对端的接收窗口为0 */  

           /* 处于紧急模式时，额外发送一个序号为snd_una的ACK包，告诉对端紧急指针 */  
           if (between(tp-&gt;snd_up, tp-&gt;snd_una + 1, tp-&gt;snd_una + 0xFFFF))  
               tcp_xmit_probe_skb(sk, 1);  

            /* 发送一个序号为snd_una -1的ACK包，长度为0，这是一个序号过时的报文。 
             * snd_una: first byte we want an ack for，所以snd_una - 1序号的字节已经被确认过了。 
             * 对端会响应一个ACK。 
             */  
            return tcp_xmit_probe_skb(sk, 0);  
        }  
    }  
</code></pre>

<p>Q：当没有新的数据可以用作探测包、或者对端的接收窗口为0时，怎么办呢？</p>

<p>A：发送一个序号为snd_una - 1、长度为0的ACK包，对端收到此包后会发送一个ACK响应。如此一来本端就能够知道对端是否还活着、接收窗口是否打开了。</p>

<pre><code>    /* This routine sends a packet with an out of date sequence number. 
     * It assumes the other end will try to ack it. 
     *  
     * Question: what should we make while urgent mode? 
     * 4.4BSD forces sending single byte of data. We cannot send out of window 
     * data, because we have SND.NXT == SND.MAX... 
     *  
     * Current solution: to send TWO zero-length segments in urgent mode: 
     * one is with SEG.SEG=SND.UNA to deliver urgent pointer, another is out-of-date with 
     * SND.UNA - 1 to probe window. 
     */  

    static int tcp_xmit_probe_skb (struct sock *sk, int urgent)  
    {  
        struct tcp_sock *tp = tcp_sk(sk);  
        struct sk_buff *skb;  

        /* We don't queue it, tcp_transmit_skb() sets ownership. */  
        skb = alloc_skb(MAX_TCP_HEADER, sk_gfp_atomic(sk, GFP_ATOMIC));  
        if (skb == NULL)  
            return -1;  

        /* Reserve space for headers and set control bits. */  
        skb_reserve(skb, MAX_TCP_HEADER);  

        /* Use a previous sequence. This should cause the other end to send an ack. 
         * Don't queue or clone SKB, just send it. 
         */  
        /* 如果没有设置紧急指针，那么发送的序号为snd_una - 1，否则发送的序号为snd_una */  
        tcp_init_nondata_skb(skb, tp-&gt;snd_una - !urgent, TCPHDR_ACK);  
        TCP_SKB_CB(skb)-&gt;when = tcp_time_stamp;  
        return tcp_transmit_skb(sk, skb, 0, GFP_ATOMIC); /* 发送探测包 */  
    }  
</code></pre>

<p>发送RST包。</p>

<pre><code>    /* We get here when a process closes a file descriptor (either due to an explicit close() 
     * or as a byproduct of exit()'ing) and there was unread data in the receive queue. 
     * This behavior is recommended by RFC 2525, section 2.17. -DaveM 
     */  

    void tcp_send_active_reset (struct sock *sk, gfp_t priority)  
    {  
        struct sk_buff *skb;  
        /* NOTE: No TCP options attached and we never retransmit this. */  
        skb = alloc_skb(MAX_TCP_HEADER, priority);  
        if (!skb) {  
            NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);  
            return;  
        }  

        /* Reserve space for headers and prepare control bits. */  
        skb_reserve(skb, MAX_TCP_HEADER); /* 为报文头部预留空间 */  
        /* 初始化不携带数据的skb的一些控制字段 */  
        tcp_init_nondata_skb(skb, tcp_acceptable_seq(sk), TCPHDR_ACK | TCPHDR_RST);  

        /* Send if off，发送此RST包*/  
        TCP_SKB_CB(skb)-&gt;when = tcp_time_stamp;  
        if (tcp_transmit_skb(sk, skb, 0, priority))  
            NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);  
        TCP_INC_STATS(sock_net(sk), TCP_MIB_OUTRSTS);  
    }  

    static inline __u32 tcp_acceptable_seq (const struct sock *sk)  
    {  
        const struct tcp_sock *tp = tcp_sk(sk);  

        /* 如果snd_nxt在对端接收窗口范围内 */  
        if (! before(tcp_wnd_end(tp), tp-&gt;snd_nxt))  
            return tp-&gt;snd_nxt;  
        else  
            return tcp_wnd_end(tp);  
    }  
</code></pre>

<h4>TCP_USER_TIMEOUT选项</h4>

<p>从上文可知同时符合以下条件时，保活定时器才会发送探测报文：</p>

<ol>
<li><p>网络中没有发送且未确认的数据包。</p></li>
<li><p>发送队列为空。</p></li>
<li><p>连接的空闲时间超过了设定的时间。</p></li>
</ol>


<p>Q：如果网络中有发送且未确认的数据包、或者发送队列不为空时，保活定时器不起作用了，岂不是不能够检测到对端的异常了？</p>

<p>A：可以使用TCP_USER_TIMEOUT，显式的指定当发送数据多久后还没有得到响应，就判定连接超时，从而主动关闭连接。</p>

<p>TCP_USER_TIMEOUT选项会影响到超时重传定时器和保活定时器。</p>

<p>(1) 超时重传定时器</p>

<p>判断连接是否超时，分3种情况：</p>

<ol>
<li><p>SYN包：当SYN包的重传次数达到上限时，判定连接超时。(默认允许重传5次，初始超时时间为1s，总共历时31s)</p></li>
<li><p>非SYN包，用户使用TCP_USER_TIMEOUT：当数据包发出去后的等待时间超过用户设置的时间时，判定连接超时。</p></li>
<li><p>非SYN包，用户没有使用TCP_USER_TIMEOUT：当数据包发出去后的等待时间超过以TCP_RTO_MIN为初始超时时间，重传boundary次所花费的时间后，判定连接超时。(boundary的最大值为tcp_retries2，默认值为15)</p></li>
</ol>


<p>(2) 保活定时器</p>

<p>判断连接是否异常，分2种情况：</p>

<ol>
<li><p>用户使用了TCP_USER_TIMEOUT选项。当连接的空闲时间超过了用户设置的时间，且有发送过探测报文。</p></li>
<li><p>用户没有使用TCP_USER_TIMEOUT选项。当发送保活探测包的次数达到了保活探测的最大次数时。</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TIME_WAIT状态下对接收到的数据包如何处理]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/29/kernel-net-timewait_state/"/>
    <updated>2015-09-29T17:53:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/29/kernel-net-timewait_state</id>
    <content type="html"><![CDATA[<p><a href="http://www.educity.cn/linux/1605134.html">http://www.educity.cn/linux/1605134.html</a></p>

<p>正常情况下主动关闭连接的一端在连接正常终止后，会进入TIME_WAIT状态，存在这个状态有以下两个原因（参考《Unix网络编程》）：</p>

<p>《UNIX网络编程.卷2：进程间通信(第2版)》[PDF]下载</p>

<p>1、保证TCP连接关闭的可靠性。如果最终发送的ACK丢失，被动关闭的一端会重传最终的FIN包，如果执行主动关闭的一端没有维护这个连接的状态信息，会发送RST包响应，导致连接不正常关闭。</p>

<p>2、允许老的重复分组在网络中消逝。假设在一个连接关闭后，发起建立连接的一端（客户端）立即重用原来的端口、IP地址和服务端建立新的连接。老的连接上的分组可能在新的连接建立后到达服务端，TCP必须防止来自某个连接的老的重复分组在连接终止后再现，从而被误解为同一个连接的化身。要实现这种功能，TCP不能给处于TIME_WAIT状态的连接启动新的连接。TIME_WAIT的持续时间是2MSL，保证在建立新的连接之前老的重复分组在网络中消逝。这个规则有一个例外：如果到达的SYN的序列号大于前一个连接的结束序列号，源自Berkeley的实现将给当前处于TIME_WAIT状态的连接启动新的化身。</p>

<p>最初在看《Unix网络编程》 的时候看到这个状态，但是在项目中发现对这个状态的理解有误，特别是第二个理由。原本认为在TIME_WAIT状态下肯定不会再使用相同的五元组（协议类型，源目的IP、源目的端口号）建立一个新的连接，看书还是不认真啊！为了加深理解，决定结合内核代码，好好来看下内核在TIME_WAIT状态下的处理。其实TIME_WAIT存在的第二个原因的解释更多的是从被动关闭一方的角度来说明的。如果是执行主动关闭的是客户端，客户端户进入TIME_WAIT状态，假设客户端重用端口号来和服务器建立连接，内核会不会允许客户端来建立连接？内核如何来处理这种情况？书本中不会对这些点讲的那么详细，要从内核源码中来找答案。</p>

<p>我们先来看服务器段进入TIME_WAIT后内核的处理，即服务器主动关闭连接。TCP层的接收函数是tcp_v4_rcv()，和TIME_WAIT状态相关的主要代码如下所示：</p>

<pre><code>    int tcp_v4_rcv(struct sk_buff *skb)
    {
        ......

        sk = __inet_lookup_skb(&amp;tcp_hashinfo, skb, th-&gt;source, th-&gt;dest);
        if (!sk)
            goto no_tcp_socket;
    process:
        if (sk-&gt;sk_state == TCP_TIME_WAIT)
            goto do_time_wait;   
            ......

    discard_it:
        /* Discard frame. */
        kfree_skb(skb);
        return 0;
        ......
    do_time_wait:
        ......

    switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {
        case TCP_TW_SYN: {
            struct sock *sk2 = inet_lookup_listener(dev_net(skb-&gt;dev),
                                &amp;tcp_hashinfo,
                                iph-&gt;daddr, th-&gt;dest,
                                inet_iif(skb));
            if (sk2) {
                inet_twsk_deschedule(inet_twsk(sk), &amp;tcp_death_row);
                inet_twsk_put(inet_twsk(sk));
                sk = sk2;
                goto process;
            }
            /* Fall through to ACK */
        }
        case TCP_TW_ACK:
            tcp_v4_timewait_ack(sk, skb);
            break;
        case TCP_TW_RST:
            goto no_tcp_socket;
        case TCP_TW_SUCCESS:;
        }
        goto discard_it;
    }
</code></pre>

<p>接收到SKb包后，会调用__inet_lookup_skb()查找对应的sock结构。如果套接字状态是TIME_WAIT状态，会跳转到do_time_wait标签处处理。从代码中可以看到，主要由tcp_timewait_state_process()函数来处理SKB包，处理后根据返回值来做相应的处理。</p>

<p>在看tcp_timewait_state_process()函数中的处理之前，需要先看一看不同的返回值会对应什么样的处理。</p>

<p>如果返回值是TCP_TW_SYN，则说明接收到的是一个“合法”的SYN包（也就是说这个SYN包可以接受），这时会首先查找内核中是否有对应的监听套接字，如果存在相应的监听套接字，则会释放TIME_WAIT状态的传输控制结构，跳转到process处开始处理，开始建立一个新的连接。如果没有找到监听套接字会执行到TCP_TW_ACK分支。</p>

<p>如果返回值是TCP_TW_ACK，则会调用tcp_v4_timewait_ack()发送ACK，然后跳转到discard_it标签处，丢掉数据包。</p>

<p>如果返回值是TCP_TW_RST，则会调用tcp_v4_send_reset()给对端发送RST包，然后丢掉数据包。</p>

<p>如果返回值是TCP_TW_SUCCESS，则会直接丢掉数据包。</p>

<p>接下来我们通过tcp_timewait_state_process()函数来看TIME_WAIT状态下的数据包处理。</p>

<p>为了方便讨论，假设数据包中没有时间戳选项，在这个前提下，tcp_timewait_state_process()中的局部变量paws_reject的值为0。</p>

<p>如果需要保持在FIN_WAIT_2状态的时间小于等于TCP_TIMEWAIT_LEN，则会从FIN_WAIT_2状态直接迁移到TIME_WAIT状态，也就是使用描述TIME_WAIT状态的sock结构代替当前的传输控制块。虽然这时的sock结构处于TIME_WAIT结构，但是还要区分内部状态，这个内部状态存储在inet_timewait_sock结构的tw_substate成员中。</p>

<p>如果内部状态为FIN_WAIT_2，tcp_timewait_state_process()中处理的关键代码片段如下所示：</p>

<pre><code>    if (tw-&gt;tw_substate == TCP_FIN_WAIT2) {
        /* Just repeat all the checks of tcp_rcv_state_process() */

        /* Out of window, send ACK */
        if (paws_reject ||
            !tcp_in_window(TCP_SKB_CB(skb)-&gt;seq, TCP_SKB_CB(skb)-&gt;end_seq,
                  tcptw-&gt;tw_rcv_nxt,
                  tcptw-&gt;tw_rcv_nxt + tcptw-&gt;tw_rcv_wnd))
            return TCP_TW_ACK;

        if (th-&gt;rst)
            goto kill;

        if (th-&gt;syn &amp;&amp; !before(TCP_SKB_CB(skb)-&gt;seq, tcptw-&gt;tw_rcv_nxt))
            goto kill_with_rst;

        /* Dup ACK? */
        if (!th-&gt;ack ||
            !after(TCP_SKB_CB(skb)-&gt;end_seq, tcptw-&gt;tw_rcv_nxt) ||
            TCP_SKB_CB(skb)-&gt;end_seq == TCP_SKB_CB(skb)-&gt;seq) {
            inet_twsk_put(tw);
            return TCP_TW_SUCCESS;
        }

        /* New data or FIN. If new data arrive after half-duplex close,
         * reset.
         */
        if (!th-&gt;fin ||
            TCP_SKB_CB(skb)-&gt;end_seq != tcptw-&gt;tw_rcv_nxt + 1) {
    kill_with_rst:
            inet_twsk_deschedule(tw, &amp;tcp_death_row);
            inet_twsk_put(tw);
            return TCP_TW_RST;
        }

        /* FIN arrived, enter true time-wait state. */
        tw-&gt;tw_substate      = TCP_TIME_WAIT;
        tcptw-&gt;tw_rcv_nxt = TCP_SKB_CB(skb)-&gt;end_seq;
        if (tmp_opt.saw_tstamp) {
            tcptw-&gt;tw_ts_recent_stamp = get_seconds();
            tcptw-&gt;tw_ts_recent      = tmp_opt.rcv_tsval;
        }

        /* I am shamed, but failed to make it more elegant.
         * Yes, it is direct reference to IP, which is impossible
         * to generalize to IPv6. Taking into account that IPv6
         * do not understand recycling in any case, it not
         * a big problem in practice. --ANK 
         */
        if (tw-&gt;tw_family == AF_INET &amp;&amp;
            tcp_death_row.sysctl_tw_recycle &amp;&amp; tcptw-&gt;tw_ts_recent_stamp &amp;&amp;
            tcp_v4_tw_remember_stamp(tw))
            inet_twsk_schedule(tw, &amp;tcp_death_row, tw-&gt;tw_timeout,
                      TCP_TIMEWAIT_LEN);
        else
            inet_twsk_schedule(tw, &amp;tcp_death_row, TCP_TIMEWAIT_LEN,
                      TCP_TIMEWAIT_LEN);

        return TCP_TW_ACK;
    }
</code></pre>

<p>如果TCP段序号不完全在接收窗口内，则返回TCP_TW_ACK，表示需要给对端发送ACK。</p>

<p>如果在FIN_WAIT_2状态下接收到的是RST包，则跳转到kill标签处处理，立即释放timewait控制块，并返回TCP_TW_SUCCESS。</p>

<p>如果是SYN包，但是SYN包的序列号在要接收的序列号之前，则表示这是一个过期的SYN包，则跳转到kill_with_rst标签处处理，此时不仅会释放TIME_WAIT传输控制块，还会返回TCP_TW_RST，要给对端发送RST包。</p>

<p>如果接收到DACK，则释放timewait控制块，并返回TCP_TW_SUCCESS。在这种情况下有一个判断条件是看包的结束序列号和起始序列号相同时，会作为DACK处理，所以之后的处理是在数据包中的数据不为空的情况下处理。前面的处理中已经处理了SYN包、RST包的情况，接下来就剩以下三种情况：</p>

<p>1、不带FIN标志的数据包</p>

<p>2、带FIN标志，但是还包含数据</p>

<p>3、FIN包，不包含数据</p>

<p>如果是前两种情况，则会调用inet_twsk_deschedule()释放time_wait控制块。inet_twsk_deschedule()中会调用到inet_twsk_put()减少time_wait控制块的引用，在外层函数中再次调用inet_twsk_put()函数时，就会真正释放time_wait控制块。</p>

<p>如果接收的是对端的FIN包，即第3种情况，则将time_wait控制块的子状态设置为TCP_TIME_WAIT，此时才是进入真正的TIME_WAIT状态。然后根据TIME_WAIT的持续时间的长短来确定是加入到twcal_row队列还是启动一个定时器，最后会返回TCP_TW_ACK，给对端发送TCP连接关闭时最后的ACK包。</p>

<p>到这里，我们看到了对FIN_WAIT_2状态（传输控制块状态为TIME_WAIT状态下，但是子状态为FIN_WAIT_2）的完整处理。</p>

<p>接下来的处理才是对真正的TIME_WAIT状态的处理，即子状态也是TIME_WAIT。</p>

<p>如果在TIME_WAIT状态下，接收到ACK包（不带数据）或RST包，并且包的序列号刚好是下一个要接收的序列号，由以下代码片段处理：</p>

<pre><code>    if (!paws_reject &amp;&amp;
        (TCP_SKB_CB(skb)-&gt;seq == tcptw-&gt;tw_rcv_nxt &amp;&amp;
        (TCP_SKB_CB(skb)-&gt;seq == TCP_SKB_CB(skb)-&gt;end_seq || th-&gt;rst))) {
        /* In window segment, it may be only reset or bare ack. */
        if (th-&gt;rst) {
            /* This is TIME_WAIT assassination, in two flavors.
            * Oh well... nobody has a sufficient solution to this
            * protocol bug yet.
            */
            if (sysctl_tcp_rfc1337 == 0) {
    kill:
                inet_twsk_deschedule(tw, &amp;tcp_death_row);
                inet_twsk_put(tw);
                return TCP_TW_SUCCESS;
            }
        }
        inet_twsk_schedule(tw, &amp;tcp_death_row, TCP_TIMEWAIT_LEN,
                  TCP_TIMEWAIT_LEN);

        if (tmp_opt.saw_tstamp) {
            tcptw-&gt;tw_ts_recent      = tmp_opt.rcv_tsval;
            tcptw-&gt;tw_ts_recent_stamp = get_seconds();
        }

        inet_twsk_put(tw);
        return TCP_TW_SUCCESS;
    }
</code></pre>

<p>如果是RST包的话，并且系统配置sysctl_tcp_rfc1337（默认情况下为0，参见/proc/sys/net/ipv4/tcp_rfc1337）的值为0，这时会立即释放time_wait传输控制块，丢掉接收的RST包。</p>

<p>如果是ACK包，则会启动TIME_WAIT定时器后丢掉接收到的ACK包。</p>

<p>接下来是对SYN包的处理。前面提到了，如果在TIME_WAIT状态下接收到序列号比上一个连接的结束序列号大的SYN包，可以接受，并建立新的连接，下面这段代码就是来处理这样的情况：</p>

<pre><code>    if (th-&gt;syn &amp;&amp; !th-&gt;rst &amp;&amp; !th-&gt;ack &amp;&amp; !paws_reject &amp;&amp;
        (after(TCP_SKB_CB(skb)-&gt;seq, tcptw-&gt;tw_rcv_nxt) ||
        (tmp_opt.saw_tstamp &amp;&amp;
          (s32)(tcptw-&gt;tw_ts_recent - tmp_opt.rcv_tsval) &lt; 0))) {
        u32 isn = tcptw-&gt;tw_snd_nxt + 65535 + 2;
        if (isn == 0)
            isn++;
        TCP_SKB_CB(skb)-&gt;when = isn;
        return TCP_TW_SYN;
    }
</code></pre>

<p>当返回TCP_TW_SYN时，在tcp_v4_rcv()中会立即释放time_wait控制块，并且开始进行正常的连接建立过程。</p>

<p>如果数据包不是上述几种类型的包，可能的情况有：</p>

<p>1、不是有效的SYN包。不考虑时间戳的话，就是序列号在上一次连接的结束序列号之前</p>

<p>2、ACK包，起始序列号不是下一个要接收的序列号</p>

<p>3、RST包，起始序列号不是下一个要接收的序列号</p>

<p>4、带数据的SKB包</p>

<p>这几种情况由以下代码处理：</p>

<pre><code>    if (!th-&gt;rst) {
        /* In this case we must reset the TIMEWAIT timer.
         *
         * If it is ACKless SYN it may be both old duplicate
         * and new good SYN with random sequence number &lt;rcv_nxt.
         * Do not reschedule in the last case.
         */
        if (paws_reject || th-&gt;ack)
            inet_twsk_schedule(tw, &amp;tcp_death_row, TCP_TIMEWAIT_LEN,
                      TCP_TIMEWAIT_LEN);

        /* Send ACK. Note, we do not put the bucket,
         * it will be released by caller.
         */
        return TCP_TW_ACK;
    }
    inet_twsk_put(tw);
    return TCP_TW_SUCCESS;
</code></pre>

<p>如果是RST包，即第3种情况，则直接返回TCP_TW_SUCCESS，丢掉RST包。</p>

<p>如果带有ACK标志的话，则会启动TIME_WAIT定时器，然后给对端发送ACK。我们知道SYN包正常情况下不会设置ACK标志，所以如果是SYN包不会启动TIME_WAIT定时器，只会给对端发送ACK，告诉对端已经收到SYN包，避免重传，但连接应该不会继续建立。</p>

<p>还有一个细节需要提醒下，就是我们看到在返回TCP_TW_ACK时，没有调用inet_twsk_put()释放对time_wait控制块的引用。这时因为在tcp_v4_rcv()中调用tcp_v4_timewait_ack()发送ACK时会用到time_wait控制块，所以需要保持对time_wait控制块的引用。在tcp_v4_timewait_ack()中发送完ACK后，会调用inet_twsk_put()释放对time_wait控制块的引用。</p>

<p>OK，现在我们对TIME_WAIT状态下接收到数据包的情况有了一个了解，知道内核会如何来处理这些包。但是看到的这些更多的是以服务器端的角度来看的，如果客户端主动关闭连接的话，进入TIME_WAIT状态的是客户端。如果客户端在TIME_WAIT状态下重用端口号来和服务器建立连接，内核会如何处理呢？</p>

<p>我编写了一个测试程序：创建一个套接字，设置SO_REUSEADDR选项，建立连接后立即关闭，关闭后立即又重复同样的过程，发现在第二次调用connect()的时候返回EADDRNOTAVAIL错误。这个测试程序很容易理解，写起来也很容易，就不贴出来了。</p>

<p>要找到这个错误是怎么返回的，需要从TCP层的连接函数tcp_4_connect()开始。在tcp_v4_connect()中没有显示返回EADDRNOTAVAIL错误的地方，可能的地方就是在调用inet_hash_connect()返回的。为了确定是不是在inet_hash_connect()中返回的，使用systemtap编写了一个脚本，发现确实是在这个函数中返回的-99错误（EADDRNOTAVAIL的值为99）。其实这个通过代码也可以看出来，在这个函数之前会先查找目的主机的路由缓存项，调用的是ip_route_connect（）函数，跟着这个函数的调用轨迹，没有发现返回EADDRNOTAVAIL错误的地方。</p>

<p>inet_hash_connect()函数只是对<code>__inet_hash_connect()</code>函数进行了简单的封装。在<code>__inet_hash_connect()</code>中如果已绑定了端口号，并且是和其他传输控制块共享绑定的端口号，则会调用check_established参数指向的函数来检查这个绑定的端口号是否可用，代码如下所示：</p>

<pre><code>    int __inet_hash_connect(struct inet_timewait_death_row *death_row,
            struct sock *sk, u32 port_offset,
            int (*check_established)(struct inet_timewait_death_row *,
                struct sock *, __u16, struct inet_timewait_sock **),
            void (*hash)(struct sock *sk))
    {
        struct inet_hashinfo *hinfo = death_row-&gt;hashinfo;
        const unsigned short snum = inet_sk(sk)-&gt;num;
        struct inet_bind_hashbucket *head;
        struct inet_bind_bucket *tb;
        int ret;
        struct net *net = sock_net(sk);

        if (!snum) {
            ......
        }

        head = &amp;hinfo-&gt;bhash[inet_bhashfn(net, snum, hinfo-&gt;bhash_size)];
        tb  = inet_csk(sk)-&gt;icsk_bind_hash;
        spin_lock_bh(&amp;head-&gt;lock);
        if (sk_head(&amp;tb-&gt;owners) == sk &amp;&amp; !sk-&gt;sk_bind_node.next) {
            hash(sk);
            spin_unlock_bh(&amp;head-&gt;lock);
            return 0;
        } else {
            spin_unlock(&amp;head-&gt;lock);
            /* No definite answer... Walk to established hash table */
            ret = check_established(death_row, sk, snum, NULL);
    out:
            local_bh_enable();
            return ret;
        }
    }
</code></pre>

<p>(sk_head(&amp;tb->owners) == sk &amp;&amp; !sk->sk_bind_node.next)这个判断条件就是用来判断是不是只有当前传输控制块在使用已绑定的端口，条件为false时，会执行else分支，检查是否可用。这么看来，调用bind()成功并不意味着这个端口就真的可以用。</p>

<p>check_established参数对应的函数是__inet_check_established()，在inet_hash_connect()中可以看到。在上面的代码中我们还注意到调用check_established()时第三个参数为NULL，这在后面的分析中会用到。</p>

<p><code>__inet_check_established()</code>函数中，会分别在TIME_WAIT传输控制块和除TIME_WIAT、LISTEN状态外的传输控制块中查找是已绑定的端口是否已经使用，代码片段如下所示：</p>

<pre><code>    /* called with local bh disabled */
    static int __inet_check_established(struct inet_timewait_death_row *death_row,
                        struct sock *sk, __u16 lport,
                        struct inet_timewait_sock **twp)
    {
        struct inet_hashinfo *hinfo = death_row-&gt;hashinfo;
        struct inet_sock *inet = inet_sk(sk);
        __be32 daddr = inet-&gt;rcv_saddr;
        __be32 saddr = inet-&gt;daddr;
        int dif = sk-&gt;sk_bound_dev_if;
        INET_ADDR_COOKIE(acookie, saddr, daddr)
        const __portpair ports = INET_COMBINED_PORTS(inet-&gt;dport, lport);
        struct net *net = sock_net(sk);
        unsigned int hash = inet_ehashfn(net, daddr, lport, saddr, inet-&gt;dport);
        struct inet_ehash_bucket *head = inet_ehash_bucket(hinfo, hash);
        spinlock_t *lock = inet_ehash_lockp(hinfo, hash);
        struct sock *sk2;
        const struct hlist_nulls_node *node;
        struct inet_timewait_sock *tw;

        spin_lock(lock);

        /* Check TIME-WAIT sockets first. */
        sk_nulls_for_each(sk2, node, &amp;head-&gt;twchain) {
            tw = inet_twsk(sk2);

        if (INET_TW_MATCH(sk2, net, hash, acookie,
                        saddr, daddr, ports, dif)) {
                if (twsk_unique(sk, sk2, twp))
                    goto unique;
                else
                    goto not_unique;
            }
        }
        tw = NULL;

        /* And established part... */
        sk_nulls_for_each(sk2, node, &amp;head-&gt;chain) {
            if (INET_MATCH(sk2, net, hash, acookie,
                        saddr, daddr, ports, dif))
                goto not_unique;
        }

    unique:
        ......
        return 0;

    not_unique:
        spin_unlock(lock);
        return -EADDRNOTAVAIL;
    }
</code></pre>

<p>可以看到返回EADDRNOTVAIL错误的有两种情况：</p>

<p>1、在TIME_WAIT传输控制块中找到匹配的端口，并且twsk_unique()返回true时</p>

<p>2、在除TIME_WAIT和LISTEN状态外的传输块中存在匹配的端口。</p>

<p>第二种情况很好容易理解了，只要状态在FIN_WAIT_1、ESTABLISHED等的传输控制块使用的端口和要查找的匹配，就会返回EADDRNOTVAIL错误。第一种情况还要取决于twsk_uniqueue()的返回值，所以接下来我们看twsk_uniqueue()中什么情况下会返回true。</p>

<p>如果是TCP套接字，twsk_uniqueue()中会调用tcp_twsk_uniqueue()来判断，返回true的条件如下所示：</p>

<pre><code>    int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp)
    {
        const struct tcp_timewait_sock *tcptw = tcp_twsk(sktw);
        struct tcp_sock *tp = tcp_sk(sk);

        if (tcptw-&gt;tw_ts_recent_stamp &amp;&amp;
            (twp == NULL || (sysctl_tcp_tw_reuse &amp;&amp;
                    get_seconds() - tcptw-&gt;tw_ts_recent_stamp &gt; 1))) {
            ......
            return 1;
        }

        return 0;
    }
</code></pre>

<p>我们前面提到过，<code>__inet_hash_connect()</code>函数调用check_established指向的函数时第三个参数为NULL，所以现在我们只需要关心tcptw->tw_ts_recent_stamp是否非零，只要这个值非零，tcp_twsk_unique()就会返回true， 在上层connect（）函数中就会返回EADDRNOTVAIL错误。tcptw->tw_ts_recent_stamp存储的是最近接收到段的时间戳值，所以正常情况下这个值不会为零。当然也可以通过调整系统的参数，让这个值可以为零，这不是本文讨论的重点，感兴趣的可以参考tcp_v4_connect()中的代码进行修改。</p>

<p>在导致返回EADDRNOTVAIL的两种情况中，第一种情况可以有办法避免，但是如果的第二次建立连接的时间和第一次关闭连接之间的时间间隔太小的话，此时第一个连接可能处在FIN_WAIT_1、FIN_WAIT_2等状态，此时没有系统参数可以用来避免返回EADDRNOTVAIL。如果你还是想无论如何都要在很短的时间内重用客户端的端口，这样也有办法，要么是用kprobe机制，要么用systemtap脚本，改变<code>__inet_check_established()</code>函数的返回值。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[内核处理time_wait状态详解]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/29/kernel-net-timewait/"/>
    <updated>2015-09-29T17:40:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/29/kernel-net-timewait</id>
    <content type="html"><![CDATA[<p><a href="http://simohayha.iteye.com/blog/566980">http://simohayha.iteye.com/blog/566980</a></p>

<p>这次来详细看内核的time_wait状态的实现，在前面介绍定时器的时候，time_wait就简单的介绍了下。这里我们会先介绍tw状态的实现，然后来介绍内核协议栈如何处理tw状态。</p>

<p>首先我们要知道在linux内核中time_wait的处理是由tcp_time_wait这个函数来做得，比如我们在closing状态收到一个fin，就会调用tcp_time_wait.而内核为time_wait状态的socket专门设计了一个结构就是inet_timewait_sock，并且是挂载在inet_ehash_bucket的tw上(这个结构前面也已经介绍过了)。这里要注意，端口号的那个hash链表中也是会保存time_wait状态的socket的。</p>

<pre><code>    struct inet_timewait_sock {  

        //common也就是包含了一些socket的必要信息。  
        struct sock_common  __tw_common;  
    #define tw_family       __tw_common.skc_family  
    #define tw_state        __tw_common.skc_state  
    #define tw_reuse        __tw_common.skc_reuse  
    #define tw_bound_dev_if     __tw_common.skc_bound_dev_if  
    #define tw_node         __tw_common.skc_nulls_node  
    #define tw_bind_node        __tw_common.skc_bind_node  
    #define tw_refcnt       __tw_common.skc_refcnt  
    #define tw_hash         __tw_common.skc_hash  
    #define tw_prot         __tw_common.skc_prot  
    #define tw_net          __tw_common.skc_net  

         //tw状态的超时时间  
        int         tw_timeout;  
        //这个用来标记我们是正常进入tw还是说由于超时等一系列原因进入(比如超时等一系列原因)  
        volatile unsigned char  tw_substate;  
        /* 3 bits hole, try to pack */  
        //和tcp option中的接收窗口比例类似  
        unsigned char       tw_rcv_wscale;  

        //也就是标示sock的4个域，源目的端口和地址  
        __be16          tw_sport;  
        __be32          tw_daddr __attribute__((aligned(INET_TIMEWAIT_ADDRCMP_ALIGN_BYTES)));  
        __be32          tw_rcv_saddr;  
        __be16          tw_dport;  
        //本地端口。  
        __u16           tw_num;  
        kmemcheck_bitfield_begin(flags);  
        /* And these are ours. */  
        //几个标记位。  
        unsigned int        tw_ipv6only     : 1,  
                    tw_transparent  : 1,  
                    tw_pad      : 14,   /* 14 bits hole */  
                    tw_ipv6_offset  : 16;  
        kmemcheck_bitfield_end(flags);  
        unsigned long       tw_ttd;  
        //链接到端口的hash表中。  
        struct inet_bind_bucket *tw_tb;  
        //链接到全局的tw状态hash表中。  
        struct hlist_node   tw_death_node;  
    };  
</code></pre>

<p>然后我们要知道linux有两种方式执行tw状态的socket，一种是等待2×MSL时间（内核中是60秒)，一种是基于RTO来计算超时时间。</p>

<p>基于RTO的超时时间也叫做recycle模式，这里内核会通过sysctl_tw_recycle（也就是说我们能通过sysctl来打开这个值)以及是否我们还保存有从对端接收到的最近的数据包的时间戳来判断是否进入打开recycle模式的处理。如果进入则会调用tcp_v4_remember_stamp来得到是否打开recycle模式。</p>

<p>下面就是这个片断的代码片断：</p>

<pre><code>    //必须要设置sysctl_tw_recycle以及保存有最后一次的时间戳.  
    if (tcp_death_row.sysctl_tw_recycle &amp;&amp; tp-&gt;rx_opt.ts_recent_stamp)  
        //然后调用remember_stamp（在ipv4中被初始化为tcp_v4_remember_stamp）来得到是否要打开recycle模式。  
        recycle_ok = icsk-&gt;icsk_af_ops-&gt;remember_stamp(sk);  
</code></pre>

<p>然后我们来看tcp_v4_remember_stamp，在看这个之前，我们需要理解inet_peer结构，这个结构也就是保存了何当前主机通信的主机的一些信息，我前面在分析ip层的时候有详细分析这个结构，因此可以看我前面的blog：</p>

<p><a href="http://simohayha.iteye.com/blog/437695">http://simohayha.iteye.com/blog/437695</a></p>

<p>tcp_v4_remember_stamp主要用来从全局的inet_peer中得到对应的当前sock的对端的信息(通过ip地址).然后设置相关的时间戳(tcp_ts_stamp和tcp_ts）.这里要特别注意一个东西，那就是inet_peer是ip层的东西，因此它的key是ip地址，它会忽略端口号。所以说这里inet_peer的这两个时间戳是专门为解决tw状态而设置的。</p>

<p>然后我们来看下tcp option的几个关键的域：</p>

<pre><code>    struct tcp_options_received {  
        /*  PAWS/RTTM data  */  

        //这个值为我们本机更新ts_recent的时间  
        long    ts_recent_stamp;  
        //这个表示最近接收的那个数据包的时间戳  
        u32 ts_recent;  
        //这个表示这个数据包发送时的时间戳    
        u32 rcv_tsval;  
        //这个表示当前数据包所回应的数据包的时间戳。  
        u32 rcv_tsecr;  
        //如果上面两个时间戳都有设置，则saw_tstamp设置为1.  
        u16 saw_tstamp : 1,   //TIMESTAMP seen on SYN packet  
            tstamp_ok : 1,    //d-scack标记  
            dsack : 1,        //Wscale seen on SYN packet  
            wscale_ok : 1,    //SACK seen on SYN packet     
            sack_ok : 4,      //下面两个是窗口扩大倍数，主要是为了解决一些特殊网络下大窗口的问题。  
            snd_wscale : 4,   
            rcv_wscale : 4;   
        /*  SACKs data  */  
        u8  num_sacks;    
        u16 user_mss;     
        u16 mss_clamp;    
    };  
</code></pre>

<p>而inet_peer中的两个时间戳与option中的ts_recent和ts_recent_stamp类似。</p>

<p>来看tcp_v4_remember_stamp的实现：</p>

<pre><code>    int tcp_v4_remember_stamp(struct sock *sk)  
    {  
        struct inet_sock *inet = inet_sk(sk);  
        struct tcp_sock *tp = tcp_sk(sk);  
        struct rtable *rt = (struct rtable *)__sk_dst_get(sk);  
        struct inet_peer *peer = NULL;  
        int release_it = 0;  

        //得到对应peer（两种得到的方式)。  
        if (!rt || rt-&gt;rt_dst != inet-&gt;daddr) {  
            peer = inet_getpeer(inet-&gt;daddr, 1);  
            release_it = 1;  
        } else {  
            if (!rt-&gt;peer)  
                rt_bind_peer(rt, 1);  
            peer = rt-&gt;peer;  
        }  

        //如果peer不存在则会返回0,也就是关闭recycle模式。  
        if (peer) {  
        //这里tcp_ts以及tcp_ts_stamp保存的是最新的时间戳，所以这里与当前的sock的时间戳比较小的话就要更新。  
        if ((s32)(peer-&gt;tcp_ts - tp-&gt;rx_opt.ts_recent) &lt;= 0 ||(peer-&gt;tcp_ts_stamp + TCP_PAWS_MSL &lt; get_seconds() &amp;&amp;  
         peer-&gt;tcp_ts_stamp &lt;= tp-&gt;rx_opt.ts_recent_stamp)) {  

        //更新时间戳。  
        peer-&gt;tcp_ts_stamp = tp-&gt;rx_opt.ts_recent_stamp;  
            peer-&gt;tcp_ts = tp-&gt;rx_opt.ts_recent;  
            }  
            if (release_it)  
                inet_putpeer(peer);  
            return 1;  
        }  

        //关闭recycle模式。  
        return 0;  
    }  
</code></pre>

<p>ok,我们来看tcp_time_wait的实现，这里删掉了ipv6以及md5的部分：</p>

<pre><code>    //这里也就是2*MSL=60秒。  
    #define TCP_TIMEWAIT_LEN (60*HZ)   

    //这里的state标记我们是正常进入tw状态，还是由于死在fin-wait-2状态才进入tw状态的。  
    void tcp_time_wait(struct sock *sk, int state, int timeo)  
    {  
        //TW的socket  
        struct inet_timewait_sock *tw = NULL;  
        const struct inet_connection_sock *icsk = inet_csk(sk);  
        const struct tcp_sock *tp = tcp_sk(sk);  

        //recycle模式的标记。  
        int recycle_ok = 0;  

        //上面已经分析过了。  
        if (tcp_death_row.sysctl_tw_recycle &amp;&amp; tp-&gt;rx_opt.ts_recent_stamp)  
        recycle_ok = icsk-&gt;icsk_af_ops-&gt;remember_stamp(sk);  
        //然后判断tw状态的sock数量是否已经超过限制。  
        if (tcp_death_row.tw_count &lt; tcp_death_row.sysctl_max_tw_buckets)  
        //没有的话alloc一个新的。  
            tw = inet_twsk_alloc(sk, state);  

        //如果tw不为空才会进入处理。  
        if (tw != NULL) {  
            struct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw);  
            //计算对应的超时时间，这里可以看到刚好是3.5*rto.  
            const int rto = (icsk-&gt;icsk_rto &lt;&lt; 2) - (icsk-&gt;icsk_rto &gt;&gt; 1);  
            //更新对应的域。  
            tw-&gt;tw_rcv_wscale    = tp-&gt;rx_opt.rcv_wscale;  
            tcptw-&gt;tw_rcv_nxt    = tp-&gt;rcv_nxt;  
            tcptw-&gt;tw_snd_nxt    = tp-&gt;snd_nxt;  
            tcptw-&gt;tw_rcv_wnd    = tcp_receive_window(tp);  
            tcptw-&gt;tw_ts_recent  = tp-&gt;rx_opt.ts_recent;  
            tcptw-&gt;tw_ts_recent_stamp = tp-&gt;rx_opt.ts_recent_stamp;  

            //更新链表(下面会分析)。  
            __inet_twsk_hashdance(tw, sk, &amp;tcp_hashinfo);  
            //如果传递进来的超时时间小于我们计算的，则让他等于我们计算的超时时间。  
            /* Get the TIME_WAIT timeout firing. */  
            if (timeo &lt; rto)  
                timeo = rto;  

            //如果打开recycle模式，则超时时间为我们基于rto计算的时间。  
            if (recycle_ok) {  
                tw-&gt;tw_timeout = rto;  
            } else {  
                //否则为2*MSL=60秒  
                tw-&gt;tw_timeout = TCP_TIMEWAIT_LEN;  
                //如果正常进入则timeo也就是超时时间为2*MSL.  
                if (state == TCP_TIME_WAIT)  
                    timeo = TCP_TIMEWAIT_LEN;  
            }  

            //最关键的一个函数，我们后面会详细分析。  
            inet_twsk_schedule(tw, &amp;tcp_death_row, timeo,  
                       TCP_TIMEWAIT_LEN);  
            //更新引用计数。  
            inet_twsk_put(tw);  
        } else {  
            LIMIT_NETDEBUG(KERN_INFO "TCP: time wait bucket table overflow\n");  
        }  
        tcp_update_metrics(sk);  
        tcp_done(sk);  
    }  
</code></pre>

<p>然后我们来看__inet_twsk_hashdance函数，这个函数主要是用于更新对应的全局hash表。有关这几个hash表的结构可以去看我前面的blog。</p>

<pre><code>    void __inet_twsk_hashdance(struct inet_timewait_sock *tw, struct sock *sk,  
                   struct inet_hashinfo *hashinfo)  
    {  
        const struct inet_sock *inet = inet_sk(sk);  
        const struct inet_connection_sock *icsk = inet_csk(sk);  
        //得到ehash。  
        struct inet_ehash_bucket *ehead = inet_ehash_bucket(hashinfo, sk-&gt;sk_hash);  
        spinlock_t *lock = inet_ehash_lockp(hashinfo, sk-&gt;sk_hash);  
        struct inet_bind_hashbucket *bhead;  

        //下面这几步是将tw sock链接到bhash中。  
        bhead = &amp;hashinfo-&gt;bhash[inet_bhashfn(twsk_net(tw), inet-&gt;num,hashinfo-&gt;bhash_size)];  
        spin_lock(&amp;bhead-&gt;lock);  
        //链接到bhash。这里icsk的icsk_bind_hash也就是bash的一个元素。  
        tw-&gt;tw_tb = icsk-&gt;icsk_bind_hash;  
        WARN_ON(!icsk-&gt;icsk_bind_hash);  
        //将tw加入到bash中。  
        inet_twsk_add_bind_node(tw, &amp;tw-&gt;tw_tb-&gt;owners);  
        spin_unlock(&amp;bhead-&gt;lock);  

        spin_lock(lock);  


        atomic_inc(&amp;tw-&gt;tw_refcnt);  
        //将tw sock加入到ehash的tw chain中。  
        inet_twsk_add_node_rcu(tw, &amp;ehead-&gt;twchain);  

        //然后从全局的establish hash中remove掉这个socket。详见sock的sk_common域。  
        if (__sk_nulls_del_node_init_rcu(sk))  
            sock_prot_inuse_add(sock_net(sk), sk-&gt;sk_prot, -1);  

        spin_unlock(lock);  
    }  
</code></pre>

<p>这里我们要知道还有一个专门的全局的struct inet_timewait_death_row类型的变量tcp_death_row来保存所有的tw状态的socket。而整个tw状态的socket并不是全部加入到定时器中，而是将tcp_death_row加入到定时器中，然后每次定时器超时通过tcp_death_row来查看定时器的超时情况，从而处理tw状态的sock。</p>

<p>而这里定时器分为两种，一种是长时间的定时器，它也就是tw_timer域，一种是短时间的定时器，它也就是twcal_timer域。</p>

<p>而这里还有两个hash表，一个是twcal_row，它对应twcal_timer这个定时器，也就是说当twcal_timer超时，它就会从twcal_row中取得对应的twsock。对应的cells保存的就是tw_timer定时器超时所用的twsock。</p>

<p>还有两个slot，一个是slot域，一个是twcal_hand域，分别表示当前对应的定时器(上面介绍的两个)所正在执行的定时器的slot。</p>

<p>而上面所说的recycle模式也就是指twcal_timer定时器。</p>

<p>来看结构。
```
    struct inet_timewait_death_row {</p>

<pre><code>    //这几个域会在tcp_death_row中被初始化。  
    int         twcal_hand;  
    unsigned long       twcal_jiffie;  
    //短时间定时器。  
    struct timer_list   twcal_timer;  
    //twcal_timer定时器对应的hash表  
    struct hlist_head   twcal_row[INET_TWDR_RECYCLE_SLOTS];  

    spinlock_t      death_lock;  
    //tw的个数。  
    int         tw_count;  
    //超时时间。  
    int         period;  
    u32         thread_slots;  
    struct work_struct  twkill_work;  
    //长时间定时器  
    struct timer_list   tw_timer;  
    int         slot;  

    //短时间的定时器对应的hash表  
    struct hlist_head   cells[INET_TWDR_TWKILL_SLOTS];  
    struct inet_hashinfo    *hashinfo;  
    int         sysctl_tw_recycle;  
    int         sysctl_max_tw_buckets;  
};  
</code></pre>

<pre><code>

这里要注意INET_TWDR_TWKILL_SLOTS为8,而INET_TWDR_RECYCLE_SLOTS为32。

ok我们接着来看tcp_death_row的初始化。
</code></pre>

<pre><code>struct inet_timewait_death_row tcp_death_row = {  

    //最大桶的个数。  
    .sysctl_max_tw_buckets = NR_FILE * 2,  
    //超时时间，  
    .period     = TCP_TIMEWAIT_LEN / INET_TWDR_TWKILL_SLOTS,  
    //锁  
    .death_lock = __SPIN_LOCK_UNLOCKED(tcp_death_row.death_lock),  

    //可以看到它是链接到全局的inet_hashinfo中的。  
    .hashinfo   = &amp;tcp_hashinfo,  
    //定时器，这里要注意超时函数。  
    .tw_timer   = TIMER_INITIALIZER(inet_twdr_hangman, 0,(unsigned long)&amp;tcp_death_row),  
    //工作队列。其实也就是销毁twsock工作的工作队列。  
    .twkill_work    = __WORK_INITIALIZER(tcp_death_row.twkill_work,                   inet_twdr_twkill_work),  
    /* Short-time timewait calendar */  

    //twcal_hand用来标记twcal_timer定时器是否还在工作。  
    .twcal_hand = -1,  
    .twcal_timer    = TIMER_INITIALIZER(inet_twdr_twcal_tick, 0,(unsigned long)&amp;tcp_death_row),  
};  
</code></pre>

<pre><code>
然后就是inet_twsk_schedule的实现，这个函数也就是tw状态的处理函数。他主要是用来基于超时时间来计算当前twsock的可用的位置。也就是来判断启动那个定时器，然后加入到那个队列。

因此这里的关键就是slot的计算。这里slot的计算是根据我们传递进来的timeo来计算的。

recycle模式下tcp_death_row的超时时间的就为2的INET_TWDR_RECYCLE_TICK幂。

我们一般桌面的hz为100,来看对应的值：
</code></pre>

<pre><code>#elif HZ &lt;= 128  
# define INET_TWDR_RECYCLE_TICK (7 + 2 - INET_TWDR_RECYCLE_SLOTS_LOG)  
</code></pre>

<pre><code>

可以看到这时它的值就为4.

而tw_timer的slot也就是长时间定时器的slot的计算是这样的，它也就是用我们传递进来的超时时间timeo/16(可以看到就是2的INET_TWDR_RECYCLE_TICK次方)然后向上取整。

而这里twdr的period被设置为
</code></pre>

<pre><code>TCP_TIMEWAIT_LEN / INET_TWDR_TWKILL_SLOTS,  

//取slot的代码片断。  
slot = DIV_ROUND_UP(timeo, twdr-&gt;period);  
</code></pre>

<pre><code>

而我们下面取slot的时候也就是会用这个值来散列。可以看到散列表的桶的数目就为INET_TWDR_TWKILL_SLOTS个，因此这里也就是把时间分为INET_TWDR_TWKILL_SLOTS份，每一段时间内的超时twsock都放在一个桶里面，而大于60秒的都放在最后一个桶。
</code></pre>

<pre><code>void inet_twsk_schedule(struct inet_timewait_sock *tw,  
               struct inet_timewait_death_row *twdr,  
               const int timeo, const int timewait_len)  
{  
    struct hlist_head *list;  
    int slot;  

    //得到slot。  
    slot = (timeo + (1 &lt;&lt; INET_TWDR_RECYCLE_TICK) - 1) &gt;&gt; INET_TWDR_RECYCLE_TICK;  

    spin_lock(&amp;twdr-&gt;death_lock);  

    /* Unlink it, if it was scheduled */  
    if (inet_twsk_del_dead_node(tw))  
        twdr-&gt;tw_count--;  
    else  
        atomic_inc(&amp;tw-&gt;tw_refcnt);  

    //判断该添加到那个定时器。  
    if (slot &gt;= INET_TWDR_RECYCLE_SLOTS) {  
        /* Schedule to slow timer */  
        //如果大于timewait_len也就是2*MSL=60秒，则slot为cells的最后一项。  
        if (timeo &gt;= timewait_len) {  
            //设为最后一项。  
            slot = INET_TWDR_TWKILL_SLOTS - 1;  
        } else {  
            //否则timeo除于period然后向上取整。  
            slot = DIV_ROUND_UP(timeo, twdr-&gt;period);  
            //如果大于cells的桶的大小，则也是放到最后一个位置。  
            if (slot &gt;= INET_TWDR_TWKILL_SLOTS)  
                slot = INET_TWDR_TWKILL_SLOTS - 1;  
        }  
        //然后设置超时时间，  
        tw-&gt;tw_ttd = jiffies + timeo;  
        //而twdr的slot为当前正在处理的slot，因此我们需要以这个slot为基准来计算真正的slot  
        slot = (twdr-&gt;slot + slot) &amp; (INET_TWDR_TWKILL_SLOTS - 1);  
        //最后取得对应的链表。  
        list = &amp;twdr-&gt;cells[slot];  
    } else {  
        //设置应当超时的时间。  
        tw-&gt;tw_ttd = jiffies + (slot &lt;&lt; INET_TWDR_RECYCLE_TICK);  
        //判断定时器是否还在工作。如果是第一次我们一定会进入下面的处理  
        if (twdr-&gt;twcal_hand &lt; 0) {  
            //如果没有或者第一次进入，则修改定时器然后重新启动定时器  
            twdr-&gt;twcal_hand = 0;  
            twdr-&gt;twcal_jiffie = jiffies;  
            //定时器的超时时间。可以看到时间为我们传进来的timeo(只不过象tick对齐了)  
            twdr-&gt;twcal_timer.expires = twdr-&gt;twcal_jiffie +(slot &lt;&lt; INET_TWDR_RECYCLE_TICK);  
            //重新添加定时器。  
            add_timer(&amp;twdr-&gt;twcal_timer);  
        } else {  
            //如果原本超时时间太小，则修改定时器的超时时间  
            if (time_after(twdr-&gt;twcal_timer.expires,  
                    jiffies + (slot &lt;&lt; INET_TWDR_RECYCLE_TICK)))  
                mod_timer(&amp;twdr-&gt;twcal_timer,  
                        jiffies + (slot &lt;&lt; INET_TWDR_RECYCLE_TICK));  

                //和上面的tw_timer定时器类似，我们要通过当前正在执行的slot也就是twcal_hand来得到真正的slot。  
                slot = (twdr-&gt;twcal_hand + slot) &amp; (INET_TWDR_RECYCLE_SLOTS - 1);  
            }  
        //取得该插入的桶。  
        list = &amp;twdr-&gt;twcal_row[slot];  
    }  

    //将tw加入到对应的链表中。  
    hlist_add_head(&amp;tw-&gt;tw_death_node, list);  
    //如果第一次则启动定时器。  
    if (twdr-&gt;tw_count++ == 0)  
        mod_timer(&amp;twdr-&gt;tw_timer, jiffies + twdr-&gt;period);  
    spin_unlock(&amp;twdr-&gt;death_lock);  
}  
</code></pre>

<pre><code>

我们先来总结一下上面的代码。当我们进入tw状态，然后我们会根据计算出来的timeo的不同来加载到不同的hash表中。而对应的定时器一个（tw_timer)是每peroid启动一次，一个是每(slot &lt;&lt; INET_TWDR_RECYCLE_TICK)启动一次。

下面的两张图很好的表示了recycle模式(twcal定时器)和非recycle模式的区别：

先是非recycle模式： 

![](/images/kernel/2015-09-29-1.jpeg)

然后是recycle模式： 

![](/images/kernel/2015-09-29-2.jpeg)


接下来我们来看两个超时函数的实现，这里我只简单的介绍下两个超时函数，一个是inet_twdr_hangman，一个是inet_twdr_twcal_tick。

在inet_twdr_hangman中，每次只是遍历对应的slot的队列，然后将队列中的所有sock删除，同时也从bind_hash中删除对应的端口信息。这个函数就不详细分析了。

而在inet_twdr_twcal_tick中，每次遍历所有的twcal_row，然后超时的进行处理(和上面一样),然后没有超时的继续处理).

这里有一个j的计算要注意，前面我们知道我们的twcal的超时时间可以说都是以INET_TWDR_RECYCLE_SLOTS对齐的，而我们这里在处理超时的同时，有可能上面又有很多sock加入到了tw状态，因此这里我们的超时检测的间隔就是1 &lt;&lt; INET_TWDR_RECYCLE_TICK。

来看inet_twdr_twcal_tick的实现：
</code></pre>

<pre><code>void inet_twdr_twcal_tick(unsigned long data)  
{  
    ............................  
    if (twdr-&gt;twcal_hand &lt; 0)  
        goto out;  

    //得到slot。  
    slot = twdr-&gt;twcal_hand;  
    //得到定时器启动时候的jiffes。  
    j = twdr-&gt;twcal_jiffie;  

    //遍历所有的twscok。  
    for (n = 0; n &lt; INET_TWDR_RECYCLE_SLOTS; n++) {  
        //判断是否超时。  
        if (time_before_eq(j, now)) {  
            //处理超时的socket  
            struct hlist_node *node, *safe;  
            struct inet_timewait_sock *tw;  
            .......................................  
        } else {  
            if (!adv) {  
                adv = 1;  
                twdr-&gt;twcal_jiffie = j;  
                twdr-&gt;twcal_hand = slot;  
            }  

        //如果不为空，则将重新添加这些定时器  
        if (!hlist_empty(&amp;twdr-&gt;twcal_row[slot])) {  
            mod_timer(&amp;twdr-&gt;twcal_timer, j);  
                goto out;  
            }  
        }  
        //设置间隔  
        j += 1 &lt;&lt; INET_TWDR_RECYCLE_TICK;  
        //更新  
        slot = (slot + 1) &amp; (INET_TWDR_RECYCLE_SLOTS - 1);  
    }  
    //处理完毕则将twcal_hand设为-1.  
    twdr-&gt;twcal_hand = -1;  

    ...............................  
}  
</code></pre>

<pre><code>
然后我们来看tcp怎么样进入tw状态。这里分为两种，一种是正常进入也就是在wait2收到一个fin，或者closing收到ack。这种都是比较简单的。我们就不分析了。

比较特殊的是，我们有可能会直接从wait1进入tw状态，或者是在wait2等待超时也将会直接进入tw状态。这个时候也就是没有收到对端的fin。

这个主要是为了处理当对端死在close_wait状态的时候，我们需要自己能够恢复到close状态，而不是一直处于wait2状态。

在看代码之前我们需要知道一个东西，那就是fin超时时间，这个超时时间我们可以通过TCP_LINGER2这个option来设置，并且这个值的最大值是sysctl_tcp_fin_timeout/HZ. 这里可以看到sysctl_tcp_fin_timeout是jiffies数，所以要转成秒。我这里简单的测试了下，linger2的默认值也就是60,刚好是2*MSL.

这里linger2也就是代表在tcp_wait2的最大生命周期。如果小于0则说明我们要跳过tw状态。

先来看在tcp_close中的处理，不过这里不理解为什么这么做的原因。

这里为什么有可能会为wait2状态呢，原因是如果设置了linger，则我们就会休眠掉，而休眠的时间可能我们已经收到ack，此时将会进入wait2的处理。
</code></pre>

<pre><code>if (sk-&gt;sk_state == TCP_FIN_WAIT2) {  
        struct tcp_sock *tp = tcp_sk(sk);  
        //如果小于0,则说明从wait2立即超时此时也就是相当于跳过tw状态，所以我们直接发送rst，然后进入close。  
        if (tp-&gt;linger2 &lt; 0) {  
            tcp_set_state(sk, TCP_CLOSE);  
            tcp_send_active_reset(sk, GFP_ATOMIC);  
            NET_INC_STATS_BH(sock_net(sk),  
                    LINUX_MIB_TCPABORTONLINGER);  
        } else {  
            //否则计算fin的时间，这里的超时时间是在linger2和3.5RTO之间取最大值。  
            const int tmo = tcp_fin_time(sk);  

            //如果超时时间很大，则说明我们需要等待时间很长，因此我们启动keepalive探测对端是否存活。  
            if (tmo &gt; TCP_TIMEWAIT_LEN) {  
            inet_csk_reset_keepalive_timer(sk,  
                tmo - TCP_TIMEWAIT_LEN);  
            } else {  
                //否则我们直接进入tw状态。  
                tcp_time_wait(sk, TCP_FIN_WAIT2, tmo);  
                goto out;  
            }  
        }  
    }  
</code></pre>

<pre><code>

还有从wait1直接进入tw，和上面类似，我就不介绍了。


最后我们来看当内核处于tw状态后，再次接收到数据包后如何处理。这里的处理函数就是tcp_timewait_state_process，而他是在tcp_v4_rcv中被调用的，它会先判断是否处于tw状态，如果是的话，进入tw的处理。

这个函数的返回值分为4种。
</code></pre>

<pre><code>enum tcp_tw_status  
{  
    //这个代表我们成功处理了数据包。  
    TCP_TW_SUCCESS = 0,  
    //我们需要发送给对端一个rst。  
    TCP_TW_RST = 1,  
    //我们接收到了重传的fin，因此我们需要重传ack。  
    TCP_TW_ACK = 2,  
    //这个表示我们需要重新建立一个连接。  
    TCP_TW_SYN = 3  
};  
</code></pre>

<pre><code>

这里可能最后一个比较难理解，这里内核注释得很详细，主要是实现了RFC1122:

引用

RFC 1122:

"When a connection is [...] on TIME-WAIT state [...] [a TCP] MAY accept a new SYN from the remote TCP to reopen the connection directly, if it:
(1)  assigns its initial sequence number for the new  connection to be larger than the largest sequence number it used on the previous connection incarnation,and

(2)  returns to TIME-WAIT state if the SYN turns out
to be an old duplicate".

来看这段处理代码：
</code></pre>

<pre><code>switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {  
    case TCP_TW_SYN: {  
        //取得一个sk。  
        struct sock *sk2 = inet_lookup_listener(dev_net(skb-&gt;dev),&amp;tcp_hashinfo,  
            iph-&gt;daddr, th-&gt;dest,inet_iif(skb));  
        if (sk2) {  
            //从tw中删除，然后继续执行（也就是开始三次握手)。  
            inet_twsk_deschedule(inet_twsk(sk), &amp;tcp_death_row);  
            inet_twsk_put(inet_twsk(sk));  
            sk = sk2;  
            goto process;  
        }  
        /* Fall through to ACK */  
    }  
    case TCP_TW_ACK:  
        //发送ack  
        tcp_v4_timewait_ack(sk, skb);  
        break;  
    //发送给对端rst。  
    case TCP_TW_RST:  
        goto no_tcp_socket;  
    //处理成功  
    case TCP_TW_SUCCESS:;  
    }  
    goto discard_it;  
</code></pre>

<p>```</p>

<p>tcp_timewait_state_process这个函数具体的实现我就不介绍了，它就是分为两部分，一部分处理tw_substate == TCP_FIN_WAIT2的情况，一部分是正常情况。在前一种情况，我们对于syn的相应是直接rst的。而后一种我们需要判断是否新建连接。</p>

<p>而对于fin的处理他们也是不一样的，wait2的话，它会将当前的tw重新加入到定时器列表(inet_twsk_schedule).而后一种则只是重新发送ack。</p>
]]></content>
  </entry>
  
</feed>
