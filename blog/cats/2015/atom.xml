<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 2015 | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/2015/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-11-17T15:30:21+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ixgbe]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-ixgbe/"/>
    <updated>2015-11-17T15:16:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-ixgbe</id>
    <content type="html"><![CDATA[<p><a href="http://www.pagefault.info/?p=403">http://www.pagefault.info/?p=403</a></p>

<p>这里分析的驱动代码是给予linux kernel 3.4.4</p>

<p>对应的文件在drivers/net/ethernet/intel 目录下，这个分析不涉及到很细节的地方，主要目的是理解下数据在协议栈和驱动之间是如何交互的。</p>

<p>首先我们知道网卡都是pci设备，因此这里每个网卡驱动其实就是一个pci驱动。并且intel这里是把好几个万兆网卡(82599/82598/x540)的驱动做在一起的。</p>

<p>首先我们来看对应的pci_driver的结构体，这里每个pci驱动都是一个pci_driver的结构体，而这里是多个万兆网卡共用这个结构体ixgbe_driver.</p>

<pre><code>    static struct pci_driver ixgbe_driver = {
        .name     = ixgbe_driver_name,
        .id_table = ixgbe_pci_tbl,
        .probe    = ixgbe_probe,
        .remove   = __devexit_p(ixgbe_remove),
    #ifdef CONFIG_PM
        .suspend  = ixgbe_suspend,
        .resume   = ixgbe_resume,
    #endif
        .shutdown = ixgbe_shutdown,
        .err_handler = &amp;ixgbe_err_handler
    };
</code></pre>

<p>然后是模块初始化方法,这里其实很简单，就是调用pci的驱动注册方法，把ixgbe挂载到pci设备链中。 这里不对pci设备的初始化做太多介绍，我以前的blog有这方面的介绍，想了解的可以去看看。这里我们只需要知道最终内核会调用probe回调来初始化ixgbe。</p>

<pre><code>    char ixgbe_driver_name[] = "ixgbe";
    static const char ixgbe_driver_string[] =
                    "Intel(R) 10 Gigabit PCI Express Network Driver";

    static int __init ixgbe_init_module(void)
    {
        int ret;
        pr_info("%s - version %s\n", ixgbe_driver_string, ixgbe_driver_version);
        pr_info("%s\n", ixgbe_copyright);

    #ifdef CONFIG_IXGBE_DCA
        dca_register_notify(&amp;dca_notifier);
    #endif

        ret = pci_register_driver(&amp;ixgbe_driver);
        return ret;
    }
</code></pre>

<p>这里不去追究具体如何调用probe的细节，我们直接来看probe函数，这个函数中通过硬件的信息来确定需要初始化那个驱动(82598/82599/x540),然后核心的驱动结构就放在下面的这个数组中。</p>

<pre><code>    static const struct ixgbe_info *ixgbe_info_tbl[] = {
        [board_82598] = &amp;ixgbe_82598_info,
        [board_82599] = &amp;ixgbe_82599_info,
        [board_X540] = &amp;ixgbe_X540_info,
    };
</code></pre>

<p>ixgbe_probe函数很长，我们这里就不详细分析了，因为这部分就是对网卡进行初始化。不过我们关注下面几个代码片段。</p>

<p>首先是根据硬件的参数来取得对应的驱动值:</p>

<pre><code>    const struct ixgbe_info *ii = ixgbe_info_tbl[ent-&gt;driver_data];
</code></pre>

<p>然后就是如何将不同的网卡驱动挂载到对应的回调中，这里做的很简单，就是通过对应的netdev的结构取得adapter，然后所有的核心操作都是保存在adapter中的，最后将ii的所有回调拷贝给adapter就可以了。我们来看代码：</p>

<pre><code>        struct net_device *netdev;
        struct ixgbe_adapter *adapter = NULL;
        struct ixgbe_hw *hw;
        .....................................

        adapter = netdev_priv(netdev);
        pci_set_drvdata(pdev, adapter);

        adapter-&gt;netdev = netdev;
        adapter-&gt;pdev = pdev;
        hw = &amp;adapter-&gt;hw;
        hw-&gt;back = adapter;
        .......................................
        memcpy(&amp;hw-&gt;mac.ops, ii-&gt;mac_ops, sizeof(hw-&gt;mac.ops));
        hw-&gt;mac.type  = ii-&gt;mac;

        /* EEPROM */
        memcpy(&amp;hw-&gt;eeprom.ops, ii-&gt;eeprom_ops, sizeof(hw-&gt;eeprom.ops));
        .....................................
</code></pre>

<p>最后需要关注的就是设置网卡属性，这些属性一般来说都是通过ethtool 可以设置的属性(比如tso/checksum等),这里我们就截取一部分:</p>

<pre><code>        netdev-&gt;features = NETIF_F_SG |
                   NETIF_F_IP_CSUM |
                   NETIF_F_IPV6_CSUM |
                   NETIF_F_HW_VLAN_TX |
                   NETIF_F_HW_VLAN_RX |
                   NETIF_F_HW_VLAN_FILTER |
                   NETIF_F_TSO |
                   NETIF_F_TSO6 |
                   NETIF_F_RXHASH |
                   NETIF_F_RXCSUM;

        netdev-&gt;hw_features = netdev-&gt;features;

        switch (adapter-&gt;hw.mac.type) {
        case ixgbe_mac_82599EB:
        case ixgbe_mac_X540:
            netdev-&gt;features |= NETIF_F_SCTP_CSUM;
            netdev-&gt;hw_features |= NETIF_F_SCTP_CSUM |
                           NETIF_F_NTUPLE;
            break;
        default:
            break;
        }

        netdev-&gt;hw_features |= NETIF_F_RXALL;
        ..................................................

        netdev-&gt;priv_flags |= IFF_UNICAST_FLT;
        netdev-&gt;priv_flags |= IFF_SUPP_NOFCS;

        if (adapter-&gt;flags &amp; IXGBE_FLAG_SRIOV_ENABLED)
            adapter-&gt;flags &amp;= ~(IXGBE_FLAG_RSS_ENABLED |
                        IXGBE_FLAG_DCB_ENABLED);
        ...................................................................
        if (pci_using_dac) {
            netdev-&gt;features |= NETIF_F_HIGHDMA;
            netdev-&gt;vlan_features |= NETIF_F_HIGHDMA;
        }

        if (adapter-&gt;flags2 &amp; IXGBE_FLAG2_RSC_CAPABLE)
            netdev-&gt;hw_features |= NETIF_F_LRO;
        if (adapter-&gt;flags2 &amp; IXGBE_FLAG2_RSC_ENABLED)
            netdev-&gt;features |= NETIF_F_LRO;
</code></pre>

<p>然后我们来看下中断的注册，因为万兆网卡大部分都是多对列网卡(配合msix)，因此对于上层软件来说，就好像有多个网卡一样，它们之间的数据是相互独立的，这里读的话主要是napi驱动的poll方法，后面我们会分析这个.</p>

<p>到了这里或许要问那么网卡是如何挂载回调给上层，从而上层来发送数据呢，这里是这样子的，每个网络设备都有一个回调函数表(比如ndo_start_xmit)来供上层调用，而在ixgbe中的话，就是ixgbe_netdev_ops，下面就是这个结构，不过只是截取了我们很感兴趣的几个地方.</p>

<p>不过这里注意，读回调并不在里面，这是因为写是软件主动的，而读则是硬件主动的。现在ixgbe是NAPI的，因此它的poll回调是ixgbe_poll，是中断注册时候通过netif_napi_add添加进去的。</p>

<pre><code>    static const struct net_device_ops ixgbe_netdev_ops = {
        .ndo_open       = ixgbe_open,
        .ndo_stop       = ixgbe_close,
        .ndo_start_xmit     = ixgbe_xmit_frame,
        .ndo_select_queue   = ixgbe_select_queue,
        .ndo_set_rx_mode    = ixgbe_set_rx_mode,
        .ndo_validate_addr  = eth_validate_addr,
        .ndo_set_mac_address    = ixgbe_set_mac,
        .ndo_change_mtu     = ixgbe_change_mtu,
        .ndo_tx_timeout     = ixgbe_tx_timeout,
        .................................................
        .ndo_set_features = ixgbe_set_features,
        .ndo_fix_features = ixgbe_fix_features,
    };
</code></pre>

<p>这里我们最关注的其实就是ndo_start_xmit回调，这个回调就是驱动提供给协议栈的发送回调接口。我们来看这个函数.</p>

<p>它的实现很简单，就是选取对应的队列，然后调用ixgbe_xmit_frame_ring来发送数据。</p>

<pre><code>    static netdev_tx_t ixgbe_xmit_frame(struct sk_buff *skb,
                        struct net_device *netdev)
    {
        struct ixgbe_adapter *adapter = netdev_priv(netdev);
        struct ixgbe_ring *tx_ring;

        if (skb-&gt;len &lt;= 0) {
            dev_kfree_skb_any(skb);
            return NETDEV_TX_OK;
        }

        /*
         * The minimum packet size for olinfo paylen is 17 so pad the skb
         * in order to meet this minimum size requirement.
         */
        if (skb-&gt;len &lt; 17) {
            if (skb_padto(skb, 17))
                return NETDEV_TX_OK;
            skb-&gt;len = 17;
        }
        //取得对应的队列
        tx_ring = adapter-&gt;tx_ring[skb-&gt;queue_mapping];
        //发送数据
        return ixgbe_xmit_frame_ring(skb, adapter, tx_ring);
    }
</code></pre>

<p>而在ixgbe_xmit_frame_ring中，我们就关注两个地方，一个是tso(什么是TSO，请自行google)，一个是如何发送.</p>

<pre><code>        tso = ixgbe_tso(tx_ring, first, &amp;hdr_len);
        if (tso &lt; 0)
            goto out_drop;
        else if (!tso)
            ixgbe_tx_csum(tx_ring, first);

        /* add the ATR filter if ATR is on */
        if (test_bit(__IXGBE_TX_FDIR_INIT_DONE, &amp;tx_ring-&gt;state))
            ixgbe_atr(tx_ring, first);

    #ifdef IXGBE_FCOE
    xmit_fcoe:
    #endif /* IXGBE_FCOE */
        ixgbe_tx_map(tx_ring, first, hdr_len);
</code></pre>

<p>调用ixgbe_tso处理完tso之后，就会调用ixgbe_tx_map来发送数据。而ixgbe_tx_map所做的最主要是两步，第一步请求DMA，第二步写寄存器，通知网卡发送数据.</p>

<pre><code>        dma = dma_map_single(tx_ring-&gt;dev, skb-&gt;data, size, DMA_TO_DEVICE);
        if (dma_mapping_error(tx_ring-&gt;dev, dma))
            goto dma_error;

        /* record length, and DMA address */
        dma_unmap_len_set(first, len, size);
        dma_unmap_addr_set(first, dma, dma);

        tx_desc-&gt;read.buffer_addr = cpu_to_le64(dma);

        for (;;) {
            while (unlikely(size &gt; IXGBE_MAX_DATA_PER_TXD)) {
                tx_desc-&gt;read.cmd_type_len =
                    cmd_type | cpu_to_le32(IXGBE_MAX_DATA_PER_TXD);

                i++;
                tx_desc++;
                if (i == tx_ring-&gt;count) {
                    tx_desc = IXGBE_TX_DESC(tx_ring, 0);
                    i = 0;
                }

                dma += IXGBE_MAX_DATA_PER_TXD;
                size -= IXGBE_MAX_DATA_PER_TXD;

                tx_desc-&gt;read.buffer_addr = cpu_to_le64(dma);
                tx_desc-&gt;read.olinfo_status = 0;
            }

            ...................................................
            data_len -= size;

            dma = skb_frag_dma_map(tx_ring-&gt;dev, frag, 0, size,
                           DMA_TO_DEVICE);
            ..........................................................

            frag++;
        }
        .................................
        tx_ring-&gt;next_to_use = i;

        /* notify HW of packet */
        writel(i, tx_ring-&gt;tail);
        .................
</code></pre>

<p>上面的操作是异步的，也就是说此时内核还不能释放SKB，而是网卡硬件发送完数据之后，会再次产生中断通知内核，然后内核才能释放内存.接下来我们来看这部分代码。</p>

<p>首先来看的是中断注册的代码，这里我们假设启用了MSIX,那么网卡的中断注册回调就是ixgbe_request_msix_irqs函数，这里我们可以看到调用request_irq函数来注册回调，并且每个队列都有自己的中断号。</p>

<pre><code>    static int ixgbe_request_msix_irqs(struct ixgbe_adapter *adapter)
    {
        struct net_device *netdev = adapter-&gt;netdev;
        int q_vectors = adapter-&gt;num_msix_vectors - NON_Q_VECTORS;
        int vector, err;
        int ri = 0, ti = 0;

        for (vector = 0; vector &lt; q_vectors; vector++) {
            struct ixgbe_q_vector *q_vector = adapter-&gt;q_vector[vector];
            struct msix_entry *entry = &amp;adapter-&gt;msix_entries[vector];
            .......................................................................
            err = request_irq(entry-&gt;vector, &amp;ixgbe_msix_clean_rings, 0,
                      q_vector-&gt;name, q_vector);
            if (err) {
                e_err(probe, "request_irq failed for MSIX interrupt "
                      "Error: %d\n", err);
                goto free_queue_irqs;
            }
            /* If Flow Director is enabled, set interrupt affinity */
            if (adapter-&gt;flags &amp; IXGBE_FLAG_FDIR_HASH_CAPABLE) {
                /* assign the mask for this irq */
                irq_set_affinity_hint(entry-&gt;vector,
                              &amp;q_vector-&gt;affinity_mask);
            }
        }

        ..............................................

        return 0;

    free_queue_irqs:
        ...............................
        return err;
    }
</code></pre>

<p>而对应的中断回调是ixgbe_msix_clean_rings,而这个函数呢，做的事情很简单(需要熟悉NAPI的原理，我以前的blog有介绍),就是调用napi_schedule来重新加入软中断处理.</p>

<pre><code>    static irqreturn_t ixgbe_msix_clean_rings(int irq, void *data)
    {
        struct ixgbe_q_vector *q_vector = data;

        /* EIAM disabled interrupts (on this vector) for us */

        if (q_vector-&gt;rx.ring || q_vector-&gt;tx.ring)
            napi_schedule(&amp;q_vector-&gt;napi);

        return IRQ_HANDLED;
    }
</code></pre>

<p>而NAPI驱动我们知道，最终是会调用网卡驱动挂载的poll回调，在ixgbe中，对应的回调就是ixgbe_poll，那么也就是说这个函数要做两个工作，一个是处理读，一个是处理写完之后的清理.</p>

<pre><code>    int ixgbe_poll(struct napi_struct *napi, int budget)
    {
        struct ixgbe_q_vector *q_vector =
                    container_of(napi, struct ixgbe_q_vector, napi);
        struct ixgbe_adapter *adapter = q_vector-&gt;adapter;
        struct ixgbe_ring *ring;
        int per_ring_budget;
        bool clean_complete = true;

    #ifdef CONFIG_IXGBE_DCA
        if (adapter-&gt;flags &amp; IXGBE_FLAG_DCA_ENABLED)
            ixgbe_update_dca(q_vector);
    #endif
        //清理写
        ixgbe_for_each_ring(ring, q_vector-&gt;tx)
            clean_complete &amp;= !!ixgbe_clean_tx_irq(q_vector, ring);

        /* attempt to distribute budget to each queue fairly, but don't allow
         * the budget to go below 1 because we'll exit polling */
        if (q_vector-&gt;rx.count &gt; 1)
            per_ring_budget = max(budget/q_vector-&gt;rx.count, 1);
        else
            per_ring_budget = budget;
        //读数据，并清理已完成的
        ixgbe_for_each_ring(ring, q_vector-&gt;rx)
            clean_complete &amp;= ixgbe_clean_rx_irq(q_vector, ring,
                                 per_ring_budget);

        /* If all work not completed, return budget and keep polling */
        if (!clean_complete)
            return budget;

        /* all work done, exit the polling mode */
        napi_complete(napi);
        if (adapter-&gt;rx_itr_setting &amp; 1)
            ixgbe_set_itr(q_vector);
        if (!test_bit(__IXGBE_DOWN, &amp;adapter-&gt;state))
            ixgbe_irq_enable_queues(adapter, ((u64)1 &lt;&lt; q_vector-&gt;v_idx));

        return 0;
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[cubic]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-cubic/"/>
    <updated>2015-11-17T15:08:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-cubic</id>
    <content type="html"><![CDATA[<p><a href="http://www.pagefault.info/?p=145">http://www.pagefault.info/?p=145</a></p>

<p>这次主要来看一下内核拥塞控制算法cubic的实现，在linux kernel中实现了很多种拥塞控制算法，不过新的内核(2.6.19之后)默认是cubic(想得到当前内核使用的拥塞控制算法可以察看/proc/sys/net/ipv4/tcp_congestion_control这个值).下面是最新的redhat 6的拥塞控制算法(rh5还是bic算法):
<code>
    [root@rhel6 ~]# cat /proc/sys/net/ipv4/tcp_congestion_control
    cubic
</code>
这个算法的paper在这里：</p>

<p><a href="http://netsrv.csc.ncsu.edu/export/cubic_a_new_tcp_2008.pdf">http://netsrv.csc.ncsu.edu/export/cubic_a_new_tcp_2008.pdf</a></p>

<p>拥塞控制算法会在tcp_ack中被调用，如果是正常的ack(比如不是重复的，不是sack等等)就会进入拥塞控制算法。</p>

<p>cubic会调用tcp_slow_start这个方法(基本上每种拥塞控制算法都会调用它)，这个方法主要是处理slow start，而内核中的slow start是这样子的，接收一个ack，snd_cwnd就会加1，然后当cwnd大于设置的拥塞窗口阀值snd_ssthresh的时候，就会进入拥塞避免状态。而在发送数据包的时候，会判断in_flight(可以认为是发送还没确认的数据包，它等于发送未确认的数据包－sack的数据段－丢失的数据段＋重传的数据段，我的前面的blog有详细解释这个数据段)是否大于snd_cwnd,如果大于等于则不会发送数据，如果小于才会继续发送数据。</p>

<p>而进入拥塞避免状态之后，窗口的增长速度将会减缓，</p>

<p>来看一下我用jprobe hook tcp_slow_start(slow start处理函数) 和 tcp_cong_avoid_ai (拥塞避免处理)的数据。</p>

<p>在下面的数据中sk表示当前socket的地址， in_flight packet表示发送还未接收的包, snd_cwnd表示发送拥塞窗口。
然后详细解释下count后面的两个值，其中第一个是snd_cwnd_cnt，表示在当前的拥塞窗口中已经发送的数据段的个数，而第二个是struct bictcp的一个域cnt，它是cubic拥塞算法的核心，主要用来控制在拥塞避免状态的时候，什么时候才能增大拥塞窗口，具体实现是通过比较它和snd_cwnd_cnt，来决定是否增大拥塞窗口，而这个值的计算，我这里不会分析，想了解的，可以去看cubic的paper。</p>

<p>还有一个需要注意的地方就是ssthresh，可以看到这个值在一开始初始化为一个最大的值，然后在进入拥塞避免状态的时候被设置为前一次拥塞窗口的大小.这个处理可以看rfc2581的这段：</p>

<p>  The initial value of ssthresh may be arbitrarily high (i.e., the size of the advertised window), but it may be reduced in response to congestion. When cwnd &lt; ssthresh, the slow-start algorithm is used and when cwnd > ssthresh, the congestion avoidance algorithm is used. When cwnd and ssthresh are equal, the sender may use either of them.</p>

<p>我们后面会看到这个值在cubic中是如何被设置的。
<code>
    //进入slow start，可以看到拥塞窗口默认初始值是3，然后每次接收到ack，都会加1.
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 2, snd_cwnd is 3, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 3, snd_cwnd is 4, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 4, snd_cwnd is 5, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 5, snd_cwnd is 6, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 4, snd_cwnd is 7, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 7, snd_cwnd is 8, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 6, snd_cwnd is 9, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 9, snd_cwnd is 10, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 10, snd_cwnd is 11, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 11, snd_cwnd is 12, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 10, snd_cwnd is 13, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 9, snd_cwnd is 14, ssthresh is 2147483647, count is [0:0]
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 14, snd_cwnd is 15, ssthresh is 2147483647, count is [0:0]
    //ssthresh被更新为当前拥塞窗口的大小，后面会看到为什么是16
    enter [slow start state] tcp_sock is 4129562112 in_flight packets is 13, snd_cwnd is 16, ssthresh is 16, count is [0:0]
    //进入拥塞避免，可以清楚的看到，此时拥塞窗口大于对应的阀值ssthresh.
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 16, snd_cwnd is 17, ssthresh is 16, count is [0:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 15, snd_cwnd is 17, ssthresh is 16, count is [1:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 16, snd_cwnd is 17, ssthresh is 16, count is [2:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 15, snd_cwnd is 17, ssthresh is 16, count is [3:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 14, snd_cwnd is 17, ssthresh is 16, count is [4:877]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 13, snd_cwnd is 17, ssthresh is 16, count is [5:877]
    //这里注意，其中count的第一个值是一直线性增长的，也就是说下面省略了大概80条log，而在这80几次中拥塞窗口一直维持在17没有变化
    ....................................................................................................................
    //可以看到cnt变为3，也就是说明当执行完拥塞避免就会增加窗口了。
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 13, snd_cwnd is 17, ssthresh is 16, count is [91:3]
    //增加窗口的大小，然后将snd_cwnd_cnt reset为0.
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 11, snd_cwnd is 18, ssthresh is 16, count is [0:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 16, snd_cwnd is 18, ssthresh is 16, count is [1:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 14, snd_cwnd is 18, ssthresh is 16, count is [2:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 12, snd_cwnd is 18, ssthresh is 16, count is [3:6]
    enter [cong avoid state] tcp_sock is 4129562112 in_flight packets is 12, snd_cwnd is 18, ssthresh is 16, count is [4:6]
</code></p>

<p>可以看到在slow start的状态，发送拥塞窗口就是很简单的每次加1，而当进入拥塞避免之后，明显的拥塞窗口的增大速度变慢很多。</p>

<p>接下来来看具体的代码是如何实现的.</p>

<p>首先来看bictcp_cong_avoid，也就是cubic拥塞控制算法的handler(一般来说在tcp_ack中被调用)，它有3个参数，第一个是对应的sock，第二个是对应的ack序列号，而第三个就是比较重要的一个变量，表示发送还没有被ack的数据包(在linux 内核tcp拥塞处理一中详细介绍过内核中这些变量)，这个变量是拥塞控制的核心。</p>

<pre><code>    static void bictcp_cong_avoid(struct sock *sk, u32 ack, u32 in_flight)
    {
        struct tcp_sock *tp = tcp_sk(sk);
        struct bictcp *ca = inet_csk_ca(sk);
        //判断发送拥塞窗口是否到达限制，如果到达限制则直接返回。
        if (!tcp_is_cwnd_limited(sk, in_flight))
            return;
        //开始决定进入slow start还是拥塞控制状态
        if (tp-&gt;snd_cwnd &lt;= tp-&gt;snd_ssthresh) {
            //是否需要reset对应的bictcp的值
            if (hystart &amp;&amp; after(ack, ca-&gt;end_seq))
                bictcp_hystart_reset(sk);
            //进入slow start状态
            tcp_slow_start(tp);
        } else {
            //进入拥塞避免状态，首先会更新ca-&gt;cnt.
            bictcp_update(ca, tp-&gt;snd_cwnd);
            //然后进入拥塞避免
            tcp_cong_avoid_ai(tp, ca-&gt;cnt);
        }
    }
</code></pre>

<p>接下来就是看tcp_is_cwnd_limited，这个函数主要是实现RFC2861中对拥塞窗口的检测。它返回1说明拥塞窗口被限制，我们需要增加拥塞窗口，否则的话，就不需要增加拥塞窗口。</p>

<p>然后这里还有两个判断，先来看第一个 gso的概念，gso是Generic Segmentation Offload的简写，他的主要功能就是尽量的延迟数据包的传输，以便与在最恰当的时机传输数据包，这个机制是处于数据包离开协议栈与进入驱动之间。比如如果驱动支持TSO的话，gso就会将多个unsegmented的数据段传递给驱动。而TSO是TCP Segmentation Offload的缩写，它表示驱动支持协议栈发送大的MTU的数据段，然后硬件负责来切包，然后将数据发送出去，这样子的话，就能提高系统的吞吐。这几个东西(还有GRO)，以后我会详细分析，现在只需要大概知道他们是干什么的。</p>

<p>而在这里如果支持gso，就有可能是tso defer住了数据包，因此这里会进行几个相关的判断，来看需不需要增加拥塞窗口。。</p>

<p>然后是burst的概念，主要用来控制网络流量的突发性增大，也就是说当left数据(还能发送的数据段数)大于burst值的时候，我们需要暂时停止增加窗口，因为此时有可能我们这边数据发送过快。</p>

<pre><code>    int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
    {
        const struct tcp_sock *tp = tcp_sk(sk);
        u32 left;
        //比较发送未确认和发送拥塞窗口的大小
        if (in_flight &gt;= tp-&gt;snd_cwnd)
            return 1;
        //得到还能发送的数据包的段数
        left = tp-&gt;snd_cwnd - in_flight;
        if (sk_can_gso(sk) &amp;&amp;
            left * sysctl_tcp_tso_win_divisor &lt; tp-&gt;snd_cwnd &amp;&amp;
            left * tp-&gt;mss_cache &lt; sk-&gt;sk_gso_max_size)
            return 1;
        //看是否还能发送的数据包是否小于等于burst
        return left &lt;= tcp_max_burst(tp);
    }
</code></pre>

<p>接下来来看snd_ssthresh是如何被设置的，这个值在加载cubic模块的时候可以传递一个我们制定的值给它，不过，默认是很大的值，我这里是2147483647,然后在接收ack期间(slow start)期间会调整这个值，在cubic中，默认是16（一般来说说当拥塞窗口到达16的时候，snd_ssthresh会被设置为16).</p>

<p>在cubic中有两个可以设置snd_ssthresh的地方一个是hystart_update，一个是bictcp_recalc_ssthresh，后一个我这里就不介绍了，以后介绍拥塞状态机的时候会详细介绍，现在只需要知道，只有遇到拥塞的时候，需要调整snd_ssthres的时候，我们才需要调用bictcp_recalc_ssthresh。</p>

<p>而hystart_update是在bictcp_acked中被调用，而bictcp_acked则是基本每次收到ack都会调用这个函数，我们来看在bictcp_acked中什么情况就会调用hystart_update：</p>

<pre><code>    /* hystart triggers when cwnd is larger than some threshold */
    if (hystart &amp;&amp; tp-&gt;snd_cwnd &lt;= tp-&gt;snd_ssthresh &amp;&amp;
        tp-&gt;snd_cwnd &gt;= hystart_low_window)
        hystart_update(sk, delay);
</code></pre>

<p>其中hystart是hybrid slow start打开的标志，默认是开启，hystart_low_window是设置snd_ssthresh的最小拥塞窗口值，默认是16。而tp->snd_ssthresh默认是一个很大的值，因此这里就知道了，当拥塞窗口增大到16的时候我们就会进去hystart_update来更新snd_ssthresh.因此hystart_updat换句话来说也就是主要用于是否退出slow start。</p>

<pre><code>    static void hystart_update(struct sock *sk, u32 delay)
    {
        struct tcp_sock *tp = tcp_sk(sk);
        struct bictcp *ca = inet_csk_ca(sk);

        if (!(ca-&gt;found &amp; hystart_detect)) {
            .................................................................
            /*
             * Either one of two conditions are met,
             * we exit from slow start immediately.
             */
            //found是一个是否退出slow start的标记
            if (ca-&gt;found &amp; hystart_detect)
                //设置snd_ssthresh
                tp-&gt;snd_ssthresh = tp-&gt;snd_cwnd;
        }
    }
</code></pre>

<p>然后是slow start的处理,这里有关abc的处理，注释都很详细了，这里就不解释了，我们主要看abc关闭的部分。这里使用cnt，也是主要为了打开abc之后的slow start。</p>

<p>这是abc（Appropriate Byte Counting）相关的rfc：</p>

<p><a href="http://www.faqs.org/rfcs/rfc3465.html">http://www.faqs.org/rfcs/rfc3465.html</a></p>

<p>Appropriate Byte Countin会导致拥塞控制算法很激进，比如打开它之后就不一定每次ack都会执行slow start，而且窗口也会增加的快很多。</p>

<pre><code>    void tcp_slow_start(struct tcp_sock *tp)
    {
        int cnt; /* increase in packets */

        /* RFC3465: ABC Slow start
         * Increase only after a full MSS of bytes is acked
         *
         * TCP sender SHOULD increase cwnd by the number of
         * previously unacknowledged bytes ACKed by each incoming
         * acknowledgment, provided the increase is not more than L
         */
        if (sysctl_tcp_abc &amp;&amp; tp-&gt;bytes_acked &lt; tp-&gt;mss_cache)
            return;
        //限制slow start的cnt
        if (sysctl_tcp_max_ssthresh &gt; 0 &amp;&amp; tp-&gt;snd_cwnd &gt; sysctl_tcp_max_ssthresh)
            cnt = sysctl_tcp_max_ssthresh &gt;&gt; 1; /* limited slow start */
        else
            cnt = tp-&gt;snd_cwnd;         /* exponential increase */

        /* RFC3465: ABC
         * We MAY increase by 2 if discovered delayed ack
         */
        if (sysctl_tcp_abc &gt; 1 &amp;&amp; tp-&gt;bytes_acked &gt;= 2*tp-&gt;mss_cache)
            cnt &lt;&lt;= 1;
        tp-&gt;bytes_acked = 0;
        //更新cnt，也就是当前拥塞窗口接受的段的个数.
        tp-&gt;snd_cwnd_cnt += cnt;
        while (tp-&gt;snd_cwnd_cnt &gt;= tp-&gt;snd_cwnd) {
            //这里snd_cwnd_cnt是snd_cwnd的几倍，拥塞窗口就增加几。
            tp-&gt;snd_cwnd_cnt -= tp-&gt;snd_cwnd;
            //如果拥塞窗口没有超过最大值，则加一
            if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp)
                tp-&gt;snd_cwnd++;
        }
    }
</code></pre>

<p>最后是拥塞避免的处理。这里主要的步骤就是通过判断当前的拥塞窗口下已经发送的数据段的个数是否大于算法计算出来的值w，如果大于我们才能增加拥塞窗口值，否则之需要增加snd_cwnd_cnt。</p>

<pre><code>    void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w)
    {
        //判断是否大于我们的标记值
        if (tp-&gt;snd_cwnd_cnt &gt;= w) {
            if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp)
                tp-&gt;snd_cwnd++;
            tp-&gt;snd_cwnd_cnt = 0;
        } else {
            //增加计数值
            tp-&gt;snd_cwnd_cnt++;
        }
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iptables]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-iptables/"/>
    <updated>2015-11-17T14:48:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/kernel-net-iptables</id>
    <content type="html"><![CDATA[<p><a href="http://my.oschina.net/kisops/blog/150995">http://my.oschina.net/kisops/blog/150995</a></p>

<p>解决：nf_conntrack: table full, dropping packet.</p>

<hr />

<p> “连接跟踪表已满，开始丢包”！相信不少用iptables的同学都会见过这个错误信息吧，这个问题曾经也困扰过我好长一段时间。此问题的解决办法有四种（nf_conntrack 在CentOS 5 / kernel &lt;= 2.6.19中名为 ip_conntrack ）：</p>

<h4>一、关闭防火墙。 简单粗暴，直接有效</h4>

<pre><code>    chkconfig iptables off
    chkconfig ip6tables off
    service iptables stop
    service ip6tables stop
</code></pre>

<p>切记：在防火墙关闭状态下，不要通过iptables指令（比如 iptables -nL）来查看当前状态！因为这样会导致防火墙被启动，而且规则为空。虽然不会有任何拦截效果，但所有连接状态都会被记录，浪费资源且影响性能并可能导致防火墙主动丢包！</p>

<h4>二、加大防火墙跟踪表的大小，优化对应的系统参数</h4>

<h5>1、状态跟踪表的最大行数的设定，理论最大值 CONNTRACK_MAX = RAMSIZE (in bytes) / 16384 / (ARCH / 32)</h5>

<p>以64G的64位操作系统为例，CONNTRACK_MAX = 64<em>1024</em>1024*1024/16384/2 = 2097152</p>

<p>即时生效请执行：
<code>
    sysctl -w net.netfilter.nf_conntrack_max = 2097152
</code></p>

<h5>2、其哈希表大小通常为总表的1/8，最大为1/2。</h5>

<p>CONNTRACK_BUCKETS = CONNTRACK_MAX / 8</p>

<p>同样64G的64位操作系统，哈希最佳范围是 262144 ~ 1048576 。</p>

<p>运行状态中通过 sysctl net.netfilter.nf_conntrack_buckets 进行查看，通过文件 /sys/module/nf_conntrack/parameters/hashsize 进行设置</p>

<p>或者新建 /etc/modprobe.d/iptables.conf ，重新加载模块才生效：
<code>
    options nf_conntrack hashsize = 262144
</code></p>

<h5>3、还有些相关的系统参数<code>sysctl -a | grep nf_conntrack</code>可以调优（/etc/sysctl.conf ）：</h5>

<pre><code>    net.netfilter.nf_conntrack_max  =   1048576
    net.netfilter.ip_conntrack_tcp_timeout_established  =   3600
    net.netfilter.nf_conntrack_tcp_timeout_close_wait  =   60
    net.netfilter.nf_conntrack_tcp_timeout_fin_wait  =   120
    net.netfilter.nf_conntrack_tcp_timeout_time_wait  =   120
</code></pre>

<p>三、使用祼表，添加“不跟踪”标识。如下示例更适合桌面系统或随意性强的服务器。因为它开启了连接的状态机制，方便和外部通信。修改 /etc/sysconfig/iptables 文件：
<code>
    *raw
    # 对TCP连接不启用追踪，解决ip_contrack满导致无法连接的问题
    -A PREROUTING -p tcp -m tcp --dport 80 -j NOTRACK
    -A PREROUTING -p tcp -m tcp --dport 22 -j NOTRACK
    -A PREROUTING -p tcp -m tcp --dport 21 -j NOTRACK
    -A PREROUTING -p tcp -m tcp --dport 11211 -j NOTRACK
    -A PREROUTING -p tcp -m tcp --dport 60000:60100 -j NOTRACK
    -A PREROUTING -p tcp -s 192.168.10.1 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 80 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 22 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 21 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 11211 -j NOTRACK
    -A OUTPUT -p tcp -m tcp --sport 60000:60100 -j NOTRACK
    -A OUTPUT -p tcp -s 192.168.10.1 -j NOTRACK
    COMMIT
    *filter
    # 允许ping
    -A INPUT -p icmp -j ACCEPT
    # 对本地回路、第5张网卡放行
    -A INPUT -i lo -j ACCEPT
    -A INPUT -i eth4 -j ACCEPT
    # 连接状态跟踪，已建立的连接允许传输数据
    -A INPUT -m state --state ESTABLISHED,RELATED,INVALID,UNTRACKED -j ACCEPT
    # filter表里存在但在raw里不存在的，默认会进行连接状态跟踪
    -A INPUT -s 192.168.10.31 -p tcp --dport 2669 -j ACCEPT
    -A INPUT -j REJECT --reject-with icmp-host-prohibited
    -A FORWARD -j REJECT --reject-with icmp-host-prohibited
    COMMIT
</code></p>

<p>或者干脆对所有连接都关闭跟踪，不跟踪任何连接状态。不过规则就限制比较严谨，进出都需要显式申明。示例 /etc/sysconfig/iptables ：</p>

<pre><code>    *raw
    # 对TCP/UDP连接不启用追踪，解决nf_contrack满导致无法连接的问题
    -A PREROUTING -p tcp -j NOTRACK
    -A PREROUTING -p udp -j NOTRACK
    -A OUTPUT -p tcp -j NOTRACK
    -A OUTPUT -p udp -j NOTRACK
    COMMIT
    *filter
    # 允许ping
    -A INPUT -p icmp -j ACCEPT
    # 对本地回路和eth1放行
    -A INPUT -i lo -j ACCEPT
    -A INPUT -i eth1 -j ACCEPT
    # 只允许符合条件的连接进行传输数据
    -A INPUT -p tcp --dport 22 -j ACCEPT
    -A INPUT -p tcp --sport 80 -j ACCEPT
    -A INPUT -p udp --sport 53 -j ACCEPT
    -A INPUT -p udp --sport 123 -j ACCEPT
    # 出去的包都不限制
    -A OUTPUT -p tcp -j ACCEPT
    -A OUTPUT -p udp -j ACCEPT
    # 输入和转发的包不符合规则的全拦截
    -A INPUT -j REJECT --reject-with icmp-host-prohibited
    -A FORWARD -j REJECT --reject-with icmp-host-prohibited
    COMMIT
</code></pre>

<p>效果如下图：</p>

<p><img src="/images/kernel/2015-11-17.png" alt="" /></p>

<h4>四、删除连接跟踪模块<code>lsmod | grep nf_conntrack</code>，不使用连接状态的跟踪功能。</h4>

<h5>1、删除nf_conntrack和相关的依赖模块，示例：</h5>

<pre><code>    rmmod nf_conntrack_ipv4
    rmmod nf_conntrack_ipv6
    rmmod xt_state
    rmmod xt_CT
    rmmod xt_conntrack
    rmmod iptable_nat
    rmmod ipt_REDIRECT
    rmmod nf_nat
    rmmod nf_conntrack
</code></pre>

<h5>2、禁用跟踪模块，把它加到黑名单（/etc/modprobe.d/blacklist.conf ）：</h5>

<pre><code>    # 禁用 nf_conntrack 模块
    blacklist nf_conntrack
    blacklist nf_conntrack_ipv6
    blacklist xt_conntrack
    blacklist nf_conntrack_ftp
    blacklist xt_state
    blacklist iptable_nat
    blacklist ipt_REDIRECT
    blacklist nf_nat
    blacklist nf_conntrack_ipv4
</code></pre>

<h5>3、去掉防火墙里所有和状态相关的配置（比如state状态，NAT功能），示例：</h5>

<pre><code>    *filter
    # 允许ping
    -A INPUT -p icmp -j ACCEPT
    # 对本地回路和第2张网卡放行
    -A INPUT -i lo -j ACCEPT
    -A INPUT -i eth1 -j ACCEPT
    # 对端口放行
    -A INPUT -p tcp --dport 1331 -j ACCEPT
    # 对IP放行
    -A INPUT -s 192.168.10.31 -j ACCEPT

    #允许本机进行DNS查询

    -A INPUT -p udp --sport 53 -j ACCEPT
    -A OUTPUT -p udp -j ACCEPT
    -A INPUT -j REJECT --reject-with icmp-host-prohibited
    -A FORWARD -j REJECT --reject-with icmp-host-prohibited
    COMMIT
</code></pre>

<p>另外，防火墙的配置文件最好也改下，不要加载任何额外模块（/etc/sysconfig/iptables-config）：</p>

<pre><code>    IPTABLES_MODULES="" # 不需要任何附加模块
    IPTABLES_MODULES_UNLOAD="no" # 避免iptables重启后sysctl中对应的参数被重置为系统默认值
    IPTABLES_SAVE_ON_STOP="no"
    IPTABLES_SAVE_ON_RESTART="no"
    IPTABLES_SAVE_COUNTER="no"
    IPTABLES_STATUS_NUMERIC="yes"
    IPTABLES_STATUS_VERBOSE="no"
    IPTABLES_STATUS_LINENUMBERS="no"
</code></pre>

<p>往往我们对连接的跟踪都是基于操作系统的（netstat / ss ），防火墙的连接状态完全是它自身实现产生的。</p>

<p>总结：防火墙有条件还是交给上层设备完成会更好，使用防火墙一定要做调优；如果不需要防火墙的跟踪功能，规则简单的可以开启NOTRACK选项，条件允许的情况下就删除它吧！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[curl命令]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/command-curl/"/>
    <updated>2015-11-17T10:04:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/command-curl</id>
    <content type="html"><![CDATA[<p>-s 静默输出；没有-s的话就是下面的情况，这是在脚本等情况下不需要的信息。</p>

<h4>监控首页各项时间指标：</h4>

<pre><code>    curl -o /dev/null -s -w '%{time_connect} %{time_starttransfer} %{time_total}' http://www.miotour.com
    0.244 1.044 2.672

    时间指标解释 ：
    time_connect        建立到服务器的 TCP 连接所用的时间
    time_starttransfer  在发出请求之后，Web 服务器返回数据的第一个字节所用的时间
    time_total          完成请求所用的时间
</code></pre>

<p>在发出请求之后，Web 服务器处理请求并开始发回数据所用的时间是 （time_starttransfer）1.044 - （time_connect）0.244 = 0.8 秒</p>

<p>客户机从服务器下载数据所用的时间是 （time_total）2.672 - （time_starttransfer）1.044 = 1.682 秒</p>

<h4>-x 指定访问IP与端口号</h4>

<pre><code>    curl -x 61.135.169.105:80 http://www.baidu.com
</code></pre>

<h4>-I 仅仅取文件的http头部</h4>

<pre><code>    curl   -I  -x 192.168.1.1:80  http://www.miotour.com
</code></pre>

<h4>用referer做的防盗链，就可以使用-e来设置</h4>

<pre><code>    curl -e "http://www.qiecuo.org"    http:// www.miotour.com -v  -I
</code></pre>

<h4>-H去构造你想要的http头部</h4>

<pre><code>    curl -H "X-Forward-For:8.8.8.8" http://www.miotour.com  -v  -I
</code></pre>

<h4>curl提交用户名和密码</h4>

<pre><code>    curl http://name:passwd@www.miotour.com
    curl -u name:passwd http://www.miotour.com
</code></pre>

<h4>-b “cookie” 此参数用来构造一个携带cookie的请求</h4>

<h4>USER AGENT   关于浏览器发送的http请求信息. Curl允许用命令制定. 发送一些用于欺骗服务器或cgi的信息.</h4>

<pre><code>    curl -A 'Mozilla/3.0 (Win95; I)' http://www.nationsbank.com/
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[alias命令]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/11/17/command-alias/"/>
    <updated>2015-11-17T09:48:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/11/17/command-alias</id>
    <content type="html"><![CDATA[<p>功能说明: 设置指令的别名。</p>

<p>语   法: alias[别名]=[指令名称]</p>

<p>参   数: 若不加任何参数，则列出目前所有的别名设置。</p>

<p>举   例:
<code>
    alias
    alias egrep='egrep --color=auto'
    alias fgrep='fgrep --color=auto'
    alias grep='grep --color=auto'
    alias l='ls -CF'
    alias la='ls -A'
    alias ll='ls -alF'
    alias ls='ls --color=auto'
</code></p>

<p>说   明：用户可利用alias，自定指令的别名。若仅输入alias，则可列出目前所有的别名设置。　alias的效力仅及于该次登入的操作。若要每次登入是即自动设好别名，可在/etc/profile或自己的~/.bashrc中设定指令的别名。</p>

<p>  如果你想给每一位用户都生效的别名，请把alias la=&lsquo;ls -al&rsquo; 一行加在/etc/bashrc最后面，bashrc是环境变量的配置文件 /etc/bashrc和~/.bashrc 区别就在于一个是设置给全系统一个是设置给单用户使用.</p>
]]></content>
  </entry>
  
</feed>
