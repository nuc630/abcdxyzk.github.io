<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel~mm | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel~mm/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-07-23T18:01:22+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[kmalloc、vmalloc、malloc的区别]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/06/02/kernel-mm-mm/"/>
    <updated>2015-06-02T16:48:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/06/02/kernel-mm-mm</id>
    <content type="html"><![CDATA[<p>blog.csdn.net/macrossdzh/article/details/5958368</p>

<p>简单的说：<br/>
  kmalloc和vmalloc是分配的是内核的内存,malloc分配的是用户的内存<br/>
  kmalloc保证分配的内存在物理上是连续的,vmalloc保证的是在虚拟地址空间上的连续,malloc不保证任何东西(这点是自己猜测的,不一定正确)<br/>
  kmalloc能分配的大小有限,vmalloc和malloc能分配的大小相对较大<br/>
  内存只有在要被DMA访问的时候才需要物理上连续<br/>
  vmalloc比kmalloc要慢</p>

<p>详细的解释：<br/>
  对于提供了MMU（存储管理器，辅助操作系统进行内存管理，提供虚实地址转换等硬件支持）的处理器而言，Linux提供了复杂的存储管理系统，使得进程所能访问的内存达到4GB。
  进程的4GB内存空间被人为的分为两个部分&ndash;用户空间与内核空间。用户空间地址分布从0到3GB(PAGE_OFFSET，在0x86中它等于0xC0000000)，3GB到4GB为内核空间。<br/>
  内核空间中，从3G到vmalloc_start这段地址是物理内存映射区域（该区域中包含了内核镜像、物理页框表mem_map等等），比如我们使用 的 VMware虚拟系统内存是160M，那么3G～3G+160M这片内存就应该映射物理内存。在物理内存映射区之后，就是vmalloc区域。对于 160M的系统而言，vmalloc_start位置应在3G+160M附近（在物理内存映射区与vmalloc_start期间还存在一个8M的gap 来防止跃界），vmalloc_end的位置接近4G(最后位置系统会保留一片128k大小的区域用于专用页面映射)</p>

<p>  kmalloc和get_free_page申请的内存位于物理内存映射区域，而且在物理上也是连续的，它们与真实的物理地址只有一个固定的偏移，因此存在较简单的转换关系，virt_to_phys()可以实现内核虚拟地址转化为物理地址：
<code>
    #define __pa(x) ((unsigned long)(x)-PAGE_OFFSET)
    extern inline unsigned long virt_to_phys(volatile void * address)
    {
        return __pa(address);
    }
</code></p>

<p>上面转换过程是将虚拟地址减去3G（PAGE_OFFSET=0XC000000）。</p>

<p>与之对应的函数为phys_to_virt()，将内核物理地址转化为虚拟地址：
<code>
    #define __va(x) ((void *)((unsigned long)(x)+PAGE_OFFSET))
    extern inline void * phys_to_virt(unsigned long address)
    {
        return __va(address);
    }
</code>
virt_to_phys()和phys_to_virt()都定义在include/asm-i386/io.h中。</p>

<p>而vmalloc申请的内存则位于vmalloc_start～vmalloc_end之间，与物理地址没有简单的转换关系，虽然在逻辑上它们也是连续的，但是在物理上它们不要求连续。</p>

<hr />

<p>blog.csdn.net/kris_fei/article/details/17243527</p>

<p>平台： msm8x25<br/>
系统： android 4.1<br/>
内核： 3.4.0</p>

<h4>概念</h4>

<p>由于系统的连续物理内存有限，这使得非连续物理内存的使用在linux内核中出现，这叫vmalloc机制。和前者一样，vmalloc机制中的虚拟地址也是连续的。</p>

<h5>Vmallocinfo</h5>

<p>Vmalloc机制并不是狭义地指使用vmalloc函数分配，其他还有如ioremap, iotable_init等。可以从/proc/vmallocinfo获取到此信息：</p>

<pre><code>    #cat /proc/vmallocinfo
    0xf3600000-0xf36ff0001044480 binder_mmap+0xb0/0x224 ioremap
    ………..
    0xf6680000-0xf66c1000 266240 kgsl_page_alloc_map_kernel+0x98/0xe8 ioremap
    0xf6700000-0xf67ff0001044480 binder_mmap+0xb0/0x224 ioremap
    …………….
    0xf6f00000-0xf6f41000 266240 kgsl_page_alloc_map_kernel+0x98/0xe8 ioremap
    0xf7200000-0xf72ff0001044480 binder_mmap+0xb0/0x224 ioremap
    0xfa000000-0xfa001000   4096 iotable_init+0x0/0xb0 phys=c0800000 ioremap
    ……………..
    0xfa105000-0xfa106000   4096 iotable_init+0x0/0xb0 phys=a9800000 ioremap
    0xfa200000-0xfa3000001048576 pmd_empty_section_gap+0x0/0x3c ioremap
    0xfa300000-0xfa4000001048576 iotable_init+0x0/0xb0 phys=100000 ioremap
    0xfa400000-0xfa5000001048576 iotable_init+0x0/0xb0 phys=aa500000 ioremap
    0xfa500000-0xfa6000001048576 pmd_empty_section_gap+0x0/0x3c ioremap
    0xfa701000-0xfa702000   4096 iotable_init+0x0/0xb0 phys=c0400000 ioremap
    …………..
    0xfa800000-0xfa9000001048576 pmd_empty_section_gap+0x0/0x3c ioremap
    0xfa900000-0xfb60000013631488 iotable_init+0x0/0xb0 phys=ac000000 ioremap
    0xfefdc000-0xff000000 147456 pcpu_get_vm_areas+0x0/0x56c vmalloc
</code></pre>

<p>上面的列数意思依次是：虚拟地址，分配大小，哪个函数分配的，物理地址，分配类型。</p>

<p>后面会提到vmalloc size的划分是按照此info来修改的。</p>

<h4>分配标志</h4>

<p>是否划分到vamlloc区域主要是以下重要的标志来决定的：</p>

<p>File: kernel/include/linux/vmalloc.h
<code>
    /* bits in flags ofvmalloc's vm_struct below */
    #defineVM_IOREMAP    0x00000001     /* ioremap()and friends */
    #define VM_ALLOC     0x00000002     /* vmalloc() */
    #defineVM_MAP        0x00000004     /* vmap()ed pages */
    #defineVM_USERMAP    0x00000008     /* suitable forremap_vmalloc_range */
    #defineVM_VPAGES     0x00000010     /* buffer for pages was vmalloc'ed */
    #defineVM_UNLIST     0x00000020     /* vm_struct is not listed in vmlist */
    /* bits [20..32]reserved for arch specific ioremap internals */
</code></p>

<p>Vmallocinfo中的函数，你可以对照源码看一下，在设置flag的时候就会有VM_IOREMAP, VM_ALLOC这些标志。</p>

<h5>Vmalloc区域</h5>

<p>Vmalloc的区域是由两个宏变量来表示： VMALLOC_START,VMALLOC_END.</p>

<p>File: kernel/arch/arm/include/asm/pgtable.h
<code>
    #defineVMALLOC_OFFSET       (8*1024*1024)
    #defineVMALLOC_START        (((unsigned long)high_memory + VMALLOC_OFFSET) &amp; ~(VMALLOC_OFFSET-1))
    #defineVMALLOC_END          0xff000000UL
</code></p>

<p>VMALLOC_START：看上去会随着high_memory的值变化。</p>

<p>VMALLOC_OFFSET：系统会在low memory和VMALLOC区域留8M，防止访问越界。因此假如理论上vmalloc size有300M，实际可用的也是只有292M。</p>

<p>File: kernel/Documentation/arm/memory.txt有给出更好的解释：
<code>
VMALLOC_START   VMALLOC_END-1    vmalloc() / ioremap() space. Memory returned byvmalloc/ioremap will be dynamically placed in this region. Machine specificstatic mappings are also located here through iotable_init(). VMALLOC_START isbased upon the value of the high_memoryvariable, and VMALLOC_END is equal to 0xff000000.
</code></p>

<p>下图摘自网络，看下VMALLOC_START和VMALLOC_END的位置。0xc0000000到VMALLOC_START为low memory虚拟地址区域。</p>

<h4>Vmallocsize 计算</h4>

<p>有了以上知识后我们看下vmalloc size是如何分配的，目前有两种方法，kernel默认分配一个, 以及开机从cmdline分配。</p>

<h5>1. 从cmdline分配</h5>

<p>File: device/qcom/msm7627a/BoardConfig.mk</p>

<p>BOARD_KERNEL_CMDLINE := androidboot.hardware=qcom loglevel=7vmalloc=200M</p>

<p>上面的值在build的时候会被赋值给kernel 的cmdline。</p>

<p>开机的时候early_vmalloc()会去读取vmalloc这个值。</p>

<p>File: kernel/arch/arm/mm/mmu.c
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
<span class='line-number'>193</span>
<span class='line-number'>194</span>
<span class='line-number'>195</span>
<span class='line-number'>196</span>
<span class='line-number'>197</span>
<span class='line-number'>198</span>
<span class='line-number'>199</span>
<span class='line-number'>200</span>
<span class='line-number'>201</span>
<span class='line-number'>202</span>
<span class='line-number'>203</span>
<span class='line-number'>204</span>
<span class='line-number'>205</span>
<span class='line-number'>206</span>
<span class='line-number'>207</span>
<span class='line-number'>208</span>
<span class='line-number'>209</span>
<span class='line-number'>210</span>
<span class='line-number'>211</span>
<span class='line-number'>212</span>
<span class='line-number'>213</span>
<span class='line-number'>214</span>
<span class='line-number'>215</span>
<span class='line-number'>216</span>
<span class='line-number'>217</span>
<span class='line-number'>218</span>
<span class='line-number'>219</span>
<span class='line-number'>220</span>
<span class='line-number'>221</span>
<span class='line-number'>222</span>
<span class='line-number'>223</span>
<span class='line-number'>224</span>
<span class='line-number'>225</span>
<span class='line-number'>226</span>
<span class='line-number'>227</span>
<span class='line-number'>228</span>
<span class='line-number'>229</span>
<span class='line-number'>230</span>
<span class='line-number'>231</span>
<span class='line-number'>232</span>
<span class='line-number'>233</span>
<span class='line-number'>234</span>
<span class='line-number'>235</span>
<span class='line-number'>236</span>
<span class='line-number'>237</span>
<span class='line-number'>238</span>
<span class='line-number'>239</span>
<span class='line-number'>240</span>
<span class='line-number'>241</span>
<span class='line-number'>242</span>
<span class='line-number'>243</span>
<span class='line-number'>244</span>
<span class='line-number'>245</span>
<span class='line-number'>246</span>
<span class='line-number'>247</span>
<span class='line-number'>248</span>
<span class='line-number'>249</span>
<span class='line-number'>250</span>
<span class='line-number'>251</span>
<span class='line-number'>252</span>
<span class='line-number'>253</span>
<span class='line-number'>254</span>
<span class='line-number'>255</span>
<span class='line-number'>256</span>
<span class='line-number'>257</span>
<span class='line-number'>258</span>
<span class='line-number'>259</span>
<span class='line-number'>260</span>
<span class='line-number'>261</span>
<span class='line-number'>262</span>
<span class='line-number'>263</span>
<span class='line-number'>264</span>
<span class='line-number'>265</span>
<span class='line-number'>266</span>
<span class='line-number'>267</span>
<span class='line-number'>268</span>
<span class='line-number'>269</span>
<span class='line-number'>270</span>
<span class='line-number'>271</span>
<span class='line-number'>272</span>
<span class='line-number'>273</span>
<span class='line-number'>274</span>
<span class='line-number'>275</span>
<span class='line-number'>276</span>
<span class='line-number'>277</span>
<span class='line-number'>278</span>
<span class='line-number'>279</span>
<span class='line-number'>280</span>
<span class='line-number'>281</span>
<span class='line-number'>282</span>
<span class='line-number'>283</span>
<span class='line-number'>284</span>
<span class='line-number'>285</span>
<span class='line-number'>286</span>
<span class='line-number'>287</span>
<span class='line-number'>288</span>
<span class='line-number'>289</span>
<span class='line-number'>290</span>
<span class='line-number'>291</span>
<span class='line-number'>292</span>
<span class='line-number'>293</span>
<span class='line-number'>294</span>
<span class='line-number'>295</span>
<span class='line-number'>296</span>
<span class='line-number'>297</span>
<span class='line-number'>298</span>
<span class='line-number'>299</span>
<span class='line-number'>300</span>
<span class='line-number'>301</span>
<span class='line-number'>302</span>
<span class='line-number'>303</span>
<span class='line-number'>304</span>
<span class='line-number'>305</span>
<span class='line-number'>306</span>
<span class='line-number'>307</span>
<span class='line-number'>308</span>
<span class='line-number'>309</span>
<span class='line-number'>310</span>
<span class='line-number'>311</span>
<span class='line-number'>312</span>
<span class='line-number'>313</span>
<span class='line-number'>314</span>
<span class='line-number'>315</span>
<span class='line-number'>316</span>
<span class='line-number'>317</span>
<span class='line-number'>318</span>
<span class='line-number'>319</span>
<span class='line-number'>320</span>
<span class='line-number'>321</span>
<span class='line-number'>322</span>
<span class='line-number'>323</span>
<span class='line-number'>324</span>
<span class='line-number'>325</span>
<span class='line-number'>326</span>
<span class='line-number'>327</span>
<span class='line-number'>328</span>
<span class='line-number'>329</span>
<span class='line-number'>330</span>
<span class='line-number'>331</span>
<span class='line-number'>332</span>
<span class='line-number'>333</span>
<span class='line-number'>334</span>
<span class='line-number'>335</span>
<span class='line-number'>336</span>
<span class='line-number'>337</span>
<span class='line-number'>338</span>
<span class='line-number'>339</span>
<span class='line-number'>340</span>
<span class='line-number'>341</span>
<span class='line-number'>342</span>
<span class='line-number'>343</span>
<span class='line-number'>344</span>
<span class='line-number'>345</span>
<span class='line-number'>346</span>
<span class='line-number'>347</span>
<span class='line-number'>348</span>
<span class='line-number'>349</span>
<span class='line-number'>350</span>
<span class='line-number'>351</span>
<span class='line-number'>352</span>
<span class='line-number'>353</span>
<span class='line-number'>354</span>
<span class='line-number'>355</span>
<span class='line-number'>356</span>
<span class='line-number'>357</span>
<span class='line-number'>358</span>
<span class='line-number'>359</span>
<span class='line-number'>360</span>
<span class='line-number'>361</span>
<span class='line-number'>362</span>
<span class='line-number'>363</span>
<span class='line-number'>364</span>
<span class='line-number'>365</span>
<span class='line-number'>366</span>
<span class='line-number'>367</span>
<span class='line-number'>368</span>
<span class='line-number'>369</span>
<span class='line-number'>370</span>
<span class='line-number'>371</span>
<span class='line-number'>372</span>
<span class='line-number'>373</span>
<span class='line-number'>374</span>
<span class='line-number'>375</span>
<span class='line-number'>376</span>
<span class='line-number'>377</span>
<span class='line-number'>378</span>
<span class='line-number'>379</span>
<span class='line-number'>380</span>
<span class='line-number'>381</span>
<span class='line-number'>382</span>
<span class='line-number'>383</span>
<span class='line-number'>384</span>
<span class='line-number'>385</span>
<span class='line-number'>386</span>
<span class='line-number'>387</span>
<span class='line-number'>388</span>
<span class='line-number'>389</span>
<span class='line-number'>390</span>
<span class='line-number'>391</span>
<span class='line-number'>392</span>
<span class='line-number'>393</span>
<span class='line-number'>394</span>
<span class='line-number'>395</span>
<span class='line-number'>396</span>
<span class='line-number'>397</span>
<span class='line-number'>398</span>
<span class='line-number'>399</span>
<span class='line-number'>400</span>
<span class='line-number'>401</span>
<span class='line-number'>402</span>
<span class='line-number'>403</span>
<span class='line-number'>404</span>
<span class='line-number'>405</span>
<span class='line-number'>406</span>
<span class='line-number'>407</span>
<span class='line-number'>408</span>
<span class='line-number'>409</span>
<span class='line-number'>410</span>
<span class='line-number'>411</span>
<span class='line-number'>412</span>
<span class='line-number'>413</span>
<span class='line-number'>414</span>
<span class='line-number'>415</span>
<span class='line-number'>416</span>
<span class='line-number'>417</span>
<span class='line-number'>418</span>
<span class='line-number'>419</span>
<span class='line-number'>420</span>
<span class='line-number'>421</span>
<span class='line-number'>422</span>
<span class='line-number'>423</span>
<span class='line-number'>424</span>
<span class='line-number'>425</span>
<span class='line-number'>426</span>
<span class='line-number'>427</span>
<span class='line-number'>428</span>
<span class='line-number'>429</span>
<span class='line-number'>430</span>
<span class='line-number'>431</span>
<span class='line-number'>432</span>
<span class='line-number'>433</span>
<span class='line-number'>434</span>
<span class='line-number'>435</span>
<span class='line-number'>436</span>
<span class='line-number'>437</span>
<span class='line-number'>438</span>
<span class='line-number'>439</span>
<span class='line-number'>440</span>
<span class='line-number'>441</span>
<span class='line-number'>442</span>
<span class='line-number'>443</span>
<span class='line-number'>444</span>
<span class='line-number'>445</span>
<span class='line-number'>446</span>
<span class='line-number'>447</span>
<span class='line-number'>448</span>
<span class='line-number'>449</span>
<span class='line-number'>450</span>
<span class='line-number'>451</span>
<span class='line-number'>452</span>
<span class='line-number'>453</span>
<span class='line-number'>454</span>
<span class='line-number'>455</span>
<span class='line-number'>456</span>
<span class='line-number'>457</span>
<span class='line-number'>458</span>
<span class='line-number'>459</span>
<span class='line-number'>460</span>
<span class='line-number'>461</span>
<span class='line-number'>462</span>
<span class='line-number'>463</span>
<span class='line-number'>464</span>
<span class='line-number'>465</span>
<span class='line-number'>466</span>
<span class='line-number'>467</span>
<span class='line-number'>468</span>
<span class='line-number'>469</span>
<span class='line-number'>470</span>
<span class='line-number'>471</span>
<span class='line-number'>472</span>
<span class='line-number'>473</span>
<span class='line-number'>474</span>
<span class='line-number'>475</span>
<span class='line-number'>476</span>
<span class='line-number'>477</span>
<span class='line-number'>478</span>
<span class='line-number'>479</span>
<span class='line-number'>480</span>
<span class='line-number'>481</span>
<span class='line-number'>482</span>
<span class='line-number'>483</span>
<span class='line-number'>484</span>
<span class='line-number'>485</span>
<span class='line-number'>486</span>
<span class='line-number'>487</span>
<span class='line-number'>488</span>
<span class='line-number'>489</span>
<span class='line-number'>490</span>
<span class='line-number'>491</span>
<span class='line-number'>492</span>
<span class='line-number'>493</span>
<span class='line-number'>494</span>
<span class='line-number'>495</span>
<span class='line-number'>496</span>
<span class='line-number'>497</span>
<span class='line-number'>498</span>
<span class='line-number'>499</span>
<span class='line-number'>500</span>
<span class='line-number'>501</span>
<span class='line-number'>502</span>
<span class='line-number'>503</span>
<span class='line-number'>504</span>
<span class='line-number'>505</span>
<span class='line-number'>506</span>
<span class='line-number'>507</span>
<span class='line-number'>508</span>
<span class='line-number'>509</span>
<span class='line-number'>510</span>
<span class='line-number'>511</span>
<span class='line-number'>512</span>
<span class='line-number'>513</span>
<span class='line-number'>514</span>
<span class='line-number'>515</span>
<span class='line-number'>516</span>
<span class='line-number'>517</span>
<span class='line-number'>518</span>
<span class='line-number'>519</span>
<span class='line-number'>520</span>
<span class='line-number'>521</span>
<span class='line-number'>522</span>
<span class='line-number'>523</span>
<span class='line-number'>524</span>
<span class='line-number'>525</span>
<span class='line-number'>526</span>
<span class='line-number'>527</span>
<span class='line-number'>528</span>
<span class='line-number'>529</span>
<span class='line-number'>530</span>
<span class='line-number'>531</span>
<span class='line-number'>532</span>
<span class='line-number'>533</span>
<span class='line-number'>534</span>
<span class='line-number'>535</span>
<span class='line-number'>536</span>
<span class='line-number'>537</span>
<span class='line-number'>538</span>
<span class='line-number'>539</span>
<span class='line-number'>540</span>
<span class='line-number'>541</span>
<span class='line-number'>542</span>
<span class='line-number'>543</span>
<span class='line-number'>544</span>
<span class='line-number'>545</span>
<span class='line-number'>546</span>
<span class='line-number'>547</span>
<span class='line-number'>548</span>
<span class='line-number'>549</span>
<span class='line-number'>550</span>
<span class='line-number'>551</span>
<span class='line-number'>552</span>
<span class='line-number'>553</span>
<span class='line-number'>554</span>
<span class='line-number'>555</span>
<span class='line-number'>556</span>
<span class='line-number'>557</span>
<span class='line-number'>558</span>
<span class='line-number'>559</span>
<span class='line-number'>560</span>
<span class='line-number'>561</span>
<span class='line-number'>562</span>
<span class='line-number'>563</span>
<span class='line-number'>564</span>
<span class='line-number'>565</span>
<span class='line-number'>566</span>
<span class='line-number'>567</span>
<span class='line-number'>568</span>
<span class='line-number'>569</span>
<span class='line-number'>570</span>
<span class='line-number'>571</span>
<span class='line-number'>572</span>
<span class='line-number'>573</span>
<span class='line-number'>574</span>
<span class='line-number'>575</span>
<span class='line-number'>576</span>
<span class='line-number'>577</span>
<span class='line-number'>578</span>
<span class='line-number'>579</span>
<span class='line-number'>580</span>
<span class='line-number'>581</span>
<span class='line-number'>582</span>
<span class='line-number'>583</span>
<span class='line-number'>584</span>
<span class='line-number'>585</span>
<span class='line-number'>586</span>
<span class='line-number'>587</span>
<span class='line-number'>588</span>
<span class='line-number'>589</span>
<span class='line-number'>590</span>
<span class='line-number'>591</span>
<span class='line-number'>592</span>
<span class='line-number'>593</span>
<span class='line-number'>594</span>
<span class='line-number'>595</span>
<span class='line-number'>596</span>
<span class='line-number'>597</span>
<span class='line-number'>598</span>
<span class='line-number'>599</span>
<span class='line-number'>600</span>
<span class='line-number'>601</span>
<span class='line-number'>602</span>
<span class='line-number'>603</span>
<span class='line-number'>604</span>
<span class='line-number'>605</span>
<span class='line-number'>606</span>
<span class='line-number'>607</span>
<span class='line-number'>608</span>
<span class='line-number'>609</span>
<span class='line-number'>610</span>
<span class='line-number'>611</span>
<span class='line-number'>612</span>
<span class='line-number'>613</span>
<span class='line-number'>614</span>
<span class='line-number'>615</span>
<span class='line-number'>616</span>
<span class='line-number'>617</span>
<span class='line-number'>618</span>
<span class='line-number'>619</span>
<span class='line-number'>620</span>
<span class='line-number'>621</span>
<span class='line-number'>622</span>
<span class='line-number'>623</span>
<span class='line-number'>624</span>
<span class='line-number'>625</span>
<span class='line-number'>626</span>
<span class='line-number'>627</span>
<span class='line-number'>628</span>
<span class='line-number'>629</span>
<span class='line-number'>630</span>
<span class='line-number'>631</span>
<span class='line-number'>632</span>
<span class='line-number'>633</span>
<span class='line-number'>634</span>
<span class='line-number'>635</span>
<span class='line-number'>636</span>
<span class='line-number'>637</span>
<span class='line-number'>638</span>
<span class='line-number'>639</span>
<span class='line-number'>640</span>
<span class='line-number'>641</span>
<span class='line-number'>642</span>
<span class='line-number'>643</span>
<span class='line-number'>644</span>
<span class='line-number'>645</span>
<span class='line-number'>646</span>
<span class='line-number'>647</span>
<span class='line-number'>648</span>
<span class='line-number'>649</span>
<span class='line-number'>650</span>
<span class='line-number'>651</span>
<span class='line-number'>652</span>
<span class='line-number'>653</span>
<span class='line-number'>654</span>
<span class='line-number'>655</span>
<span class='line-number'>656</span>
<span class='line-number'>657</span>
<span class='line-number'>658</span>
<span class='line-number'>659</span>
<span class='line-number'>660</span>
<span class='line-number'>661</span>
<span class='line-number'>662</span>
<span class='line-number'>663</span>
<span class='line-number'>664</span>
<span class='line-number'>665</span>
<span class='line-number'>666</span>
<span class='line-number'>667</span>
<span class='line-number'>668</span>
<span class='line-number'>669</span>
<span class='line-number'>670</span>
<span class='line-number'>671</span>
<span class='line-number'>672</span>
<span class='line-number'>673</span>
<span class='line-number'>674</span>
<span class='line-number'>675</span>
<span class='line-number'>676</span>
<span class='line-number'>677</span>
<span class='line-number'>678</span>
<span class='line-number'>679</span>
<span class='line-number'>680</span>
<span class='line-number'>681</span>
<span class='line-number'>682</span>
<span class='line-number'>683</span>
<span class='line-number'>684</span>
<span class='line-number'>685</span>
<span class='line-number'>686</span>
<span class='line-number'>687</span>
<span class='line-number'>688</span>
<span class='line-number'>689</span>
<span class='line-number'>690</span>
<span class='line-number'>691</span>
<span class='line-number'>692</span>
<span class='line-number'>693</span>
<span class='line-number'>694</span>
<span class='line-number'>695</span>
<span class='line-number'>696</span>
<span class='line-number'>697</span>
<span class='line-number'>698</span>
<span class='line-number'>699</span>
<span class='line-number'>700</span>
<span class='line-number'>701</span>
<span class='line-number'>702</span>
<span class='line-number'>703</span>
<span class='line-number'>704</span>
<span class='line-number'>705</span>
<span class='line-number'>706</span>
<span class='line-number'>707</span>
<span class='line-number'>708</span>
<span class='line-number'>709</span>
<span class='line-number'>710</span>
<span class='line-number'>711</span>
<span class='line-number'>712</span>
<span class='line-number'>713</span>
<span class='line-number'>714</span>
<span class='line-number'>715</span>
<span class='line-number'>716</span>
<span class='line-number'>717</span>
<span class='line-number'>718</span>
<span class='line-number'>719</span>
<span class='line-number'>720</span>
<span class='line-number'>721</span>
<span class='line-number'>722</span>
<span class='line-number'>723</span>
<span class='line-number'>724</span>
<span class='line-number'>725</span>
<span class='line-number'>726</span>
<span class='line-number'>727</span>
<span class='line-number'>728</span>
<span class='line-number'>729</span>
<span class='line-number'>730</span>
<span class='line-number'>731</span>
<span class='line-number'>732</span>
<span class='line-number'>733</span>
<span class='line-number'>734</span>
<span class='line-number'>735</span>
<span class='line-number'>736</span>
<span class='line-number'>737</span>
<span class='line-number'>738</span>
<span class='line-number'>739</span>
<span class='line-number'>740</span>
<span class='line-number'>741</span>
<span class='line-number'>742</span>
<span class='line-number'>743</span>
<span class='line-number'>744</span>
<span class='line-number'>745</span>
<span class='line-number'>746</span>
<span class='line-number'>747</span>
<span class='line-number'>748</span>
<span class='line-number'>749</span>
<span class='line-number'>750</span>
<span class='line-number'>751</span>
<span class='line-number'>752</span>
<span class='line-number'>753</span>
<span class='line-number'>754</span>
<span class='line-number'>755</span>
<span class='line-number'>756</span>
<span class='line-number'>757</span>
<span class='line-number'>758</span>
<span class='line-number'>759</span>
<span class='line-number'>760</span>
<span class='line-number'>761</span>
<span class='line-number'>762</span>
<span class='line-number'>763</span>
<span class='line-number'>764</span>
<span class='line-number'>765</span>
<span class='line-number'>766</span>
<span class='line-number'>767</span>
<span class='line-number'>768</span>
<span class='line-number'>769</span>
<span class='line-number'>770</span>
<span class='line-number'>771</span>
<span class='line-number'>772</span>
<span class='line-number'>773</span>
<span class='line-number'>774</span>
<span class='line-number'>775</span>
<span class='line-number'>776</span>
<span class='line-number'>777</span>
<span class='line-number'>778</span>
<span class='line-number'>779</span>
<span class='line-number'>780</span>
<span class='line-number'>781</span>
<span class='line-number'>782</span>
<span class='line-number'>783</span>
<span class='line-number'>784</span>
<span class='line-number'>785</span>
<span class='line-number'>786</span>
<span class='line-number'>787</span>
<span class='line-number'>788</span>
<span class='line-number'>789</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>static int__init early_vmalloc(char &lt;em&gt;arg)
</span><span class='line'>{
</span><span class='line'>    /&lt;/em&gt;cmdline中的vmalloc会被解析到vmlloc_reserve中。*/
</span><span class='line'>    unsigned long vmalloc_reserve = memparse(arg, NULL);&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    /*小于16M则用16M。*/
</span><span class='line'>if (vmalloc_reserve &lt; SZ_16M) {
</span><span class='line'>    vmalloc_reserve = SZ_16M;
</span><span class='line'>    printk(KERN_WARNING
</span><span class='line'>            "vmalloc area too small, limiting to %luMB\n",
</span><span class='line'>            vmalloc_reserve &gt;&gt; 20);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>/*大于可用虚拟地址内存则使用可用地址部分再减去32M。*/
</span><span class='line'>if (vmalloc_reserve &gt; VMALLOC_END - (PAGE_OFFSET + SZ_32M)) {
</span><span class='line'>    vmalloc_reserve = VMALLOC_END - (PAGE_OFFSET + SZ_32M);
</span><span class='line'>    printk(KERN_WARNING
</span><span class='line'>            "vmalloc area is too big, limiting to %luMB\n",
</span><span class='line'>            vmalloc_reserve &gt;&gt; 20);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>/*计算偏移起始地址。*/
</span><span class='line'>vmalloc_min = (void *)(VMALLOC_END - vmalloc_reserve);
</span><span class='line'>return 0;
</span><span class='line'>}
</span><span class='line'>early_param("vmalloc",early_vmalloc);
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>vmalloc_min会影响arm_lowmem_limit,arm_lowmem_limit其实就是high_memory。因为此过程不是我们要分析的重点，如果有兴趣可分析kernel/arch/arm/mm/mmu.c中的sanity_check_meminfo()函数。
</span><span class='line'>
</span><span class='line'>所以，VMALLOC_START受到了hight_memory的影响而发生了变化，最终使得vmalloc size也变化了！
</span><span class='line'>
</span><span class='line'>##### 2. 开机默认分配：
</span><span class='line'>
</span><span class='line'>File: kernel/arch/arm/mm/mmu.c
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;static void * __initdata vmalloc_min =
</span><span class='line'>(void *)(VMALLOC_END - (240 &lt;&lt; 20) - VMALLOC_OFFSET);
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>当cmdline无vmalloc参数传进来的时候，early_vmalloc()函数也不会调用到，vmalloc_min的值就会被默认传进来了，默认是240M。
</span><span class='line'>
</span><span class='line'>后面的步骤和方法1一样了！
</span><span class='line'>
</span><span class='line'>开机log有memory layout 信息：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;[    0.000000] [cpuid: 0] Virtual kernelmemory layout:
</span><span class='line'>[    0.000000] [cpuid:0]     vector  : 0xffff0000 - 0xffff1000  (   4 kB)
</span><span class='line'>[    0.000000] [cpuid:0]     fixmap  : 0xfff00000 - 0xfffe0000   (896 kB)
</span><span class='line'>[    0.000000] [cpuid:0]     vmalloc : 0xf3000000 - 0xff000000   ( 192MB)
</span><span class='line'>[    0.000000] [cpuid:0]     lowmem  : 0xc0000000 - 0xf2800000   (808 MB)
</span><span class='line'>[    0.000000] [cpuid:0]     pkmap   : 0xbfe00000 -0xc0000000   (   2 MB)
</span><span class='line'>[    0.000000] [cpuid:0]     modules : 0xbf000000 - 0xbfe00000  (  14 MB)
</span><span class='line'>[    0.000000] [cpuid:0]       .text : 0xc0008000 -0xc0893034   (8749 kB)
</span><span class='line'>[    0.000000] [cpuid:0]       .init : 0xc0894000 -0xc08cdc00   ( 231 kB)
</span><span class='line'>[    0.000000] [cpuid:0]       .data : 0xc08ce000 -0xc09f8eb8   (1196 kB)
</span><span class='line'>[    0.000000] [cpuid:0]        .bss : 0xc0a78eec -0xc0f427a8   (4903 kB)
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;```&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;其中看到vmalloc为192MB , cmdline中使用vmllaoc就是200M。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Lowmem为地段内存部分，可见lowmem和vmalloc中间有8M空隙。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;Vmalloc该分配多大?&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Linux内核版本从3.2到3.3 默认的vmalloc size由128M 增大到了240M，3.4.0上的&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;修改Commit信息如下：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;To accommodate all static mappings on machines withpossible highmem usage, the default vmalloc area size is changed to 240 MB sothat VMALLOC_START is no higher than 0xf0000000 by default.&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;看其意思是因为开机的静态映射增加了，所以要扩大。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;另外3.2到3.3版本的一个重大变化是将android引入到主线内核中。我想增大vmalloc size到240M是基于此考虑吧。当然，各家厂商都也可以基于自己平台来动态修改size的。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;那么如何判断当前vmalloc size不足呢？&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;/proc/meminfo中有vmalloc信息:&lt;br/&gt;
</span><span class='line'>VmallocTotal:     540672 kB&lt;br/&gt;
</span><span class='line'>VmallocUsed:      165268 kB&lt;br/&gt;
</span><span class='line'>VmallocChunk:     118788kB&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;事实上这里的VmallocUsed只是表示已经被真正使用掉的vmalloc区域，但是区域之前的空隙也就是碎片没有被计算进去。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;所以，回到前面说的/proc/vmallocinfo，假设我们的vmalloc size就是200M。那么区域为0xf3000000- 0xff000000，从vmallocinfo中可以看到，前面大部分虚拟地址空间都用掉了，剩下0xfb600000到0xfefdc000这57M空间，假如申请了64M，那么就会失败了。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;开机分配使用掉vmalloc之后到底该剩余多少目前没有具体依据，一般来说1GB RAM可以设置为400~600M。&lt;/p&gt;
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[Linux-2.6.32 NUMA架构之内存和调度]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2015/06/02/kernel-mm-alloc-numa/"/&gt;
</span><span class='line'>&lt;updated&gt;2015-06-02T15:32:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2015/06/02/kernel-mm-alloc-numa&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;p&gt;&lt;a href="http://blog.chinaunix.net/uid-7295895-id-3076420.html"&gt;http://blog.chinaunix.net/uid-7295895-id-3076420.html&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Linux-2.6.32 NUMA架构之内存和调度&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  本文将以XLP832通过ICI互连形成的NUMA架构进行分析，主要包括内存管理和调度两方面，参考内核版本2.6.32.9；NUMA架构常见配置选项有：CONFIG_SMP, CONFIG_NUMA, CONFIG_NEED_MULTIPLE_NODES, CONFIG_NODES_SHIFT, CONFIG_SPARSEMEM, CONFIG_CGROUPS, CONFIG_CPUSETS, CONFIG_MIGRATION等。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;本文试图从原理上介绍，尽量避免涉及代码的实现细节。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;1 NUMA架构简介&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;NUMA(Non Uniform Memory Access)即非一致内存访问架构，市面上主要有X86_64(JASPER)和MIPS64(XLP)体系。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;1.1 概念&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;NUMA具有多个节点(Node)，每个节点可以拥有多个CPU(每个CPU可以具有多个核或线程)，节点内使用共有的内存控制器，因此节点的所有内存对于本节点的所有CPU都是等同的，而对于其它节点中的所有CPU都是不同的。节点可分为本地节点(Local Node)、邻居节点(Neighbour Node)和远端节点(Remote Node)三种类型。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;本地节点：对于某个节点中的所有CPU，此节点称为本地节点；&lt;br/&gt;
</span><span class='line'>邻居节点：与本地节点相邻的节点称为邻居节点；&lt;br/&gt;
</span><span class='line'>远端节点：非本地节点或邻居节点的节点，称为远端节点。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;邻居节点和远端节点，称作非本地节点(Off Node)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;CPU访问不同类型节点内存的速度是不相同的：本地节点&gt;邻居节点&gt;远端节点。访问本地节点的速度最快，访问远端节点的速度最慢，即访问速度与节点的距离有关，距离越远访问速度越慢，此距离称作Node Distance。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;常用的NUMA系统中：硬件设计已保证系统中所有的Cache是一致的(Cache Coherent, ccNUMA)；不同类型节点间的Cache同步时间不一样，会导致资源竞争不公平，对于某些特殊的应用，可以考虑使用FIFO Spinlock保证公平性。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;1.2 关键信息&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;1) 物理内存区域与Node号之间的映射关系；&lt;br/&gt;
</span><span class='line'>2) 各Node之间的Node Distance；&lt;br/&gt;
</span><span class='line'>3) 逻辑CPU号与Node号之间的映射关系。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;2 XLP832 NUMA初始化&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;首先需要完成1.2节中描述的3个关键信息的初始化。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;2.1 CPU和Node的关系&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;start_kernel()-&gt;setup_arch()-&gt;prom_init():&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    #ifdef CONFIG_NUMA
</span><span class='line'>    build_node_cpu_map();
</span><span class='line'>#endif
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;build_node_cpu_map()函数工作：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;a) 确定CPU与Node的相互关系，做法很简单：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    #define cpu_to_node(cpu)       (cpu &gt;&gt; 5)
</span><span class='line'>#define cpumask_of_node    (NODE_CPU_MASK(node)) /* node0:0~31; node1: 32~63 */
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;说明：XLP832每个节点有1个物理CPU，每个物理CPU有8个核，每个核有4个超线程，因此每个节点对应32个逻辑CPU，按节点依次展开。另外，实际物理存在的CPU数目是通过DTB传递给内核的；numa_node_id()可以获取当前CPU所处的Node号。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;b) 设置每个物理存在的节点的在线状态，具体是通过node_set_online()函数来设置全局变量&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;nodemask_t node_states[];&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这样，类似于CPU号，Node号也就具有如下功能宏：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>for_each_node(node);
</span><span class='line'>for_each_online_node(node);
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;详细可参考include/linux/nodemask.h&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;2.2 Node Distance确立&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;作用：建立buddy时用，可以依此来构建zonelist，以及zone relaim(zone_reclaim_mode)使用，详见后面的4.2.2节。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;2.3 内存区域与Node的关系&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;start_kernel()-&gt;setup_arch()-&gt;arch_mem_init-&gt;bootmem_init()-&gt;nlm_numa_bootmem_init():&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;nlm_get_dram_mapping();&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;XLP832上电后的默认memory-mapped物理地址空间分布：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-06-02-30.jpg" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  其中PCIE配置空间映射地址范围为[0x1800_0000, 0x1BFF_FFFF]，由寄存器ECFG_BASE和ECFG_LIMIT指定(注：但这2个寄存器本身是处于PCIE配置空间之中的)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;PCIE配置空间：&lt;br/&gt;
</span><span class='line'>  PCIE配置空间与memory-mapped物理地址的映射方式：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-06-02-31.jpg" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;XLP832实现了所有设备都位于虚拟总线0上，每个节点有8个设备，按节点依次排开。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;DRAM映射寄存器组：&lt;br/&gt;
</span><span class='line'>  每个节点都独立实现有几组不同类型的DRAM(每组有8个相同类型的)寄存器可以配置DRAM空间映射到物理地址空间中的基址和大小，以及所属的节点信息(这些寄存器的值事先会由bootloader设好)；这组寄存器位于虚拟总线0的设备0/8/16/24(依次对应每个节点的第一个设备号)的Function0(每个设备最多可定义8个Function，每个Function有着独立的PCIE 4KB的配置空间)的PCIE配置空间中(这个配置空间实现的是DRAM/Bridge控制器)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;本小节涉及到的3组不同类型的寄存器(注：按索引对应即DRAM_BAR&lt;n&gt;,DRAM_LIMIT&lt;n&gt;和 DRAM_NODE_TRANSLATION&lt;n&gt;描述一个内存区域属性)：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;第一组(DRAM空间映射物理空间基址)：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>DRAM_BAR0: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x54
</span><span class='line'>DRAM_BAR1: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x55
</span><span class='line'>DRAM_BAR2: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x56
</span><span class='line'>DRAM_BAR3: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x57
</span><span class='line'>DRAM_BAR4: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x58
</span><span class='line'>DRAM_BAR5: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x59
</span><span class='line'>DRAM_BAR6: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x5A
</span><span class='line'>DRAM_BAR7: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x5B
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;第二组(DRAM空间映射物理空间长度)：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>DRAM_LIMIT0: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x5C
</span><span class='line'>DRAM_LIMIT1: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x5D
</span><span class='line'>DRAM_LIMIT2: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x5E
</span><span class='line'>DRAM_LIMIT3: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x5F
</span><span class='line'>DRAM_LIMIT4: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x60
</span><span class='line'>DRAM_LIMIT5: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x61
</span><span class='line'>DRAM_LIMIT6: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x62
</span><span class='line'>DRAM_LIMIT7: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x63
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;第三组(节点相关)：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>DRAM_NODE_TRANSLATION0: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x64
</span><span class='line'>DRAM_NODE_TRANSLATION1: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x65
</span><span class='line'>DRAM_NODE_TRANSLATION2: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x66
</span><span class='line'>DRAM_NODE_TRANSLATION3: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x67
</span><span class='line'>DRAM_NODE_TRANSLATION4: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x68
</span><span class='line'>DRAM_NODE_TRANSLATION5: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x69
</span><span class='line'>DRAM_NODE_TRANSLATION6: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x6A
</span><span class='line'>DRAM_NODE_TRANSLATION7: PCIe Bus 0, Device 0/8/16/24, Function 0, Register 0x6B
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;根据上述的PCIE配置空间memory-mapped映射方式便可直接获取寄存器中的值，就可以建立各个节点中的所有内存区域(最多8个区域)信息。关于这些寄存器的使用可以参考“XLP® Processor Family Programming Reference Manual”的“Chapter 7 Memory and I/O Subsystem”。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;3 Bootmem初始化&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;bootmem_init()-&gt;…-&gt;init_bootmem_node()-&gt;init_bootmem_core():&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-06-02-32.jpg" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;每个节点拥有各自的bootmem管理(code&amp;data之前可以为空闲页面)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;4 Buddy初始化&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;初始化流程最后会设置全局struct node_active_region early_node_map[]用于初始化Buddy系统，for_each_online_node()遍历所有在线节点调用free_area_init_node()初始化，主要初始化每个zone的大小和所涉及页面的struct page结构(flags中初始化有所属zone和node信息，由set_page_links()函数设置)等。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;4.1 NUMA带来的变化&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;1) pglist_data
</span><span class='line'>&lt;code&gt;
</span><span class='line'>typedef struct pglist_data {
</span><span class='line'>    struct zone node_zones[MAX_NR_ZONES];
</span><span class='line'>    struct zonelist node_zonelists[MAX_ZONELISTS];
</span><span class='line'>    int nr_zones;
</span><span class='line'>    struct bootmem_data *bdata;
</span><span class='line'>    unsigned long node_start_pfn;
</span><span class='line'>    unsigned long node_present_pages; /* total number of physical pages */
</span><span class='line'>    unsigned long node_spanned_pages; /* total size of physical pagerange, including holes */
</span><span class='line'>    int node_id;
</span><span class='line'>    wait_queue_head_t kswapd_wait;
</span><span class='line'>    struct task_struct *kswapd;
</span><span class='line'>    int kswapd_max_order;
</span><span class='line'>} pg_data_t;
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;a)上节的bootmem结构的描述信息存放在NODE_DATA(node)-&gt; bdata中；NODE_DATA(i)宏返回节点i的struct pglist_data结构，需要在架构相关的mmzone.h中实现；&lt;br/&gt;
</span><span class='line'>b) #define MAX_ZONELISTS 2，请参考后面的“zonelist初始化”。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;2) zone
</span><span class='line'>&lt;code&gt;
</span><span class='line'>struct zone {
</span><span class='line'>#ifdef CONFIG_NUMA
</span><span class='line'>    int node;
</span><span class='line'>    /*
</span><span class='line'>     * zone reclaim becomes active if more unmapped pages exist.
</span><span class='line'>     */
</span><span class='line'>    unsigned long        min_unmapped_pages;
</span><span class='line'>    unsigned long        min_slab_pages;
</span><span class='line'>    struct per_cpu_pageset   *pageset[NR_CPUS];
</span><span class='line'>#else
</span><span class='line'>    … …
</span><span class='line'>};
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;a)最终调用kmalloc_node()为pageset成员在每个CPU的对应的内存节点分配内存；&lt;br/&gt;
</span><span class='line'>b)min_unmapped_pages 对应/proc/sys/vm/min_unmapped_ratio，默认值为1；&lt;br/&gt;
</span><span class='line'>  min_slab_pages对应/proc/sys/vm/min_slab_ratio，默认值为5；&lt;br/&gt;
</span><span class='line'>  作用：当剩余可回收的非文件映射和SLAB页面超过这2个值时，才激活当前zone回收；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;c) 增加了zone对应的节点号。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;4.2 zonelist初始化&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  本节讲述zonelist的构建方式，实现位于start_kernel()-&gt;build_all_zonelists()中，zonelist的组织方式非常关键(这一点与以前的2.6.21内核版本不一样，2.6.32组织得更清晰)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;4.2.1 zonelist order&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;NUMA系统中存在多个节点，每个节点对应一个struct pglist_data结构，此结构中可以包含多个zone，如：ZONE_DMA, ZONE_NORMAL，这样就产生几种排列顺序，以2个节点2个zone为例(zone从高到低排列, ZONE_DMA0表示节点0的ZONE_DMA，其它类似)：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;a) Legacy方式&lt;br/&gt;
</span><span class='line'>&lt;img src="/images/kernel/2015-06-02-33.jpg" alt="" /&gt;&lt;br/&gt;
</span><span class='line'>  每个节点只排列自己的zone；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;b)Node方式&lt;br/&gt;
</span><span class='line'>&lt;img src="/images/kernel/2015-06-02-34.jpg" alt="" /&gt;&lt;br/&gt;
</span><span class='line'>  按节点顺序依次排列，先排列本地节点的所有zone，再排列其它节点的所有zone。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;c) Zone方式&lt;br/&gt;
</span><span class='line'>&lt;img src="/images/kernel/2015-06-02-35.jpg" alt="" /&gt;&lt;br/&gt;
</span><span class='line'>  按zone类型从高到低依次排列各节点的同相类型zone。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;可通过启动参数“numa_zonelist_order”来配置zonelist order，内核定义了3种配置：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>#define ZONELIST_ORDER_DEFAULT  0 /* 智能选择Node或Zone方式 */
</span><span class='line'>#define ZONELIST_ORDER_NODE     1 /* 对应Node方式 */
</span><span class='line'>#define ZONELIST_ORDER_ZONE     2 /* 对应Zone方式 */
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;默认配置为ZONELIST_ORDER_DEFAULT，由内核通过一个算法来判断选择Node或Zone方式，算法思想：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;a) alloc_pages()分配内存是按照ZONE从高到低的顺序进行的，例如上节“Node方式”的图示中，从ZONE_NORMAL0中分配内存时，ZONE_NORMAL0中无内存时将落入较低的ZONE_DMA0中分配，这样当ZONE_DMA0比较小的时候，很容易将ZONE_DMA0中的内存耗光，这样是很不理智的，因为还有更好的分配方式即从ZONE_NORMAL1中分配；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;b) 内核会检测各ZONE的页面数来选择Zone组织方式，当ZONE_DMA很小时，选择ZONELIST_ORDER_DEFAULT时，内核将倾向于选择ZONELIST_ORDER_ZONE方式，否则选择ZONELIST_ORDER_NODE方式。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;另外，可以通过/proc/sys/vm/numa_zonelist_order动态改变zonelist order的分配方式。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;4.2.2 Node Distance&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;上节中的例子是以2个节点为例，如果有&gt;2个节点存在，就需要考虑不同节点间的距离来安排节点，例如以4个节点2个ZONE为例，各节点的布局(如4个XLP832物理CPU级联)值如下：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-06-02-36.jpg" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;上图中，Node0和Node2的Node Distance为25，Node1和Node3的Node Distance为25，其它的Node Distance为15。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h6&gt;4.2.2.1 优先进行Zone Reclaim&lt;/h6&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;另外，当Node Distance超过20的时候，内核会在某个zone分配内存不足的时候，提前激活本zone的内存回收工作，由全局变量zone_reclaim_mode控制，build_zonelists()中：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    /*
</span><span class='line'> * If another node is sufficiently far away then it is better
</span><span class='line'> * to reclaim pages in a zone before going off node.
</span><span class='line'> */
</span><span class='line'>if (distance &gt; RECLAIM_DISTANCE)
</span><span class='line'>    zone_reclaim_mode = 1;
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;通过/proc/sys/vm/zone_reclaim_mode可以动态调整zone_reclaim_mode的值来控制回收模式，含义如下：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>#define RECLAIM_OFF    0
</span><span class='line'>#define RECLAIM_ZONE  (1&lt;&lt;0)     /* Run shrink_inactive_list on the zone */
</span><span class='line'>#define RECLAIM_WRITE (1&lt;&lt;1)     /* Writeout pages during reclaim */
</span><span class='line'>#define RECLAIM_SWAP  (1&lt;&lt;2)     /* Swap pages out during reclaim */
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h6&gt;4.2.2.2 影响zonelist方式&lt;/h6&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;采用Node方式组织的zonelist为：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-06-02-37.jpg" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  即各节点按照与本节点的Node Distance距离大小来排序，以达到更优的内存分配。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;4.2.3 zonelist[2]&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;配置NUMA后，每个节点将关联2个zonelist：&lt;br/&gt;
</span><span class='line'>  1) zonelist[0]中存放以Node方式或Zone方式组织的zonelist，包括所有节点的zone；&lt;br/&gt;
</span><span class='line'>  2) zonelist[1]中只存放本节点的zone即Legacy方式；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;zonelist[1]用来实现仅从节点自身zone中的内存分配(参考&lt;code&gt;__GFP_THISNODE&lt;/code&gt;标志)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;5 SLAB初始化&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;配置NUMA后对SLAB(本文不涉及SLOB或SLUB)的初始化影响不大，只是在分配一些变量采用类似Buddy系统的per_cpu_pageset(单面页缓存)在CPU本地节点进行内存分配。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;5.1 NUMA带来的变化&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    struct kmem_cache {
</span><span class='line'>    struct array_cache *array[NR_CPUS];
</span><span class='line'>    … …
</span><span class='line'>    struct kmem_list3 *nodelists[MAX_NUMNODES];
</span><span class='line'>};
</span><span class='line'>
</span><span class='line'>struct kmem_list3 {
</span><span class='line'>    … …
</span><span class='line'>    struct array_cache *shared;    /* shared per node */
</span><span class='line'>    struct array_cache **alien;    /* on other nodes */
</span><span class='line'>    … …
</span><span class='line'>};
</span><span class='line'>
</span><span class='line'>struct slab {
</span><span class='line'>    … …
</span><span class='line'>    unsigned short nodeid;
</span><span class='line'>    … …
</span><span class='line'>};
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;上面的4种类型的指针变量在SLAB初始化完毕后将改用kmalloc_node()分配的内存。具体实现请参考enable_cpucache()，此函数最终调用alloc_arraycache()和alloc_kmemlist()来分配这些变量代表的空间。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  nodelists[MAX_NUMNODES]存放的是所有节点对应的相关数据，本文称作SLAB节点。每个节点拥有各自的数据；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;注：有些非NUMA系统比如非连续内存系统可能根据不同的内存区域定义多个节点(实际上Node Distance都是0即物理内存访问速度相同)，所以这些变量并没有采用CONFIG_NUMA宏来控制，本文暂称为NUMA带来的变化。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;5.2 SLAB缓存&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;配置NUMA后，SLAB将有三种类型的缓存：本地缓存(当前CPU的缓存)，共享缓存(节点内的缓存)和外部缓存(节点间的缓存)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SLAB系统分配对象时，先从本地缓存中查找，如果本地缓存为空，则将共享缓存中的缓存搬运本地缓存中，重新从本地缓存中分配；如果共享缓存为空，则从SLAB中进行分配；如果SLAB中已经无空闲对象，则分配新的SLAB后重新分配本地缓存。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SLAB系统释放对象时，先不归还给SLAB (简化分配流程，也可充分利用CPU Cache)，如果是同节点的SLAB对象先放入本地缓存中，如果本地缓存溢出(满)，则转移一部分(以batch为单位)至共享缓存中；如果是跨节点释放，则先放入外部缓存中，如果外部缓存溢出，则转移一部分至共享缓存中，以供后续分配时使用；如果共享缓存溢出，则调用free_block()函数释放溢出的缓存对象。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;关于这三种类型缓存的大小以及参数设置，不在本文的讨论范围。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;本地缓存&lt;br/&gt;
</span><span class='line'>  kmem_cache-&gt; array[] 中缓存每个CPU的SLAB cached objects；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;共享缓存&lt;br/&gt;
</span><span class='line'>  kmem_list3[]-&gt;shared(如果存在shared缓存)中缓存与当前CPU同节点的所有CPU (如XLP832 NUMA系统中的Node0包含为CPU0~CPU31) 本地缓存溢出的缓存，详细实现请参考cache_flusharray()；另外，大对象SLAB不存在共享缓存。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;外部缓存&lt;br/&gt;
</span><span class='line'>  kmem_list3[]-&gt;alien中存放其它节点的SLAB cached objects，当在某个节点上分配的SLAB 的object在另外一个节点上被释放的时候(即slab-&gt;nodeid与numa_node_id()当前节点不相等时)，将加入到对象所在节点的alien缓存中(如果不存在此alien缓存，此对象不会被缓存，而是直接释放给此对象所属SLAB)，否则加入本地缓存或共享缓存(本地缓存溢出且存在shared缓存时)；当alien缓存满的时候，会调用cache_free_alien()搬迁至shared缓存中(如果不存在shared缓存，直接释放给SLAB)；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;slab-&gt;nodeid记录本SLAB内存块(若干个页面)所在的节点。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;示例&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;例如2个节点，CPU0~31位于Node0，CPU32~CPU63位于Node1：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;64个(依次对应于CPU0~CPU63)本地缓存&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;array[0~31]:在Node0分配“array_cache结构+cached Objs指针”；&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;array[32~63]:在Node1分配“array_cache结构+cached Objs指针”；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;2个SLAB节点&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[0]:在Node0分配“kmem_list3结构”；&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[1]:在Node1分配“kmem_list3结构”；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SLAB节点0(CPU0~CPU31)共享缓存和外部缓存alien[1]&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[0]-&gt;shared:在Node0分配“array_cache结构+cached Objs指针”；&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[0]-&gt;alien:在Node0分配“节点数&lt;em&gt;sizeof(void&lt;/em&gt;)”；&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[0]-&gt;alien[0]:置为NULL；&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[0]-&gt;alien[1]:在Node0分配“array_cache结构+cached Objs指针”；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SLAB节点1(CPU32~CPU63)共享缓存和外部缓存alien[0]&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[1]-&gt;shared:在Node1分配“array_cache结构+cached Objs指针”；&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[1]-&gt;alien:在Node1分配“节点数&lt;em&gt;sizeof(void&lt;/em&gt;)”；&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[1]-&gt;alien[0]:在Node1分配“array_cache结构+cached Objs指针”；&lt;br/&gt;
</span><span class='line'>kmem_cache-&gt;nodelists[1]-&gt;alien[1]:置为NULL；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;另外，可以用内核启动参数“use_alien_caches”来控制是否开启alien缓存：默认值为1，当系统中的节点数目为1时，use_alien_caches初始化为0；use_alien_caches目的是用于某些多节点非连续内存(访问速度相同)的非NUMA系统。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;由上可见，随着节点个数的增加，SLAB明显会开销越来越多的缓存，这也是SLUB涎生的一个重要原因。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;5.3 __GFP_THISNODE&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SLAB在某个节点创建新的SLAB时，都会置&lt;code&gt;__GFP_THISNODE&lt;/code&gt;标记向Buddy系统提交页面申请，Buddy系统中看到此标记，选用申请节点的Legacy zonelist[1]，仅从申请节点的zone中分配内存，并且不会走内存不足流程，也不会重试或告警，这一点需要引起注意。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SLAB在申请页面的时候会置GFP_THISNODE标记后调用cache_grow()来增长SLAB；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;GFP_THISNODE定义如下：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>#ifdef CONFIG_NUMA
</span><span class='line'>#define GFP_THISNODE     (__GFP_THISNODE | __GFP_NOWARN | __GFP_NORETRY)
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;6 调度初始化&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;配置NUMA后负载均衡会多一层NUMA调度域，根据需要在topology.h中定义，示例：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>#define SD_NODE_INIT (struct sched_domain) {                 \
</span><span class='line'>    .parent             = NULL,                              \
</span><span class='line'>    .child              = NULL,                              \
</span><span class='line'>    .groups             = NULL,                              \
</span><span class='line'>    .min_interval       = 8,                                 \
</span><span class='line'>    .max_interval       = 32,                                \
</span><span class='line'>    .busy_factor        = 32,                                \
</span><span class='line'>    .imbalance_pct      = 125,                               \
</span><span class='line'>    .cache_nice_tries   = 1,                                 \
</span><span class='line'>    .flags              = SD_LOAD_BALANCE | SD_BALANCE_EXEC, \
</span><span class='line'>    .last_balance       = jiffies,                           \
</span><span class='line'>    .balance_interval   = 1,                                 \
</span><span class='line'>    .nr_balance_failed  = 0,                                 \
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  顺便提一下，2.6.32对于实时任务不走负载均衡流程，采用了全局优先级调度的思想，保证实时任务的及时运行；这样的做法同时也解决了低版本内核在处理同一个逻辑CPU上相同最高优先级实时任务的负载均衡的时延。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;7 NUMA内存分配&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Zonelist[2]组织方式在NUMA内存分配过程中起着至关重要的作用，它决定了整个页面在不同节点间的申请顺序和流程。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;7.1显式分配&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;显式分配即指定节点的分配函数，此类基础分配函数主要有2个：Buddy系统的   alloc_pages_node()和SLAB系统的kmem_cache_alloc_node()，其它的函数都可以从这2个派生出来。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;例如，kmalloc_node()最终调用kmem_cache_alloc_node()进行分配。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;7.1.1 Buddy显式分配&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;alloc_pages_node(node, gfp_flags, order)分配流程：&lt;br/&gt;
</span><span class='line'>  1) 如果node小于0，node取本地节点号(node = numa_node_id())；&lt;br/&gt;
</span><span class='line'>  2) NODE_DATA(node)得到node对应的struct pglist_data结构，从而得到zonelist[2]；&lt;br/&gt;
</span><span class='line'>  3) 如果gfp_flags含有&lt;code&gt;__GFP_THISNODE&lt;/code&gt;标志，仅在此节点分配内存，使用node节点的Legacy zonelist[1]，否则使用其包含所有节点zone的zonelist[0] (见4.2.2.3节)；&lt;br/&gt;
</span><span class='line'>  4) 遍历确定出来的zonelist结构中包含的每一个符合要求的zone，gfp_flags指定了本次分配中的最高的zone，如&lt;code&gt;__GFP_HIGHMEM&lt;/code&gt;表示最高的zone为ZONE_HIGH；&lt;br/&gt;
</span><span class='line'>  5) 分配结束。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;7.1.2 SLAB显式分配&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;kmem_cache_alloc_node(cachep, gfp_flags, node)分配流程：&lt;br/&gt;
</span><span class='line'>  1) 如果node值为-1，node取本地节点号(node = numa_node_id())；&lt;br/&gt;
</span><span class='line'>  2) 如果node &lt; -1，则执行fall back行为，此行为与用户策略有关，有点类似隐式分配：&lt;br/&gt;
</span><span class='line'>a) 根据用户策略(包括CPUSET和内存策略)依次选取节点，根据gfp_flags选取合适的zonelist进行分配；&lt;br/&gt;
</span><span class='line'>b) 如果内存不足分配失败，则跳过内存策略直接进行隐式Buddy页面分配(仍受CPUSET的限定，关于CPUSET和内存策略后面会介绍)，最终构建成新的SLAB并完成本次分配；转5)；&lt;br/&gt;
</span><span class='line'>  3) 如果node是正常节点号，则先在node节点上根据gfp_flags选取合适的zonelist进行分配；&lt;br/&gt;
</span><span class='line'>  4) 如果3)中node节点内存不足分配失败，转2) a)执行fall back行为。&lt;br/&gt;
</span><span class='line'>  5) 分配结束。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;注：fall back行为指的是某个节点上内存不足时会落到此节点的zonelist[0]中定义的其它节点zone分配。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;7.1.3 设备驱动&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;配置CONFIG_NUMA后，设备会关联一个NUMA节点信息，struct device结构中会多一个numa_node字段记录本设备所在的节点，这个结构嵌套在各种类型的驱动中，如struct net_device结构。
</span><span class='line'>&lt;code&gt;
</span><span class='line'>struct device {
</span><span class='line'>    … …
</span><span class='line'>    #ifdef CONFIG_NUMA
</span><span class='line'>        int          numa_node;    /* NUMA node this device is close to */
</span><span class='line'>    #endif
</span><span class='line'>    … …
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;附&lt;code&gt;__netdev_alloc_skb()&lt;/code&gt;的实现：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    struct sk_buff *__netdev_alloc_skb(struct net_device *dev,
</span><span class='line'>        unsigned int length, gfp_t gfp_mask)
</span><span class='line'>{
</span><span class='line'>    int node = dev-&gt;dev.parent ? dev_to_node(dev-&gt;dev.parent) : -1;
</span><span class='line'>    struct sk_buff *skb;
</span><span class='line'>
</span><span class='line'>    skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask, 0, node);
</span><span class='line'>    if (likely(skb)) {
</span><span class='line'>        skb_reserve(skb, NET_SKB_PAD);
</span><span class='line'>        skb-&gt;dev = dev;
</span><span class='line'>    }
</span><span class='line'>    return skb;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;code&gt;__alloc_skb()&lt;/code&gt;最终调用kmem_cache_alloc_node()和kmalloc_node()在此node上分配内存。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;7.2 隐式分配和内存策略&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;隐式分配即不指定节点的分配函数，此类基础分配函数主要有2个：Buddy系统的   alloc_pages()和SLAB系统的kmem_cache_alloc()，其它的函数都可以从这2个派生出来。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;隐式分配涉及到NUMA内存策略(Memory Policy)，内核定义了四种内存策略。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;注：隐式分配还涉及到CPUSET，本文后面会介绍。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;7.2.1 内存策略&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;内核mm/mempolicy.c中实现了NUMA内存的四种内存分配策略：MPOL_DEFAULT, MPOL_PREFERRED, MPOL_INTERLEAVE和MPOL_BIND，内存策略会从父进程继承。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;MPOL_DEFAULT：使用本地节点的zonelist；&lt;br/&gt;
</span><span class='line'>MPOL_PREFERRED：使用指定节点的zonelist；&lt;br/&gt;
</span><span class='line'>MPOL_BIND： 设置一个节点集合，只能从这个集合中节点的zone申请内存：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  1)无&lt;code&gt;__GFP_THISNODE&lt;/code&gt;申请标记，使用本地节点的zonelist[0]；&lt;br/&gt;
</span><span class='line'>  2)置有&lt;code&gt;__GFP_THISNODE&lt;/code&gt;申请标记，如果本地节点：&lt;br/&gt;
</span><span class='line'>a)在集合中，使用本地节点的zonelist[1]；&lt;br/&gt;
</span><span class='line'>b)不在集合中，使用集合中最小节点号的zonelist[1]；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;MPOL_INTERLEAVE：采用Round-Robin方式从设定的节点集合中选出某个节点，使用此节点的zonelist；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;内核实现的内存策略，用struct mempolicy结构来描述：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    struct mempolicy {
</span><span class='line'>    atomic_t refcnt;
</span><span class='line'>    unsigned short mode;              /* See MPOL_* above */
</span><span class='line'>    unsigned short flags;             /* See set_mempolicy() MPOL_F_* above */
</span><span class='line'>    union {
</span><span class='line'>        short         preferred_node; /* preferred */
</span><span class='line'>        nodemask_t    nodes;          /* interleave/bind */
</span><span class='line'>        /* undefined for default */
</span><span class='line'>    } v;
</span><span class='line'>    union {
</span><span class='line'>        nodemask_t cpuset_mems_allowed;     /* relative to these nodes */
</span><span class='line'>        nodemask_t user_nodemask;           /* nodemask passed by user */
</span><span class='line'>    } w;
</span><span class='line'>};
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;成员mode表示使用四种分配策略中的哪一种，联合体v根据不同的分配策略记录相应的分配信息。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;另外，MPOL_PREFERRED策略有一种特殊的模式，当其flags置上MPOL_F_LOCAL标志后，将等同于MPOL_DEFAULT策略，内核默认使用此种策略，见全局变量default_policy。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;内存策略涉及的分配函数有2个：alloc_pages_current()和alloc_page_vma()，可以分别为不同任务以及任务的不同VMA设置内存策略。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;7.2.2 Buddy隐式分配&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;以默认的NUMA内存策略为例讲解，alloc_pages(gfp_flags, order)分配流程：&lt;br/&gt;
</span><span class='line'>  1) 得到本地节点对应的struct pglist_data结构，从而得到zonelist[2]；&lt;br/&gt;
</span><span class='line'>  2) 如果gfp_flags含有&lt;code&gt;__GFP_THISNODE&lt;/code&gt;标志，仅在此节点分配内存即使用本地节点的Legacy zonelist[1]，否则使用zonelist[0] (见4.2.2.3节)；&lt;br/&gt;
</span><span class='line'>  3) 遍历确定出来的zonelist结构中包含的每一个符合要求的zone，gfp_flags指定了本次分配中的最高的zone，如&lt;code&gt;__GFP_HIGHMEM&lt;/code&gt;表示最高的zone为ZONE_HIGH；&lt;br/&gt;
</span><span class='line'>  4) 分配结束。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;7.2.3 SLAB隐式分配&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;以默认的NUMA内存策略为例讲解，kmem_cache_alloc(cachep, gfp_flags)分配流程：&lt;br/&gt;
</span><span class='line'>  1) 调用&lt;code&gt;____cache_alloc()&lt;/code&gt;函数在本地节点local_node分配，此函数无fall back行为；&lt;br/&gt;
</span><span class='line'>  2) 如果1)中本地节点内存不足分配失败，调用&lt;code&gt;____cache_alloc_node&lt;/code&gt;(cachep, gfp_flags,local_node)再次尝试在本地节点分配，如果还失败此函数会进行fall back行为；&lt;br/&gt;
</span><span class='line'>  3) 分配结束。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;7.3 小结&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;上文提到的所有的内存分配函数都允许fall back行为，但有2种情况例外：&lt;br/&gt;
</span><span class='line'>  1) &lt;code&gt;__GFP_THISNODE&lt;/code&gt;分配标记限制了只能从某一个节点上分配内存；&lt;br/&gt;
</span><span class='line'>  2) MPOL_BIND策略，限制了只能从一个节点集合中的节点上分配内存；&lt;br/&gt;
</span><span class='line'>   (gfp_zone(gfp_flags) &lt; policy_zone的情况，MPOL_BIND不限制节点)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;注：还有一种情况，CPUSET限制的内存策略，后面会介绍。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;8 CPUSET&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;CPUSET基于CGROUP的框架构建的子系统，有如下特点：&lt;br/&gt;
</span><span class='line'>  1) 限定一组任务所允许使用的内存Node和CPU资源；&lt;br/&gt;
</span><span class='line'>  2) CPUSET在内核各子系统中添加的检测代码很少，对内核没有性能影响；&lt;br/&gt;
</span><span class='line'>  3) CPUSET的限定优先级高于内存策略(针对于Node)和绑定(针对于CPU)；&lt;br/&gt;
</span><span class='line'>  4) 没有额外实现系统调用接口，只能通过/proc文件系统和用户交互。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;本节只讲述CPUSET的使用方法和说明。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;8.1 创建CPUSET&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;因为CPUSET只能使用/proc文件系统访问，所以第一步就要先mount cpuset文件系统，配置CONFIG_CGROUPS和CONFIG_CPUSETS后/proc/filesystems中将有这个文件系统。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;CPUSET是分层次的，可以在cpuset文件系统根目录是最顶层的CPUSET，可以在其下创建CPUSET子项，创建方式很简单即创建一个新的目录。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;mount命令：mount nodev –t cpuset /your_dir或mount nodev –t cgroup –o cpuset /your_dir&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Mount成功后，进入mount目录，这个就是最顶层的CPUSET了(top_cpuset)，下面附一个演示例子：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-06-02-38.jpg" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;8.2 CPUSET文件&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;介绍几个重要的CPUSET文件：&lt;br/&gt;
</span><span class='line'>1) tasks，实际上是CGROUPS文件，为此CPUSET包含的线程pid集合；&lt;br/&gt;
</span><span class='line'>  echo 100 &gt; tasks&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;2) cgroup.procs是CGROUPS文件，为此CPUSET包含的线程组tgid集合；&lt;br/&gt;
</span><span class='line'>  echo 100 &gt; cgroup.procs&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;3) cpus是CPUSET文件，表示此CPUSET允许的CPU；&lt;br/&gt;
</span><span class='line'>  echo 0-8 &gt; cpus&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;4) mems是CPUSET文件，表示此CPUSET允许的内存节点；
</span><span class='line'>  echo 0-1 &gt; mems  (对应于struct task_struct中的mems_allowed字段)&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;5) sched_load_balance，为CPUSET文件，设置cpus集合的CPU是否参与负载均衡；
</span><span class='line'>  echo 0 &gt; sched_load_balance (禁止负载均衡)；默认值为1表示开启负载均衡；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;6) sched_relax_domain_level，为CPUSET文件，数值代表某个调度域级别，大于此级别的调度域层次将禁用闲时均衡和唤醒均衡，而其余级别的调度域都开启；
</span><span class='line'>也可以通过启动参数“relax_domain_level”设置，其值含义：&lt;br/&gt;
</span><span class='line'>  -1 : 无效果，此为默认值&lt;br/&gt;
</span><span class='line'>   0 - 设置此值会禁用所有调度域的闲时均衡和唤醒均衡&lt;br/&gt;
</span><span class='line'>   1 - 超线程域&lt;br/&gt;
</span><span class='line'>   2 - 核域&lt;br/&gt;
</span><span class='line'>   3 - 物理域&lt;br/&gt;
</span><span class='line'>   4 - NUMA域&lt;br/&gt;
</span><span class='line'>   5 - ALLNODES模式的NUMA域&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;7) mem_exclusive和mem_hardwall，为CPUSET文件，表示内存硬墙标记；默认为0，表示软墙；有关CPUSET的内存硬墙(HardWall)和内存软墙(SoftWall)，下文会介绍；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;8) memory_spread_page和memory_spread_slab，为CPUSET文件，设定CPUSET中的任务PageCache和SLAB(创建时置有SLAB_MEM_SPREAD)以Round-Robin方式使用内存节点(类似于MPOL_INTERLEAVE)；默认为0，表示未开启；struct task_struct结构中增加成员cpuset_mem_spread_rotor记录下次使用的节点号；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;9) memory_migrate，为CPUSET文件，表明开启此CPUSET的内存迁移，默认为0；&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  当一个任务从一个CPUSET1(mems值为0)迁移至另一个CPUSET2(mems值为1)的时候，此任务在节点0上分配的页面内容将迁移至节点1上分配新的页面(将数据同步到新页面)，这样就避免了此任务的非本地节点的内存访问。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-06-02-39.jpg" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;上图为单Node，8个CPU的系统。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;1) 顶层CPUSET包含了系统中的所有CPU以及Node，而且是只读的，不能更改；&lt;br/&gt;
</span><span class='line'>2) 顶层CPUSET包含了系统中的所有任务，可以更改；&lt;br/&gt;
</span><span class='line'>3) child为新创建的子CPUSET，子CPUSET的资源不能超过父CPUSET的资源；&lt;br/&gt;
</span><span class='line'>4) 新创建的CPUSET的mems和cpus都是空的，使用前必须先初始化；&lt;br/&gt;
</span><span class='line'>5) 添加任务：设置tasks和cgroup.procs文件；&lt;br/&gt;
</span><span class='line'>6) 删除任务：将任务重新添加至其它CPUSET(如顶层)就可以从本CPUSET删除任务。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;8.3 利用CPUSET限定CPU和Node&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;设置步骤：&lt;br/&gt;
</span><span class='line'>1) 在某个父CPUSET中创建子CPUSET；&lt;br/&gt;
</span><span class='line'>2) 在子CPUSET目录下，输入指定的Node号至mems文件；&lt;br/&gt;
</span><span class='line'>3) 在子CPUSET目录下，输入指定的Node号至mems文件；&lt;br/&gt;
</span><span class='line'>4) 在子CPUSET目录下，设定任务至tasks或group.procs文件；&lt;br/&gt;
</span><span class='line'>5) 还可以设置memory_migrate为1，激活内存页面的迁移功能。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这样限定后，此CPUSET中所有的任务都将使用限定的CPU和Node，但毕竟系统中的任务并不能完全孤立，比如还是可能会全局共享Page Cache，动态库等资源，因此内核在某些情况下还是可以允许打破这个限制，如果不允许内核打破这个限制，需要设定CPUSET的内存硬墙标志即mem_exclusive或mem_hardwall置1即可；CPUSET默认是软墙。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;硬软墙用于Buddy系统的页面分配，优先级高于内存策略，请参考内核函数：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;cpuset_zone_allowed_hardwall()和cpuset_zone_allowed_softwall()&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;另外，当内核分不到内存将导致Oops的时候，CPUSET所有规则将被打破，毕竟一个系统的正常运行才是最重要的：&lt;br/&gt;
</span><span class='line'>  1) &lt;code&gt;__GFP_THISNODE&lt;/code&gt;标记分配内存的时候(通常是SLAB系统)；&lt;br/&gt;
</span><span class='line'>  2) 中断中分配内存的时候；&lt;br/&gt;
</span><span class='line'>  3) 任务置有TIF_MEMDIE标记即被内核OOM杀死的任务。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;8.4 利用CPUSET动态改变调度域结构&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;利用sched_load_balance文件可以禁用掉某些CPU的负载均衡，同时重新构建调度域，此功能类似启动参数“isolcpus”的功能。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;8个CPU的系统中，系统中存在一个物理域，现需要禁掉CPU4~CPU7的负载均衡，配置步骤为：&lt;br/&gt;
</span><span class='line'>  1) “mkdir child”在顶层CPUSET中创建子CPUSET，记为child；&lt;br/&gt;
</span><span class='line'>  2) “echo 0-3 &gt; child/cpus ”(新建CPUSET的sched_load_balance默认是是打开的)；&lt;br/&gt;
</span><span class='line'>  3) “echo 0 &gt; sched_load_balance”关闭顶层CPUSET的负载均衡。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;操作过程见下图：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-06-02-40.jpg" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;由图可见，CPU4~CPU7的调度域已经不存在了，具体效果是将CPU4~CPU7从负载均衡中隔离出来。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;9 NUMA杂项&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;1) /sys/devices/system/node/中记录有系统中的所有内存节点信息；&lt;br/&gt;
</span><span class='line'>2)任务额外关联一个/proc/&lt;tid&gt;/numa_smaps文件信息；&lt;br/&gt;
</span><span class='line'>3) tmpfs可以指定在某个Node上创建；&lt;br/&gt;
</span><span class='line'>4) libnuma库和其numactl小工具可以方便操作NUMA内存；&lt;br/&gt;
</span><span class='line'>5) … …&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h3&gt;10 参考资料&lt;/h3&gt;
</span><span class='line'>
</span><span class='line'>&lt;ol&gt;
</span><span class='line'>&lt;li&gt;www.kernel.org&lt;/li&gt;
</span><span class='line'>&lt;li&gt;ULK3&lt;/li&gt;
</span><span class='line'>&lt;li&gt;XLP® Processor Family Programming Reference Manual&lt;/li&gt;
</span><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[Linux内存管理]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2015/06/02/kernel-mm-alloc/"/&gt;
</span><span class='line'>&lt;updated&gt;2015-06-02T15:05:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2015/06/02/kernel-mm-alloc&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;p&gt;&lt;a href="http://blog.csdn.net/myarrow/article/details/8624687"&gt;http://blog.csdn.net/myarrow/article/details/8624687&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;a href="http://blog.csdn.net/myarrow/article/details/8682819"&gt;http://blog.csdn.net/myarrow/article/details/8682819&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;1. Linux物理内存三级架构&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/2015-06-02-20.png" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;对于内存管理，Linux采用了与具体体系架构不相关的设计模型，实现了良好的可伸缩性。它主要由内存节点node、内存区域zone和物理页框page三级架构组成。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;内存节点node&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  内存节点node是计算机系统中对物理内存的一种描述方法，一个总线主设备访问位于同一个节点中的任意内存单元所花的代价相同，而访问任意两个不同节点中的内存单元所花的代价不同。在一致存储结构(Uniform Memory Architecture，简称UMA)计算机系统中只有一个节点，而在非一致性存储结构(NUMA)计算机系统中有多个节点。Linux内核中使用数据结构pg_data_t来表示内存节点node。如常用的ARM架构为UMA架构。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;内存区域zone&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  内存区域位于同一个内存节点之内，由于各种原因它们的用途和使用方法并不一样。如基于IA32体系结构的个人计算机系统中，由于历史原因使得ISA设备只能使用最低16MB来进行DMA传输。又如，由于Linux内核采用&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt; • 物理页框page&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;2. Linux虚拟内存三级页表&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Linux虚拟内存三级管理由以下三级组成：&lt;br/&gt;
</span><span class='line'> • PGD: Page Global Directory (页目录)&lt;br/&gt;
</span><span class='line'> • PMD: Page Middle Directory (页目录)&lt;br/&gt;
</span><span class='line'> • PTE:  Page Table Entry     (页表项)&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;每一级有以下三个关键描述宏：&lt;br/&gt;
</span><span class='line'> • SHIFT&lt;br/&gt;
</span><span class='line'> • SIZE&lt;br/&gt;
</span><span class='line'> • MASK&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;如页的对应描述为：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>/* PAGE_SHIFT determines the page size  asm/page.h */  
</span><span class='line'>#define PAGE_SHIFT      12  
</span><span class='line'>#define PAGE_SIZE       (_AC(1,UL) &lt;&lt; PAGE_SHIFT)  
</span><span class='line'>#define PAGE_MASK       (~(PAGE_SIZE-1))  
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;数据结构定义如下：</span></code></pre></td></tr></table></div></figure>
    /<em> asm/page.h </em>/<br/>
    typedef unsigned long pteval_t;</p>

<pre><code>typedef pteval_t pte_t;  
typedef unsigned long pmd_t;  
typedef unsigned long pgd_t[2];  
typedef unsigned long pgprot_t;  

#define pte_val(x)      (x)  
#define pmd_val(x)      (x)  
#define pgd_val(x)  ((x)[0])  
#define pgprot_val(x)   (x)  

#define __pte(x)        (x)  
#define __pmd(x)        (x)  
#define __pgprot(x)     (x)  
</code></pre>

<pre><code>
##### 2.1 Page Directory (PGD and PMD)

  每个进程有它自己的PGD( Page Global Directory)，它是一个物理页，并包含一个pgd_t数组。其定义见&lt;asm/page.h&gt;。 进程的pgd_t数据见 task_struct -&gt; mm_struct -&gt; pgd_t * pgd;    

  ARM架构的PGD和PMD的定义如下&lt;arch/arm/include/asm/pgtable.h&gt;：
</code></pre>

<pre><code>#define PTRS_PER_PTE  512    // PTE中可包含的指针&lt;u32&gt;数 (21-12=9bit)  
#define PTRS_PER_PMD  1  
#define PTRS_PER_PGD  2048   // PGD中可包含的指针&lt;u32&gt;数 (32-21=11bit)&lt;/p&gt;&lt;p&gt;#define PTE_HWTABLE_PTRS (PTRS_PER_PTE)  
#define PTE_HWTABLE_OFF  (PTE_HWTABLE_PTRS * sizeof(pte_t))  
#define PTE_HWTABLE_SIZE (PTRS_PER_PTE * sizeof(u32))
/*  
 * PMD_SHIFT determines the size of the area a second-level page table can map  
 * PGDIR_SHIFT determines what a third-level page table entry can map  
 */  
#define PMD_SHIFT  21  
#define PGDIR_SHIFT  21
</code></pre>

<pre><code>
虚拟地址SHIFT宏图：

![](/images/kernel/2015-06-02-21.png)  

虚拟地址MASK和SIZE宏图：

![](/images/kernel/2015-06-02-22.png)  


##### 2.2 Page Table Entry

PTEs, PMDs和PGDs分别由pte_t, pmd_t 和pgd_t来描述。为了存储保护位，pgprot_t被定义，它拥有相关的flags并经常被存储在page table entry低位(lower bits)，其具体的存储方式依赖于CPU架构。

每个pte_t指向一个物理页的地址，并且所有的地址都是页对齐的。因此在32位地址中有PAGE_SHIFT(12)位是空闲的，它可以为PTE的状态位。

PTE的保护和状态位如下图所示：

![](/images/kernel/2015-06-02-23.png)  

##### 2.3 如何通过3级页表访问物理内存
为了通过PGD、PMD和PTE访问物理内存，其相关宏在asm/pgtable.h中定义。

• pgd_offset 

根据当前虚拟地址和当前进程的mm_struct获取pgd项的宏定义如下： 
</code></pre>

<pre><code>/* to find an entry in a page-table-directory */  
#define pgd_index(addr)     ((addr) &gt;&gt; PGDIR_SHIFT)  //获得在pgd表中的索引  

#define pgd_offset(mm, addr)    ((mm)-&gt;pgd + pgd_index(addr)) //获得pmd表的起始地址  

/* to find an entry in a kernel page-table-directory */  
#define pgd_offset_k(addr)  pgd_offset(&amp;init_mm, addr)  
</code></pre>

<pre><code>
• pmd_offset

根据通过pgd_offset获取的pgd 项和虚拟地址，获取相关的pmd项(即pte表的起始地址) 
</code></pre>

<pre><code>/* Find an entry in the second-level page table.. */  
#define pmd_offset(dir, addr)   ((pmd_t *)(dir))   //即为pgd项的值  
</code></pre>

<pre><code>
• pte_offset

  根据通过pmd_offset获取的pmd项和虚拟地址，获取相关的pte项(即物理页的起始地址)
</code></pre>

<pre><code>#ifndef CONFIG_HIGHPTE  
#define __pte_map(pmd)      pmd_page_vaddr(*(pmd))  
#define __pte_unmap(pte)    do { } while (0)  
#else  
#define __pte_map(pmd)      (pte_t *)kmap_atomic(pmd_page(*(pmd)))  
#define __pte_unmap(pte)    kunmap_atomic(pte)  
#endif  

#define pte_index(addr)     (((addr) &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1))  

#define pte_offset_kernel(pmd,addr) (pmd_page_vaddr(*(pmd)) + pte_index(addr))  

#define pte_offset_map(pmd,addr)    (__pte_map(pmd) + pte_index(addr))  
#define pte_unmap(pte)          __pte_unmap(pte)  

#define pte_pfn(pte)        (pte_val(pte) &gt;&gt; PAGE_SHIFT)  
#define pfn_pte(pfn,prot)   __pte(__pfn_to_phys(pfn) | pgprot_val(prot))  

#define pte_page(pte)       pfn_to_page(pte_pfn(pte))  
#define mk_pte(page,prot)   pfn_pte(page_to_pfn(page), prot)  

#define set_pte_ext(ptep,pte,ext) cpu_set_pte_ext(ptep,pte,ext)  
#define pte_clear(mm,addr,ptep) set_pte_ext(ptep, __pte(0), 0)  
</code></pre>

<pre><code>其示意图如下图所示：

![](/images/kernel/2015-06-02-24.png)  

##### 2.4 根据虚拟地址获取物理页的示例代码

  根据虚拟地址获取物理页的示例代码详见&lt;mm/memory.c中的函数follow_page&gt;。
</code></pre>

<pre><code>/** 
 * follow_page - look up a page descriptor from a user-virtual address 
 * @vma: vm_area_struct mapping @address 
 * @address: virtual address to look up 
 * @flags: flags modifying lookup behaviour 
 * 
 * @flags can have FOLL_ flags set, defined in &lt;linux/mm.h&gt; 
 * 
 * Returns the mapped (struct page *), %NULL if no mapping exists, or 
 * an error pointer if there is a mapping to something not represented 
 * by a page descriptor (see also vm_normal_page()). 
 */  
struct page *follow_page(struct vm_area_struct *vma, unsigned long address,  
            unsigned int flags)  
{  
    pgd_t *pgd;  
    pud_t *pud;  
    pmd_t *pmd;  
    pte_t *ptep, pte;  
    spinlock_t *ptl;  
    struct page *page;  
    struct mm_struct *mm = vma-&gt;vm_mm;  

    page = follow_huge_addr(mm, address, flags &amp; FOLL_WRITE);  
    if (!IS_ERR(page)) {  
        BUG_ON(flags &amp; FOLL_GET);  
        goto out;  
    }  

    page = NULL;  
    pgd = pgd_offset(mm, address);  
    if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))  
        goto no_page_table;  

    pud = pud_offset(pgd, address);  
    if (pud_none(*pud))  
        goto no_page_table;  
    if (pud_huge(*pud) &amp;&amp; vma-&gt;vm_flags &amp; VM_HUGETLB) {  
        BUG_ON(flags &amp; FOLL_GET);  
        page = follow_huge_pud(mm, address, pud, flags &amp; FOLL_WRITE);  
        goto out;  
    }  
    if (unlikely(pud_bad(*pud)))  
        goto no_page_table;  

    pmd = pmd_offset(pud, address);  
    if (pmd_none(*pmd))  
        goto no_page_table;  
    if (pmd_huge(*pmd) &amp;&amp; vma-&gt;vm_flags &amp; VM_HUGETLB) {  
        BUG_ON(flags &amp; FOLL_GET);  
        page = follow_huge_pmd(mm, address, pmd, flags &amp; FOLL_WRITE);  
        goto out;  
    }  
    if (pmd_trans_huge(*pmd)) {  
        if (flags &amp; FOLL_SPLIT) {  
            split_huge_page_pmd(mm, pmd);  
            goto split_fallthrough;  
        }  
        spin_lock(&amp;mm-&gt;page_table_lock);  
        if (likely(pmd_trans_huge(*pmd))) {  
            if (unlikely(pmd_trans_splitting(*pmd))) {  
                spin_unlock(&amp;mm-&gt;page_table_lock);  
                wait_split_huge_page(vma-&gt;anon_vma, pmd);  
            } else {  
                page = follow_trans_huge_pmd(mm, address,  
                                 pmd, flags);  
                spin_unlock(&amp;mm-&gt;page_table_lock);  
                goto out;  
            }  
        } else  
            spin_unlock(&amp;mm-&gt;page_table_lock);  
        /* fall through */  
    }  
split_fallthrough:  
    if (unlikely(pmd_bad(*pmd)))  
        goto no_page_table;  

    ptep = pte_offset_map_lock(mm, pmd, address, &amp;ptl);  

    pte = *ptep;  
    if (!pte_present(pte))  
        goto no_page;  
    if ((flags &amp; FOLL_WRITE) &amp;&amp; !pte_write(pte))  
        goto unlock;  

    page = vm_normal_page(vma, address, pte);  
    if (unlikely(!page)) {  
        if ((flags &amp; FOLL_DUMP) ||  
            !is_zero_pfn(pte_pfn(pte)))  
            goto bad_page;  
        page = pte_page(pte);  
    }  

    if (flags &amp; FOLL_GET)  
        get_page(page);  
    if (flags &amp; FOLL_TOUCH) {  
        if ((flags &amp; FOLL_WRITE) &amp;&amp;  
            !pte_dirty(pte) &amp;&amp; !PageDirty(page))  
            set_page_dirty(page);  
        /* 
         * pte_mkyoung() would be more correct here, but atomic care 
         * is needed to avoid losing the dirty bit: it is easier to use 
         * mark_page_accessed(). 
         */  
        mark_page_accessed(page);  
    }  
    if ((flags &amp; FOLL_MLOCK) &amp;&amp; (vma-&gt;vm_flags &amp; VM_LOCKED)) {  
        /* 
         * The preliminary mapping check is mainly to avoid the 
         * pointless overhead of lock_page on the ZERO_PAGE 
         * which might bounce very badly if there is contention. 
         * 
         * If the page is already locked, we don't need to 
         * handle it now - vmscan will handle it later if and 
         * when it attempts to reclaim the page. 
         */  
        if (page-&gt;mapping &amp;&amp; trylock_page(page)) {  
            lru_add_drain();  /* push cached pages to LRU */  
            /* 
             * Because we lock page here and migration is 
             * blocked by the pte's page reference, we need 
             * only check for file-cache page truncation. 
             */  
            if (page-&gt;mapping)  
                mlock_vma_page(page);  
            unlock_page(page);  
        }  
    }  
unlock:  
    pte_unmap_unlock(ptep, ptl);  
out:  
    return page;  

bad_page:  
    pte_unmap_unlock(ptep, ptl);  
    return ERR_PTR(-EFAULT);  

no_page:  
    pte_unmap_unlock(ptep, ptl);  
    if (!pte_none(pte))  
        return page;  

no_page_table:  
    /* 
     * When core dumping an enormous anonymous area that nobody 
     * has touched so far, we don't want to allocate unnecessary pages or 
     * page tables.  Return error instead of NULL to skip handle_mm_fault, 
     * then get_dump_page() will return NULL to leave a hole in the dump. 
     * But we can only make this optimization where a hole would surely 
     * be zero-filled if handle_mm_fault() actually did handle it. 
     */  
    if ((flags &amp; FOLL_DUMP) &amp;&amp;  
        (!vma-&gt;vm_ops || !vma-&gt;vm_ops-&gt;fault))  
        return ERR_PTR(-EFAULT);  
    return page;  
}
</code></pre>

<pre><code>
-------------

#### 1. First Fit分配器

  First Fit分配器是最基本的内存分配器，它使用bitmap而不是空闲块列表来表示内存。在bitmap中，如果page对应位为1，则表示此page已经被分配，为0则表示此page没有被分配。为了分配小于一个page的内存块，First Fit分配器记录了最后被分配的PFN (Page Frame Number)和分配的结束地址在页内的偏移量。随后小的内存分配被Merge到一起并存储到同一页中。

  First Fit分配器不会造成严重的内存碎片，但其效率较低，由于内存经常通过线性地址进行search，而First Fit中的小块内存经常在物理内存的开始处，为了分配大块内存而不得不扫描前面大量的内存。

#### 2. Boot Memory分配器

  物理内存分配器如何分配内存来初始化其自己呢？

  答案是：通过Boot Memory分配器来实现，而Boot Memory分配器则通过最基本的First Fit分配器来实现。

##### 2.1 Boot Map定义 

  Boot Map通过数据结构bootmem_data来定义，详见&lt;linux/bootmem.h&gt;，其定义如下所示： 
</code></pre>

<pre><code>typedef struct bootmem_data {  
    unsigned long node_boot_start; // 描述的物理内存的起始地址  
    unsigned long node_low_pfn;    // 结束物理地址，即ZONE_NORMAL的结束  
    void *node_bootmem_map;        // 描述“使用或空闲的位图”的地址  
    unsigned long last_offset;     // 最后被分配的页内偏移量，即在llast_pos描述的物理页中，  
                                 // 从last_offset开始，没有被分配   
    unsigned long last_pos;        // 最后被分配的页的PFN  
} bootmem_data_t;  
</code></pre>

<pre><code>
所有bootmem_data被放于全局变量bdata_list中。

##### 2.2 Boot Memory分配器初始化

  每一个CPU架构被要求提供setup_arch函数，它负责获取初始化boot memory分配器的必要参数。不同的CPU架构通过不同的函数来实现，如ARM通过bootmem_init来实现。它负责获取以下参数：
</code></pre>

<pre><code>• min_low_pfn： 系统中可获得的最小的PFN，装载kernel image结束之后的第一页，在mm/bootmem.c中定义
• max_low_pfn：低端内存(ZONE_NORMAL)中可获得的最大PFN
• highstart_pfn：高端内存(ZONE_HIGHMEM)的起始PFN
• highend_pfn：高端内存(ZONE_HIGHMEM)的结束PFN
• max_pfn：系统中可获得的最大的PFN， 在mm/bootmem.c中定义
</code></pre>

<pre><code>
PFN是在物理内存map的偏移量，以page为单位。Kernel可直接访问ZONE_NORMAL，其偏移量为：PAGE_OFFSET。

通过以上5个参数明确了可用物理内存之后，调用init_bootmem-&gt;init_bootmem_core来初始化contig_page_data。它主要完成以下两件事：  
  1) 将把与此node对应pgdat_data_t插入到pgdat_list中  
  2) 初始化bootmem_data_t的中参数，并分配表示页分配状态的bitmap，其大小为: (end_pfn-start_pfn+7)/8  

bitmap的物理地址为：bootmem_data_t-&gt;node_boot_start  
bitmap的虚拟地直为：bootmem_data_t-&gt;node_bootmem_map  

##### 2.3 分配内存
 • reserve_bootmem：用于预留物理页面。但用于通用的内存分配是低率的，它主要用于各种驱动(如：Video Codec)预留内存。

常用的内存分配函数如下(in UMA架构，我们常的ARM架构为UMA架构)：
</code></pre>

<pre><code>• alloc_bootmem
• alloc_bootmem_low
• alloc_bootmem_pages
• alloc_bootmem_low_pages
</code></pre>

<pre><code>
其调用关系如下图所示：

###### 2.3.1 `__alloc_bootmem`

`__alloc_bootmem()` 需要以下参数：  
 • pgdat 用于分配内存块的节点，在UMA架构中，它被忽略，因为它总是为：contig_page_data  
 • size  指定请求分配的内存大小，以字节为单位  
 • align 请求以多少字节对齐，地于小块内存分配，一般以SMP_CACHE_BYTES对齐，如在X86上，与L1硬件cache对齐  
 • goal  偏好的分配内存的起始地址,  

###### 2.3.2 __alloc_bootmem_core

它从goal指定的地址开始，线性地扫描内存，以寻找可以满足内存分配要求的内存块。它的另外一项功能是决定是否需要把新分配的内存块与以前已经分配的内存块merge到一起。

分配内存常用函数定义如下： 
</code></pre>

<pre><code>#ifdef CONFIG_NO_BOOTMEM  
/* We are using top down, so it is safe to use 0 here */  
#define BOOTMEM_LOW_LIMIT 0  
#else  
#define BOOTMEM_LOW_LIMIT __pa(MAX_DMA_ADDRESS)  
#endif  

#define alloc_bootmem(x) \  
    __alloc_bootmem(x, SMP_CACHE_BYTES, BOOTMEM_LOW_LIMIT)  
#define alloc_bootmem_align(x, align) \  
    __alloc_bootmem(x, align, BOOTMEM_LOW_LIMIT)  
#define alloc_bootmem_nopanic(x) \  
    __alloc_bootmem_nopanic(x, SMP_CACHE_BYTES, BOOTMEM_LOW_LIMIT)  
#define alloc_bootmem_pages(x) \  
    __alloc_bootmem(x, PAGE_SIZE, BOOTMEM_LOW_LIMIT)  
#define alloc_bootmem_pages_nopanic(x) \  
    __alloc_bootmem_nopanic(x, PAGE_SIZE, BOOTMEM_LOW_LIMIT)  
#define alloc_bootmem_node(pgdat, x) \  
    __alloc_bootmem_node(pgdat, x, SMP_CACHE_BYTES, BOOTMEM_LOW_LIMIT)  
#define alloc_bootmem_node_nopanic(pgdat, x) \  
    __alloc_bootmem_node_nopanic(pgdat, x, SMP_CACHE_BYTES, BOOTMEM_LOW_LIMIT)  
#define alloc_bootmem_pages_node(pgdat, x) \  
    __alloc_bootmem_node(pgdat, x, PAGE_SIZE, BOOTMEM_LOW_LIMIT)  
#define alloc_bootmem_pages_node_nopanic(pgdat, x) \  
    __alloc_bootmem_node_nopanic(pgdat, x, PAGE_SIZE, BOOTMEM_LOW_LIMIT)  

#define alloc_bootmem_low(x) \  
    __alloc_bootmem_low(x, SMP_CACHE_BYTES, 0)  
#define alloc_bootmem_low_pages(x) \  
    __alloc_bootmem_low(x, PAGE_SIZE, 0)  
#define alloc_bootmem_low_pages_node(pgdat, x) \  
    __alloc_bootmem_low_node(pgdat, x, PAGE_SIZE, 0)  
</code></pre>

<pre><code>
##### 2.4 释放内存

调用free_bootmem来释放内存。
</code></pre>

<pre><code>void __init free_bootmem(unsigned long addr, unsigned long size)  
{  
    unsigned long start, end;  

    kmemleak_free_part(__va(addr), size);  

    start = PFN_UP(addr);  
    end = PFN_DOWN(addr + size);  

    mark_bootmem(start, end, 0, 0);  
}
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SMP、NUMA体系结构]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/06/02/kernel-mm-smp-numa/"/>
    <updated>2015-06-02T14:34:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/06/02/kernel-mm-smp-numa</id>
    <content type="html"><![CDATA[<p><a href="http://www.cnblogs.com/yubo/archive/2010/04/23/1718810.html">http://www.cnblogs.com/yubo/archive/2010/04/23/1718810.html</a></p>

<p>blog.csdn.net/skymanwu/article/details/7551670</p>

<p>  从系统架构来看，目前的商用服务器大体可以分为三类，即对称多处理器结构 (SMP ： Symmetric Multi-Processor) ，非一致存储访问结构 (NUMA ： Non-Uniform Memory Access) ，以及海量并行处理结构 (MPP ： Massive Parallel Processing) 。它们的特征分别描述如下：</p>

<h4>1. SMP(Symmetric Multi-Processor)</h4>

<p>  SMP (Symmetric Multi Processing),对称多处理系统内有许多紧耦合多处理器，在这样的系统中，所有的CPU共享全部资源，如总线，内存和I/O系统等，操作系统或管理数据库的复本只有一个，这种系统有一个最大的特点就是共享所有资源。多个CPU之间没有区别，平等地访问内存、外设、一个操作系统。操作系统管理着一个队列，每个处理器依次处理队列中的进程。如果两个处理器同时请求访问一个资源（例如同一段内存地址），由硬件、软件的锁机制去解决资源争用问题。Access to RAM is serialized; this and cache coherency issues causes performance to lag slightly behind the number of additional processors in the system.</p>

<p><img src="/images/kernel/2015-06-02-10.gif" alt="" /></p>

<p>  所谓对称多处理器结构，是指服务器中多个 CPU 对称工作，无主次或从属关系。各 CPU 共享相同的物理内存，每个 CPU 访问内存中的任何地址所需时间是相同的，因此 SMP 也被称为一致存储器访问结构 (UMA ： Uniform Memory Access) 。对 SMP 服务器进行扩展的方式包括增加内存、使用更快的 CPU 、增加 CPU 、扩充 I/O( 槽口数与总线数 ) 以及添加更多的外部设备 ( 通常是磁盘存储 ) 。</p>

<p>  SMP 服务器的主要特征是共享，系统中所有资源 (CPU 、内存、 I/O 等 ) 都是共享的。也正是由于这种特征，导致了 SMP 服务器的主要问题，那就是它的扩展能力非常有限。对于 SMP 服务器而言，每一个共享的环节都可能造成 SMP 服务器扩展时的瓶颈，而最受限制的则是内存。由于每个 CPU 必须通过相同的内存总线访问相同的内存资源，因此随着 CPU 数量的增加，内存访问冲突将迅速增加，最终会造成 CPU 资源的浪费，使 CPU 性能的有效性大大降低。实验证明， SMP 服务器 CPU 利用率最好的情况是 2 至 4 个 CPU 。</p>

<p><img src="/images/kernel/2015-06-02-11.gif" alt="" /></p>

<h4>2. NUMA(Non-Uniform Memory Access)</h4>

<p>  由于 SMP 在扩展能力上的限制，人们开始探究如何进行有效地扩展从而构建大型系统的技术， NUMA 就是这种努力下的结果之一。利用 NUMA 技术，可以把几十个 CPU( 甚至上百个 CPU) 组合在一个服务器内。其 CPU 模块结构如图 2 所示：</p>

<p><img src="/images/kernel/2015-06-02-12.gif" alt="" /><br/>
图 2.NUMA 服务器 CPU 模块结构</p>

<p>  NUMA 服务器的基本特征是具有多个 CPU 模块，每个 CPU 模块由多个 CPU( 如 4 个 ) 组成，并且具有独立的本地内存、 I/O 槽口等。由于其节点之间可以通过互联模块 ( 如称为 Crossbar Switch) 进行连接和信息交互，因此每个 CPU 可以访问整个系统的内存 ( 这是 NUMA 系统与 MPP 系统的重要差别 ) 。显然，访问本地内存的速度将远远高于访问远地内存 ( 系统内其它节点的内存 ) 的速度，这也是非一致存储访问 NUMA 的由来。由于这个特点，为了更好地发挥系统性能，开发应用程序时需要尽量减少不同 CPU 模块之间的信息交互。</p>

<p>  利用 NUMA 技术，可以较好地解决原来 SMP 系统的扩展问题，在一个物理服务器内可以支持上百个 CPU 。比较典型的 NUMA 服务器的例子包括 HP 的 Superdome 、 SUN15K 、 IBMp690 等。</p>

<p>  但 NUMA 技术同样有一定缺陷，由于访问远地内存的延时远远超过本地内存，因此当 CPU 数量增加时，系统性能无法线性增加。如 HP 公司发布 Superdome 服务器时，曾公布了它与 HP 其它 UNIX 服务器的相对性能值，结果发现， 64 路 CPU 的 Superdome (NUMA 结构 ) 的相对性能值是 20 ，而 8 路 N4000( 共享的 SMP 结构 ) 的相对性能值是 6.3 。从这个结果可以看到， 8 倍数量的 CPU 换来的只是 3 倍性能的提升。</p>

<hr />

<p><img src="/images/kernel/2015-06-02-13.png" alt="" /></p>

<p><img src="/images/kernel/2015-06-02-14.jpg" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux 中的零拷贝技术 splice]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/07/kernel-mm-splice/"/>
    <updated>2015-05-07T15:26:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/07/kernel-mm-splice</id>
    <content type="html"><![CDATA[<p><a href="http://hi.baidu.com/renguihuashi/item/ef71f8e28d74f5f22b09a415">http://hi.baidu.com/renguihuashi/item/ef71f8e28d74f5f22b09a415</a></p>

<p>linux下如何实现文件对拷呢？</p>

<p>最容易想到的方法就是，申请一份内存buf，read 源文件一段数据到buf，然后将此buf write到目标文件，示例代码如下：
<code>
    char buf[max_read];
    off_t size = stat_buf.st_size;
    while ( off_in &lt; size ) {
        int len = size - off_in &gt; max_read ? max_read : size - off_in;
        len = read(f_in, buf, len);
        off_in += len;
        write(f_out, buf, len);
    }
</code></p>

<p>还有一种大家都知道的方式，就是通过mmap实现，示例代码如下：
<code>
    size_t filesize = stat_buf.st_size;
    source = mmap(0, filesize, PROT_READ, MAP_SHARED, f_in, 0);
    target = mmap(0, filesize, PROT_WRITE, MAP_SHARED, f_out, 0);
    memcpy(target, source, filesize);
</code>
因为mmap不需要内核态和用户态的内存拷贝，效率大大提高。</p>

<p>本文还想介绍另外一种，是今天无意google到的，就是如标题所述，基于splice实现，splice是Linux 2.6.17新加入的系统调用，官方文档的描述是，用于在两个文件间移动数据，而无需内核态和用户态的内存拷贝，但需要借助管道（pipe）实现。大概原理就是通过pipe buffer实现一组内核内存页（pages of kernel memory）的引用计数指针（reference-counted pointers），数据拷贝过程中并不真正拷贝数据，而是创建一个新的指向内存页的指针。也就是说拷贝过程实质是指针的拷贝。示例代码如下：
<code>
    int pipefd[2];
    pipe( pipefd );
    int max_read = 4096;
    off_t size = stat_buf.st_size;
    while ( off_in &lt; size ) {
        int len = size - off_in &gt; max_read ? max_read : size - off_in;
        len = splice(f_in, &amp;off_in, pipefd[1], NULL, len, SPLICE_F_MORE |SPLICE_F_MOVE);
        splice(pipefd[0], NULL, f_out, &amp;off_out, len, SPLICE_F_MORE |SPLICE_F_MOVE);
    }
</code>
使用splice一定要注意，因为其借助管道实现，而管道有众所周知的空间限制问题，超过了限制就会hang住，所以每次写入管道的数据量好严格控制，保守的建议值是一个内存页大小，即4k。另外，off_in和off_out传递的是指针，其值splice会做一定变动，使用时应注意。</p>

<p>splice kernel bug: <a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=baff42ab1494528907bf4d5870359e31711746ae">https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=baff42ab1494528907bf4d5870359e31711746ae</a></p>

<hr />

<p><a href="http://ogris.de/howtos/splice.html">http://ogris.de/howtos/splice.html</a></p>

<p><a href="http://blog.csdn.net/eroswang/article/details/1999034">http://blog.csdn.net/eroswang/article/details/1999034</a></p>

<p><a href="http://stackoverflow.com/questions/1580923/how-can-i-use-linuxs-splice-function-to-copy-a-file-to-another-file">http://stackoverflow.com/questions/1580923/how-can-i-use-linuxs-splice-function-to-copy-a-file-to-another-file</a></p>

<pre><code>   EINVAL Target  file  system  doesn't  support  splicing; target file is
          opened in append mode; neither of the descriptors  refers  to  a
          pipe; or offset given for non-seekable device.
</code></pre>

<hr />

<h4>file to file sample</h4>

<pre><code>    #define _GNU_SOURCE
    #include &lt;fcntl.h&gt;
    #include &lt;stdio.h&gt;
    #include &lt;unistd.h&gt;
    #include &lt;errno.h&gt;
    #include &lt;string.h&gt;
    #include &lt;time.h&gt;

    int main(int argc, char **argv)
    {
        int pipefd[2];
        int result;
        FILE *in_file;
        FILE *out_file;
        char buff[65537];

        if (argc != 3) {
            printf("usage: ./client infile outfile\n");
            exit(0);
        }
        result = pipe(pipefd);

        in_file = fopen(argv[1], "rb");
        out_file = fopen(argv[2], "wb");

        off_t off_in = 0, off_out = 0;
        int len = 1024*1024*30;
        while (len &gt; 0) {
            int size = 65536;
            if (len &lt; size) size = len;
            len -= size;

            result = splice(fileno(in_file), &amp;off_in, pipefd[1], NULL, size, SPLICE_F_MORE | SPLICE_F_MOVE);
            result = splice(pipefd[0], NULL, fileno(out_file), &amp;off_out, size, SPLICE_F_MORE | SPLICE_F_MOVE);
            //printf("%d\n", result);

    //        read(fileno(in_file), buff, size);
    //        write(fileno(out_file), buff, size);
        }
        close(pipefd[0]);
        close(pipefd[1]);
        fclose(in_file);
        fclose(out_file);

        return 0;
    }
</code></pre>

<h4>more sample</h4>

<p><a href="/download/kernel/splice_sample.tar.gz">splice sample</a></p>

<p>like:<br/>
file to socket<br/>
socket to file<br/>
socket to socket</p>
]]></content>
  </entry>
  
</feed>
