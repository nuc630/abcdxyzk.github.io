<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel~mm | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel~mm/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-09-30T16:01:54+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[kernel 3.10内核源码分析--Out of Memory(OOM)处理流程]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/30/kernel-mm-oom/"/>
    <updated>2015-09-30T15:56:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/30/kernel-mm-oom</id>
    <content type="html"><![CDATA[<p><a href="http://blog.chinaunix.net/uid-20671208-id-4440249.html">http://blog.chinaunix.net/uid-20671208-id-4440249.html</a></p>

<p>Out Of Memory(OOM)，即内存耗尽，当系统中内存耗尽时，如果不做处理，将处于崩溃的边缘，因为无内核资源可用，而系统运行时刻都可能需要申请内存。这时，内核需要采取一定的措施来防止系统崩溃，这就是我们熟知的OOM流程，其实就是要回收一些内存，而走到OOM流程，已经基本说明其它的回收内存的手段都已经尝试过了(比如回收cache)，这里通常只能通过kill进程来回收内存了，而选择被kill进程的标准就比较简单直接了，总体就是：谁用的多，就kill谁。</p>

<p>OOM处理的基本流程简单描述如下：</p>

<p>1、检查是否配置了/proc/sys/kernel/panic_on_oom，如果是则直接触发panic。</p>

<p>2、检查是否配置了oom_kill_allocating_task，即是否需要kill current进程来回收内存，如果是，且current进程是killable的，则kill current进程。</p>

<p>3、根据既定策略选择需要kill的process，基本策略为：通过进程的内存占用情况计算“点数”，点数最高者被选中。</p>

<p>4、如果没有选出来可kill的进程，那么直接panic(通常不会走到这个流程，但也有例外，比如，当被选中的进程处于D状态，或者正在被kill)</p>

<p>5、kill掉被选中的进程，以释放内存。</p>

<p>代码注释如下：</p>

<pre><code>    /*
      * OOM处理的主流程，上面的注释应该比较清楚了。
      */
    void out_of_memory(struct zonelist *zonelist, gfp_t gfp_mask,
            int order, nodemask_t *nodemask, bool force_kill)
    {
        const nodemask_t *mpol_mask;
        struct task_struct *p;
        unsigned long totalpages;
        unsigned long freed = 0;
        unsigned int uninitialized_var(points);
        enum oom_constraint constraint = CONSTRAINT_NONE;
        int killed = 0;

        // 调用block通知链oom_nofify_list中的函数
        blocking_notifier_call_chain(&amp;oom_notify_list, 0, &amp;freed);

        if (freed &gt; 0)
            /* Got some memory back in the last second. */
            return;

        /*
         * If current has a pending SIGKILL or is exiting, then automatically
         * select it. The goal is to allow it to allocate so that it may
         * quickly exit and free its memory.
         */
        /*
         * 如果当前进程有pending的SIGKILL(9)信号，或者正在退出，则选择当前进程来kill,
         * 这样可以最快的达到释放内存的目的。
         */
        if (fatal_signal_pending(current) || current-&gt;flags &amp; PF_EXITING) {
            set_thread_flag(TIF_MEMDIE);
            return;
        }

        /*
         * Check if there were limitations on the allocation (only relevant for
         * NUMA) that may require different handling.
         */
        /*
         * 检查是否有限制，有几种不同的限制策略，仅用于NUMA场景
         */
        constraint = constrained_alloc(zonelist, gfp_mask, nodemask,
                            &amp;totalpages);
        mpol_mask = (constraint == CONSTRAINT_MEMORY_POLICY) ? nodemask : NULL;
        // 检查是否配置了/proc/sys/kernel/panic_on_oom，如果是则直接触发panic
        check_panic_on_oom(constraint, gfp_mask, order, mpol_mask);

        /*
         * 检查是否配置了oom_kill_allocating_task，即是否需要kill current进程来
         * 回收内存，如果是，且current进程是killable的，则kill current进程。
         */
        if (sysctl_oom_kill_allocating_task &amp;&amp; current-&gt;mm &amp;&amp;
         !oom_unkillable_task(current, NULL, nodemask) &amp;&amp;
         current-&gt;signal-&gt;oom_score_adj != OOM_SCORE_ADJ_MIN) {
            get_task_struct(current);
            // kill被选中的进程。
            oom_kill_process(current, gfp_mask, order, 0, totalpages, NULL,
                     nodemask,
                     "Out of memory (oom_kill_allocating_task)");
            goto out;
        }

        // 根据既定策略选择需要kill的process。
        p = select_bad_process(&amp;points, totalpages, mpol_mask, force_kill);
        /* Found nothing?!?! Either we hang forever, or we panic. */
        /*
         * 如果没有选出来，即没有可kill的进程，那么直接panic
         * 通常不会走到这个流程，但也有例外，比如，当被选中的进程处于D状态，或者正在被kill
         */
        if (!p) {
            dump_header(NULL, gfp_mask, order, NULL, mpol_mask);
            panic("Out of memory and no killable processes...\n");
        }
        // kill掉被选中的进程，以释放内存。
        if (PTR_ERR(p) != -1UL) {
            oom_kill_process(p, gfp_mask, order, points, totalpages, NULL,
                     nodemask, "Out of memory");
            killed = 1;
        }
    out:
        /*
         * Give the killed threads a good chance of exiting before trying to
         * allocate memory again.
         */
        /*
         * 在重新分配内存之前，给被kill的进程1s的时间完成exit相关处理，通常情况
         * 下，1s应该够了。
         */
        if (killed)
            schedule_timeout_killable(1);
    }
</code></pre>

<p>out_of_memory->select_bad_process</p>

<p>通过select_bad_process函数选择被kill的进程，其基本流程为：</p>

<p>1、遍历系统中的所有进程，进行"点数"计算</p>

<p>2、进行一些特殊情况的处理，比如: 优先选择触发OOM的进程、不处理正在exit的进程等。</p>

<p>3、计算"点数"，选择点数最大的进程。通过函数oom_badness()</p>

<p>代码注释和分析如下：</p>

<pre><code>    /*
      * OOM流程中，用来选择被kill的进程的函数
      * @ppoints:点数，用来计算每个进程被"选中"可能性，点数越高，越可能被"选中"
      */
    static struct task_struct *select_bad_process(unsigned int *ppoints,
            unsigned long totalpages, const nodemask_t *nodemask,
            bool force_kill)
    {
        struct task_struct *g, *p;
        struct task_struct *chosen = NULL;
        unsigned long chosen_points = 0;

        rcu_read_lock();
        // 遍历系统中的所有进程，进行"点数"计算
        do_each_thread(g, p) {
            unsigned int points;

            /*
             * 进行一些特殊情况的处理，比如: 优先选择触发OOM的进程、不处理
             * 正在exit的进程等。
             */        
            switch (oom_scan_process_thread(p, totalpages, nodemask,
                            force_kill)) {
            case OOM_SCAN_SELECT:
                chosen = p;
                chosen_points = ULONG_MAX;
                /* fall through */
            case OOM_SCAN_CONTINUE:
                continue;
            case OOM_SCAN_ABORT:
                rcu_read_unlock();
                return ERR_PTR(-1UL);
            case OOM_SCAN_OK:
                break;
            };
            // 计算"点数"，选择点数最大的进程。
            points = oom_badness(p, NULL, nodemask, totalpages);
            if (points &gt; chosen_points) {
                chosen = p;
                chosen_points = points;
            }
        } while_each_thread(g, p);
        if (chosen)
            get_task_struct(chosen);
        rcu_read_unlock();

        *ppoints = chosen_points * 1000 / totalpages;
        return chosen;
    }
</code></pre>

<p>out_of_memory->select_bad_process->oom_scan_process_thread</p>

<p>oom_scan_process_thread函数的分析和注释如下：</p>

<pre><code>    enum oom_scan_t oom_scan_process_thread(struct task_struct *task,
            unsigned long totalpages, const nodemask_t *nodemask,
            bool force_kill)
    {
        // 如果进程正在exit
        if (task-&gt;exit_state)
            return OOM_SCAN_CONTINUE;
        /*
         * 如果进程不能被kill，比如: init进程或进程在nodemask对应的节点上，
         * 没有可以释放的内存。
         */
        if (oom_unkillable_task(task, NULL, nodemask))
            return OOM_SCAN_CONTINUE;

        /*
         * This task already has access to memory reserves and is being killed.
         * Don't allow any other task to have access to the reserves.
         */
        /*
         * 如果有进程正在被OOM流程kill，那么应该有内存可以释放了，就不需要再kill
         * 其它进程了，此时返回abort，结束oom kill流程。
         */
        if (test_tsk_thread_flag(task, TIF_MEMDIE)) {
            if (unlikely(frozen(task)))
                __thaw_task(task);
            if (!force_kill)
                return OOM_SCAN_ABORT;
        }
        // 如果不存在mm了(可能进程刚退出了)
        if (!task-&gt;mm)
            return OOM_SCAN_CONTINUE;

        /*
         * If task is allocating a lot of memory and has been marked to be
         * killed first if it triggers an oom, then select it.
         */
        // 优先选择触发OOM的进程。
        if (oom_task_origin(task))
            return OOM_SCAN_SELECT;

        if (task-&gt;flags &amp; PF_EXITING &amp;&amp; !force_kill) {
            /*
             * If this task is not being ptraced on exit, then wait for it
             * to finish before killing some other task unnecessarily.
             */
            if (!(task-&gt;group_leader-&gt;ptrace &amp; PT_TRACE_EXIT))
                return OOM_SCAN_ABORT;
        }
        return OOM_SCAN_OK;
    }
</code></pre>

<p>out_of_memory->select_bad_process->oom_badness</p>

<p>oom_badness用于计算进程的“点数”，点数最高者被选中，代码注释和分析如下：</p>

<pre><code>    /*
     * 计算进程"点数"(代表进程被选中的可能性)的函数，点数根据进程占用的物理内存来计算
     * 物理内存占用越多，被选中的可能性越大。root processes有3%的bonus。
     */
    unsigned long oom_badness(struct task_struct *p, struct mem_cgroup *memcg,
                 const nodemask_t *nodemask, unsigned long totalpages)
    {
        long points;
        long adj;

        if (oom_unkillable_task(p, memcg, nodemask))
            return 0;
        // 确认进程是否还存在
        p = find_lock_task_mm(p);
        if (!p)
            return 0;

        adj = (long)p-&gt;signal-&gt;oom_score_adj;
        if (adj == OOM_SCORE_ADJ_MIN) {
            task_unlock(p);
            return 0;
        }

        /*
         * The baseline for the badness score is the proportion of RAM that each
         * task's rss, pagetable and swap space use.
         */
        // 点数=rss(驻留内存/占用物理内存)+pte数+交换分区用量
        points = get_mm_rss(p-&gt;mm) + p-&gt;mm-&gt;nr_ptes +
             get_mm_counter(p-&gt;mm, MM_SWAPENTS);
        task_unlock(p);

        /*
         * Root processes get 3% bonus, just like the __vm_enough_memory()
         * implementation used by LSMs.
         */
        /*
         * root用户启动的进程，有总 内存*3% 的bonus，就是说可以使用比其它进程多3%的内存
         * 3%=30/1000
         */
        if (has_capability_noaudit(p, CAP_SYS_ADMIN))
            adj -= 30;

        /* Normalize to oom_score_adj units */
        // 归一化"点数"单位
        adj *= totalpages / 1000;
        points += adj;

        /*
         * Never return 0 for an eligible task regardless of the root bonus and
         * oom_score_adj (oom_score_adj can't be OOM_SCORE_ADJ_MIN here).
         */
        return points &gt; 0 ? points : 1;
    }
</code></pre>

<p>out_of_memory->oom_kill_process</p>

<p>oom_kill_process()函数用于：kill被选中的进程，其实就是给指定进程发送SIGKILL信号，待被选中进程返回用户态时，进行信号处理。</p>

<p>相关代码注释和分析如下：</p>

<pre><code>    /*
      * kill被选中的进程，在OOM流程中被调用
      */
    void oom_kill_process(struct task_struct *p, gfp_t gfp_mask, int order,
             unsigned int points, unsigned long totalpages,
             struct mem_cgroup *memcg, nodemask_t *nodemask,
             const char *message)
    {
        struct task_struct *victim = p;
        struct task_struct *child;
        struct task_struct *t = p;
        struct mm_struct *mm;
        unsigned int victim_points = 0;
        static DEFINE_RATELIMIT_STATE(oom_rs, DEFAULT_RATELIMIT_INTERVAL,
                         DEFAULT_RATELIMIT_BURST);

        /*
         * If the task is already exiting, don't alarm the sysadmin or kill
         * its children or threads, just set TIF_MEMDIE so it can die quickly
         */
        /*
         * 如果进程正在exiting，就没有必要再kill它了，直接设置TIF_MEMDIE，然后返回。
        */
        if (p-&gt;flags &amp; PF_EXITING) {
            set_tsk_thread_flag(p, TIF_MEMDIE);
            put_task_struct(p);
            return;
        }

        if (__ratelimit(&amp;oom_rs))
            dump_header(p, gfp_mask, order, memcg, nodemask);

        task_lock(p);
        pr_err("%s: Kill process %d (%s) score %d or sacrifice child\n",
            message, task_pid_nr(p), p-&gt;comm, points);
        task_unlock(p);

        /*
         * If any of p's children has a different mm and is eligible for kill,
         * the one with the highest oom_badness() score is sacrificed for its
         * parent. This attempts to lose the minimal amount of work done while
         * still freeing memory.
         */
        /*
         * 如果被选中的进程的子进程，不跟其共享mm(通常是这样)，且膐om_badness的
         * 得分更高，那么重新选择该子进程为被kill的进程。
         */
        read_lock(&amp;tasklist_lock);
        do {
            // 遍历被选中进程的所有子进程
            list_for_each_entry(child, &amp;t-&gt;children, sibling) {
                unsigned int child_points;

                // 如果不共享mm
                if (child-&gt;mm == p-&gt;mm)
                    continue;
                /*
                 * oom_badness() returns 0 if the thread is unkillable
                 */
                // 计算child?om_badness得分
                child_points = oom_badness(child, memcg, nodemask,
                                    totalpages);
                // 如果child得分更高，则将被选中进程换成child
                if (child_points &gt; victim_points) {
                    put_task_struct(victim);
                    victim = child;
                    victim_points = child_points;
                    get_task_struct(victim);
                }
            }
        } while_each_thread(p, t);
        read_unlock(&amp;tasklist_lock);

        rcu_read_lock();
        /*
         * 遍历确认被选中进程的线程组，判断是否还存在task_struct-&gt;mm，如果不存在
         * (有可能这个时候进程退出了，或释放了mm),就没必要再kill了。
         * 如果存在则选择线程组中的进程。
         */
        p = find_lock_task_mm(victim);
        if (!p) {
            rcu_read_unlock();
            put_task_struct(victim);
            return;
        // 如果新选择的进程跟之前的不是同一个，那么更新victim。
        } else if (victim != p) {
            get_task_struct(p);
            put_task_struct(victim);
            victim = p;
        }

        /* mm cannot safely be dereferenced after task_unlock(victim) */
        mm = victim-&gt;mm;
        pr_err("Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB\n",
            task_pid_nr(victim), victim-&gt;comm, K(victim-&gt;mm-&gt;total_vm),
            K(get_mm_counter(victim-&gt;mm, MM_ANONPAGES)),
            K(get_mm_counter(victim-&gt;mm, MM_FILEPAGES)));
        task_unlock(victim);

        /*
         * Kill all user processes sharing victim-&gt;mm in other thread groups, if
         * any. They don't get access to memory reserves, though, to avoid
         * depletion of all memory. This prevents mm-&gt;mmap_sem livelock when an
         * oom killed thread cannot exit because it requires the semaphore and
         * its contended by another thread trying to allocate memory itself.
         * That thread will now get access to memory reserves since it has a
         * pending fatal signal.
         */
        /*
         * 遍历系统中的所有进程，寻找在其它线程组中，跟被选中进程(victim)共享mm结构
         * 的进程(内核线程除外)，共享mm结构即共享进程地址空间，比如fork后exec之前，
         * 父子进程是共享mm的，回收内存必须要将共享mm的所有进程都kill掉。
         */
        for_each_process(p)
            if (p-&gt;mm == mm &amp;&amp; !same_thread_group(p, victim) &amp;&amp;
             !(p-&gt;flags &amp; PF_KTHREAD)) {
                if (p-&gt;signal-&gt;oom_score_adj == OOM_SCORE_ADJ_MIN)
                    continue;

                // 进行task_struct相关操作时，通常需要获取该锁。
                task_lock(p);    /* Protect -&gt;comm from prctl() */
                pr_err("Kill process %d (%s) sharing same memory\n",
                    task_pid_nr(p), p-&gt;comm);
                task_unlock(p);
                // 通过向被选中的进程发送kill信号，来kill进程。
                do_send_sig_info(SIGKILL, SEND_SIG_FORCED, p, true);
            }
        rcu_read_unlock();

        // 进程设置TIF_MEMDIE标记，表示进程正在被oom killer终止中。
        set_tsk_thread_flag(victim, TIF_MEMDIE);
        /*
         * 最终通过向被选中的进程发送kill信号，来kill进程，被kill的进程在从内核态
         * 返回用户态时，进行信号处理。
         * 被选中的进程可以是自己(current)，则current进程会在oom流程执行完成后，返回
         * 用户态时，处理信号。
         */
        do_send_sig_info(SIGKILL, SEND_SIG_FORCED, victim, true);
        put_task_struct(victim);
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kernel 3.10内核源码分析--内核页表创建]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/30/kernel-mm-init/"/>
    <updated>2015-09-30T15:53:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/30/kernel-mm-init</id>
    <content type="html"><![CDATA[<p><a href="http://blog.chinaunix.net/uid-20671208-id-4440253.html">http://blog.chinaunix.net/uid-20671208-id-4440253.html</a></p>

<p>内核页表创建基本流程：
<code>
    start_kernel
        setup_arch
            init_mem_mapping
                init_range_memory_mapping
                    init_memory_mapping
                        kernel_physical_mapping_init  
</code></p>

<pre><code>    /*
      * 创建内核页表，将内核页表中能线性映射的部分(0-896M，还要刨去ISA等区域)
      * 进行映射，创建相应的页表项，在内核初始化的时候(setup_arch())完成。
      */
    unsigned long __init
    kernel_physical_mapping_init(unsigned long start,
                 unsigned long end,
                 unsigned long page_size_mask)
    {
        int use_pse = page_size_mask == (1&lt;&lt;PG_LEVEL_2M);
        unsigned long last_map_addr = end;
        unsigned long start_pfn, end_pfn;
         /*内核页表页目录所在的位置，其所占的内存是在head_32.S中预先分配好的*/
        pgd_t *pgd_base = swapper_pg_dir;
        int pgd_idx, pmd_idx, pte_ofs;
        unsigned long pfn;
        pgd_t *pgd;
        pmd_t *pmd;
        pte_t *pte;
        unsigned pages_2m, pages_4k;
        int mapping_iter;
        /*计算欲映射区域的起始和结束pfn*/
        start_pfn = start &gt;&gt; PAGE_SHIFT;
        end_pfn = end &gt;&gt; PAGE_SHIFT;

        /*
         * First iteration will setup identity mapping using large/small pages
         * based on use_pse, with other attributes same as set by
         * the early code in head_32.S
         *
         * Second iteration will setup the appropriate attributes (NX, GLOBAL..)
         * as desired for the kernel identity mapping.
         *
         * This two pass mechanism conforms to the TLB app note which says:
         *
         * "Software should not write to a paging-structure entry in a way
         * that would change, for any linear address, both the page size
         * and either the page frame or attributes."
         */
        mapping_iter = 1;

        if (!cpu_has_pse)
            use_pse = 0;

    repeat:
        pages_2m = pages_4k = 0;
        pfn = start_pfn;
        pgd_idx = pgd_index((pfn&lt;&lt;PAGE_SHIFT) + PAGE_OFFSET);
        /*
         * pgd、pmd等存放的是本级页表中对应index项的虚拟地址，页表项的内容中存放的是
         * 下一级页表的起始物理地址
         */
        pgd = pgd_base + pgd_idx;
        for (; pgd_idx &lt; PTRS_PER_PGD; pgd++, pgd_idx++) {
            //创建pmd，如果没有pmd，则返回pgd。实际通过get_free_page接口分配，此时buddy系统已经可用?
            pmd = one_md_table_init(pgd);

            if (pfn &gt;= end_pfn)
                continue;
    #ifdef CONFIG_X86_PAE
            pmd_idx = pmd_index((pfn&lt;&lt;PAGE_SHIFT) + PAGE_OFFSET);
            pmd += pmd_idx;
    #else
            pmd_idx = 0;
    #endif
            for (; pmd_idx &lt; PTRS_PER_PMD &amp;&amp; pfn &lt; end_pfn;
             pmd++, pmd_idx++) {
                /*
                 * 页框虚拟地址，就是物理地址(pfn * PAGE_SIZE)+固定偏移
                 * 这就是线性映射的实质。
                */
                unsigned int addr = pfn * PAGE_SIZE + PAGE_OFFSET;

                /*
                 * Map with big pages if possible, otherwise
                 * create normal page tables:
                 */
                if (use_pse) {
                    unsigned int addr2;
                    pgprot_t prot = PAGE_KERNEL_LARGE;
                    /*
                     * first pass will use the same initial
                     * identity mapping attribute + _PAGE_PSE.
                     */
                    pgprot_t init_prot =
                        __pgprot(PTE_IDENT_ATTR |
                            _PAGE_PSE);

                    pfn &amp;= PMD_MASK &gt;&gt; PAGE_SHIFT;
                    addr2 = (pfn + PTRS_PER_PTE-1) * PAGE_SIZE +
                        PAGE_OFFSET + PAGE_SIZE-1;

                    if (is_kernel_text(addr) ||
                     is_kernel_text(addr2))
                        prot = PAGE_KERNEL_LARGE_EXEC;

                    pages_2m++;
                    if (mapping_iter == 1)
                        set_pmd(pmd, pfn_pmd(pfn, init_prot));
                    else
                        set_pmd(pmd, pfn_pmd(pfn, prot));

                    pfn += PTRS_PER_PTE;
                    continue;
                }
                // 创建页表
                pte = one_page_table_init(pmd);

                pte_ofs = pte_index((pfn&lt;&lt;PAGE_SHIFT) + PAGE_OFFSET);
                pte += pte_ofs;
                // 填写每项页表的内容。
                for (; pte_ofs &lt; PTRS_PER_PTE &amp;&amp; pfn &lt; end_pfn;
                 pte++, pfn++, pte_ofs++, addr += PAGE_SIZE) {
                    pgprot_t prot = PAGE_KERNEL;
                    /*
                     * first pass will use the same initial
                     * identity mapping attribute.
                     */
                    pgprot_t init_prot = __pgprot(PTE_IDENT_ATTR);

                    if (is_kernel_text(addr))
                        prot = PAGE_KERNEL_EXEC;

                    pages_4k++;
                    if (mapping_iter == 1) {
                        // 将pfn(页框号)和相关属性转换为物理地址，然后写入pte中
                        set_pte(pte, pfn_pte(pfn, init_prot));
                        last_map_addr = (pfn &lt;&lt; PAGE_SHIFT) + PAGE_SIZE;
                    } else
                        set_pte(pte, pfn_pte(pfn, prot));
                }
            }
        }
        if (mapping_iter == 1) {
            /*
             * update direct mapping page count only in the first
             * iteration.
             */
            update_page_count(PG_LEVEL_2M, pages_2m);
            update_page_count(PG_LEVEL_4K, pages_4k);

            /*
             * local global flush tlb, which will flush the previous
             * mappings present in both small and large page TLB's.
             */
            __flush_tlb_all();

            /*
             * Second iteration will set the actual desired PTE attributes.
             */
            mapping_iter = 2;
            goto repeat;
        }
        return last_map_addr;
</code></pre>

<p>swapper_pg_dir为内核页表页目录所在的位置，其所占的内存是在head_32.S中预先分配好的，从下面的汇编代码看，预先分配了1024*4=4k的空间，可以容纳1024个entry。</p>

<pre><code>    ENTRY(swapper_pg_dir)
        .fill 1024,4,0
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux中Buffer cache]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/23/kernel-mm-buffer/"/>
    <updated>2015-09-23T17:15:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/23/kernel-mm-buffer</id>
    <content type="html"><![CDATA[<p><a href="http://www.linuxidc.com/Linux/2013-01/78140.htm">http://www.linuxidc.com/Linux/2013-01/78140.htm</a></p>

<h4>buffer cache和page cache的区别</h4>

<p>Page cache和buffer cache到底有什么区别呢？很多时候我们不知道系统在做IO操作的时候到底是走了page cache还是buffer cache？其实，buffer cache和page  cache是Linux中两个比较简单的概念，在此对其总结说明。</p>

<p>Page cache是vfs文件系统层的cache，例如 对于一个ext3文件系统而言，每个文件都会有一棵radix树管理文件的缓存页，这些被管理的缓存页被称之为page cache。所以，page cache是针对文件系统而言的。例如，ext3文件系统的页缓存就是page cache。Buffer cache是针对设备的，每个设备都会有一棵radix树管理数据缓存块，这些缓存块被称之为buffer cache。通常对于ext3文件系统而言，page cache的大小为4KB，所以ext3每次操作的数据块大小都是4KB的整数倍。Buffer cache的缓存块大小通常由块设备的大小来决定，取值范围在512B~4KB之间，取块设备大小的最大公约数。</p>

<hr />

<p><a href="http://alanwu.blog.51cto.com/3652632/1112079">http://alanwu.blog.51cto.com/3652632/1112079</a></p>

<h3>Linux中Buffer cache性能问题一探究竟</h3>

<h4>1, Buffer cache的作用</h4>

<p>为了提高磁盘设备的IO性能，我们采用内存作为磁盘设备的cache。用户操作磁盘设备的时候，首先将数据写入内存，然后再将内存中的脏数据定时刷新到磁盘。这个用作磁盘数据缓存的内存就是所谓的buffer cache。在以前的Linux系统中，有很完善的buffer cache软件层，专门负责磁盘数据的缓存。在磁盘设备的上层往往会架构文件系统，为了提高文件系统的性能，VFS层同样会提供文件系统级别的page cache。这样就导致系统中存在两个cache，并且重叠在一起，显得没有必要和冗余。为了解决这个问题，在现有的Linux系统中对buffer cache软件层进行了弱化，并且和page cache进行了整合。Buffer cache和page cache都采用radix tree进行维护，只有当访问裸设备的时候才会使用buffer cache，正常走文件系统的IO不会使用buffer cache。</p>

<p>我们知道ext3文件系统的page cache都是以page页大小为单位的，那么buffer cache中缓存块大小究竟是多大呢？其对性能影响如何呢？这两天我在Linux-2.6.23平台上针对这个问题做了很多实验，得到了一些数据结果，并从源代码分析中得到设置缓存块大小的方法。在此对这个buffer cache的性能问题进行分析说明，供大家讨论。</p>

<h4>2, Buffer cache的性能问题</h4>

<h5>2.1 测试实验</h5>

<p>首先让我们来做一个实验，在Linux-2.6.23平台上，采用dd工具对一个块设备进行顺序写操作，可以采用如下的命令格式：</p>

<pre><code>    dd if=/dev/zero of=/dev/sda2 bs=&lt;request_size&gt; count=100
</code></pre>

<p>采用该命令在不同buffer cache块（blk_size）大小配置的情况下测试不同请求大小（req_size）的IO性能，可以得到如下表所示的测试数据：</p>

<p>表：不同buffer cache块大小配置下的吞吐量</p>

<p><img src="/images/kernel/2015-09-23-1.jpg" alt="" /></p>

<p>将表中的数据做成性能对比图，如下图所示：</p>

<p><img src="/images/kernel/2015-09-23-2.jpg" alt="" /></p>

<p>从图中可以看出，在请求大小小于Cache块大小的时候，Cache块越大，IO性能越高；但是，请求大小大于Cache块大小之后，性能都有明显的飞跃。</p>

<p>例如，当buffer cache块大小被配置成2KB时，小于2KB的块性能基本都在19MB/s左右；当buffer cache块大小被配置成512B时，小于512B的写性能都保持在5MB/s；当buffer cache块大小被配置成1024B时，小于1KB的写性能基本都保持在9.5MB/s上下。这就说明对于小于cache块大小的small_write，buffer cache越大，其性能会越好，反之，性能越差，这就是buffer cache的作用。</p>

<p>观察发现一旦请求大小大于等于cache块大小之后，性能急剧提升，由于测试工具的IO压力足够大，能够一下子将磁盘性能耗尽。这是为什么呢？其实，当请求块比较小时，对于cache块而言是“局部操作”，这种“局部操作”会引入buffer cache的数据读操作，并且数据读操作和用户写操作存在顺序关系，这就极大的影响了IO的写性能。因此，当请求大小大于cache块时，并且能够和Cache块对齐时，就能够充分利用磁盘的IO带宽，所以就产生了上图中所示的性能飞跃。</p>

<p>看到上图中的测试结果之后，我们就会想在实际应用中，我们该如何选择buffer cache的块大小？如果请求大小是512B时，显然将buffer cache块设置成512比较合适；如果请求大小是256B时，显然将buffer cache块设置成2KB比较合适。所以，个人认为块大小的设置还需要根据实际的应用来决定，不同的应用需要设置不同的块大小，这样才能使整体性能达到最佳。</p>

<h5>2.2 Buffer cache块大小</h5>

<p>Linux系统在创建块设备的时候是如何设置块大小的呢？这里面涉及到Linux针对块大小设置的一个小小算法。在此结合源码对Linux的这个方法加以说明。</p>

<p>总体来说，Linux决定buffer cache块大小采用的是“最大块大小”的设计思想。Linux根据块设备容量决定buffer cache的块大小，并且将值域限定在512B和4KB之间。当然，这个值域内的元素不是连续的，并且都是2的幂。在这个值域的基础上取块设备大小的最大公约数，这个值就是buffer cache的块大小。这种算法的指导思想就是buffer cache的块越大越好，因此，能够取2KB就不会选择512B。Linux中算法实现代码如下所示：</p>

<pre><code>    void bd_set_size(struct block_device *bdev, loff_t size)
    {
        unsigned bsize = bdev_logical_block_size(bdev);

        bdev-&gt;bd_inode-&gt;i_size = size;      //size为块设备大小
        while (bsize &lt; PAGE_CACHE_SIZE) {   //bsize不能大于Page size
            if (size &amp; bsize)
                break;
            bsize &lt;&lt;= 1;    //bsize只能取2的幂
        }
        bdev-&gt;bd_block_size = bsize;
        /* 设置buffer cache块大小 */
        bdev-&gt;bd_inode-&gt;i_blkbits = blksize_bits(bsize);
    }
</code></pre>

<h4>3, 小结</h4>

<p>本文对buffer cache的性能问题进行了分析，通过实验发现当请求块比较小时，buffer cache块大小对IO性能有很大的影响。Linux根据块设备的容量采用“最大cache块”的思想决定buffer cache的块大小。在实际应用中，我们应该根据应用特征，通过实际测试来决定buffer cache块大小。</p>

<hr />

<p>通常Linux的“block size”指的是1024 bytes，Linux用1024-byte blocks 作为buffer cache的基本单位。但linux的文件系统的block确不一样。例如ext3系统，block size是4096。使用tune2fs可以查看带文件系统的磁盘分区的相关信息，包括block size。</p>

<p>例如：
<code>
    tune2fs -l /dev/sda2 |grep "Block size"
    Block size:               4096
</code></p>

<p>另一个工具dumpe2fs也可以。 dumpe2fs /dev/sda2 | grep &ldquo;Block size&rdquo;</p>

<p>其实本来这几个概念不是很难，主要是NND他们的名字都一样，都叫“Block Size”。</p>

<p>1.硬件上的 block size, 应该是"sector size"，linux的扇区大小是512byte</p>

<p>2.有文件系统的分区的block size, 是"block size"，大小不一，可以用工具查看</p>

<p>3.没有文件系统的分区的block size，也叫“block size”，大小指的是1024 byte</p>

<p>4.Kernel buffer cache 的block size, 就是"block size"，大部分PC是1024</p>

<p>5.磁盘分区的"cylinder size"，用fdisk -l可以查看。</p>

<p>我们来看看fdisk显示的不同的信息，理解一下这几个概念：
<code>
    Disk /dev/hda: 250.0 GB, 250059350016 bytes
    255 heads, 63 sectors/track, 30401 cylinders
    Units = cylinders of 16065 * 512 = 8225280 bytes
       Device Boot    Start       End    Blocks   Id  System
    /dev/hda1   *         1      1305  10482381   83  Linux
    /dev/hda2          1306      1566   2096482+  82  Linux swap
    /dev/hda3          1567     30401 231617137+  83  Linux
</code></p>

<p>8225280就是cylinder size。一共有30401个cylinder。Start和End分别标记的是各个分区的起始cylinder。第4列显示的就是以1024为单位的block（这一列最容易把人搞晕）。为什么“2096482+”有个“+”号呢？因为啊，总size除1024除不尽，是个约数。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux内核页回收 swappiness参数]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/18/kernel-mm-swappiness/"/>
    <updated>2015-09-18T11:21:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/18/kernel-mm-swappiness</id>
    <content type="html"><![CDATA[<p><a href="http://www.douban.com/note/349467816/">http://www.douban.com/note/349467816/</a></p>

<p>本文主要尝试解释两个问题：<br/>
1. swappiness的确切含义是什么，它对内核进行页回收机制的影响。<br/>
2. swappiness设置成0，为什么系统仍然可能会有swap发生。</p>

<h4>一. 关于内存分配与页回收(page reclaim)</h4>

<p>page reclaim发生的场景主要有两类，一个是kswapd后台线程进行的活动，另一个是direct reclaim，即分配页时没有空闲内存满足，需要立即直接进行的页回收。大体上内存分配的流程会分为两部分，一部分是fast path，另一部分是slow path，通常内存使用非紧张情况下，都会在fast path就可以满足要求。并且fast path下的内存分配不会出现dirty writeback及swap等页回收引起的IO阻塞情况。</p>

<p>fast path大体流程如下：</p>

<p>1.如果系统挂载使用了memory cgroup，则首先检查是否超过cgroup限额，如果超过则进行direct reclaim，通过do_try_to_free_pages完成。如果没超过则进行cgroup的charge工作（charge是通过两阶段提交完成的，这里不展开了）。</p>

<p>2.从本地prefered zone内存节点查找空闲页，需要判断是否满足系统watermark及dirty ratio的要求，如果满足则从buddy system上摘取相应page,否则尝试对本地prefered zone进行页回收,本次fast path下页回收只会回收clean page，即不会考虑dirty page以及mapped page，这样就不会产生任何swap及writeback，即不会引起任何blocking的IO操作，如果这次回收仍然无法满足请求的内存页数目则进入slow path</p>

<p>slow path大体流程如下：</p>

<ol>
<li><p>首先唤醒kswapd进行page reclaim后台操作。</p></li>
<li><p>重新尝试本地prefered zone进行分配内存，如果失败会根据请求的GFP相关参数决定是否尝试忽略watermark, dirty ratio以及本地节点分配等要求进行再次重试，这一步中如果分配页时有指定<code>__GFP_NOFAIL</code>标记，则分配失败会一直等待重试。</p></li>
<li><p>如果没有<code>__GFP_NOFAIL</code>标记，则会需开始进行page compact及page direct reclaim操作，之后如果仍然没有可用内存，则进入OOM流程。</p></li>
</ol>


<p>相关内容可以参阅内核代码<code>__alloc_pages</code>函数的逻辑，另外无论page reclaim是由谁发起的，最终都会统一入口到shrink_zone，即针对每个zone独立进行reclaim操作，最终会进入shrink_lruvec函数，进行每个zone相应page lru链表的扫描与回收操作。</p>

<h4>二. 关于页回收的一些背景知识</h4>

<p>页回收大体流程会先在每个zone上扫描相应的page链表，主要包括inactive anon/active anon(匿名页链表)以及inactive file/active file链表（file cache/映射页链表），一共四条链表，我们所有使用过的page在被回收前基本是保存在这四条链表中的某一条中的（还有一部分在unevictable链表中，忽略），根据其被引用的次数会决定其处于active还是inactive链表中，根据其类型决定处于anon还是file链表中。</p>

<p>页回收总体会扫描逐个内存节点的所有zone，然后先扫描active，将不频繁访问的页挪到inactive链表中，随后扫描inactive链表，会将其中被频繁引用的页重新挪回到active中，确认不频繁的页则最终被回收，如果是file based的页则根据是否clean进行释放或回写(writeback，filecache则直接释放)，如果是anon则进行swap，所以本文实际关心的是swappiness参数对anon链表扫描的影响。</p>

<p>另外还需要了解前面描述的四个链表原来是放在zone数据结构上的，后来引入了mem_cgroup则，重新定义了一组mem_cgroup_per_zone/mem_cgroup_per_node的数据结构，这四个链表同时定义在这组数据结构上，如果系统开启了mem cgroup则使用后者，否则用前者。</p>

<p>另外再重点说下swap只是page reclaim的一种处理措施，主要针对anon page，我们最终来看下swappiness的确切含义</p>

<h4>三. swappiness对page reclaim的确切影响</h4>

<p>page reclaim逻辑中对前面所述四个链表进行扫描的逻辑在vmscan.c中的get_scan_count函数内，该函数大部分逻辑注释写得非常清楚，我们简单梳理下，主要关注scan_balance变量的取值：</p>

<ol>
<li><p>首先如果系统禁用了swap或者没有swap空间，则只扫描file based的链表，即不进行匿名页链表扫描
代码：
<code>
 if (!sc-&gt;may_swap || (get_nr_swap_pages() &lt;= 0)) {
     scan_balance = SCAN_FILE;
     goto out;
 }
</code></p></li>
<li><p>如果当前进行的不是全局页回收（cgroup资源限额引起的页回收），并且swappiness设为0，则不进行匿名页链表扫描，这个是没得商量，这里swappiness值直接决定了是否有swap发生，设成0则肯定不会发生，另外需要注意，这种情况下需要设置的是cgroup配置文件memory.swappiness，而不是全局的sysctl vm.swappiness
代码：
<code>
 if (!global_reclaim(sc) &amp;&amp; !vmscan_swappiness(sc)) {
     scan_balance = SCAN_FILE;
     goto out;
 }
</code></p></li>
<li><p>如果进行链表扫描前设置的priority(这个值决定扫描多少分之一的链表元素)为0，且swappiness非0，则可能会进行swap
代码：
<code>
 if (!sc-&gt;priority &amp;&amp; vmscan_swappiness(sc)) {
     scan_balance = SCAN_EQUAL;
     goto out;
 }
</code></p></li>
<li><p>如果是全局页回收，并且当前空闲内存和所有file based链表page数目的加和都小于系统的high watermark，则必须进行匿名页回收，则必然会发生swap,可以看到这里swappiness的值如何设置是完全无关的，这也解释了为什么其为0，系统也会进行swap的原因，另外最后我们会详细解释系统page watermark是如何计算的。
代码：
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> anon = get_lru_size(lruvec, LRU_ACTIVE_ANON) +
</span><span class='line'>         get_lru_size(lruvec, LRU_INACTIVE_ANON);
</span><span class='line'> file = get_lru_size(lruvec, LRU_ACTIVE_FILE) +
</span><span class='line'>         get_lru_size(lruvec, LRU_INACTIVE_FILE);&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt; if (global_reclaim(sc)) {
</span><span class='line'>     free = zone_page_state(zone, NR_FREE_PAGES);
</span><span class='line'>     if (unlikely(file + free &lt;= high_wmark_pages(zone))) {
</span><span class='line'>         scan_balance = SCAN_ANON;
</span><span class='line'>         goto out;
</span><span class='line'>     }
</span><span class='line'> }</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>如果系统inactive file链表比较充足，则不考虑进行匿名页的回收，即不进行swap
代码：
<code>
 if (!inactive_file_is_low(lruvec)) {
     scan_balance = SCAN_FILE;
     goto out;
 }
</code></p></li>
<li><p>最后一种情况则要根据swappiness值与之前统计的file与anon哪个更有价值来综合决定file和anon链表扫描的比例，这时如果swappiness设置成0，则也不会扫描anon链表，即不进行swap,代码比较多，不再贴出。</p></li>
</ol>


<h4>四. 系统内存watermark的计算</h4>

<p>前面看到系统内存watermark对页回收机制是有决定影响的，其实在内存分配中也会频繁用到这个值，确切的说它有三个值，分别是low,min和high,根据分配页时来指定用哪个，如果系统空闲内存低于相应watermark则分配会失败，这也是进入slow path或者wakeup kswapd的依据。</p>

<p>实际这个值的计算是通过sysctl里的vm.min_free_kbytes来决定的，大体的计算公式如下：</p>

<pre><code>    pages_min = min_free_kbytes &gt;&gt; (PAGE_SHIFT - 10);
    tmp = (u64)pages_min * zone-&gt;managed_pages;
    do_div(tmp, lowmem_pages);
    zone-&gt;watermark[WMARK_MIN] = tmp;
    zone-&gt;watermark[WMARK_LOW] = min_wmark_pages(zone) + (tmp &gt;&gt; 2);
    zone-&gt;watermark[WMARK_HIGH] = min_wmark_pages(zone) + (tmp &gt;&gt; 1);
</code></pre>

<p>即根据min_free_kbytes的值按照每个zone管理页面的比例算出zone的min_watermark，然后再加min的1/4就是low，加1/2就是high了</p>

<h4>总结：</h4>

<p>swappiness的值是个参考值，是否会发生swap跟当前是哪种page reclaim及系统当前状态都有关系，所以设置了swappiness=0并不代表一定没有swap发生，同时设为0也确实会可能发生OOM。</p>

<p>个人仍然认为线上环境设置swappiness=0是没有任何问题的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux swap实现]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/09/18/kernel-mm-swap/"/>
    <updated>2015-09-18T11:16:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/09/18/kernel-mm-swap</id>
    <content type="html"><![CDATA[<p><a href="http://blog.csdn.net/freas_1990/article/details/9090601">http://blog.csdn.net/freas_1990/article/details/9090601</a></p>

<p>swap是现代Unix操作系统一个非常重要的特性。尤其在大型数据库服务器上，swap往往是性能首要查看指标。</p>

<p>通俗的说法，在Unix里，将开辟一个磁盘分区，用作swap，这块磁盘将作为内存的的替代品，在内存不够用的时候，把一部分内存空间交换到磁盘上去。</p>

<p>而Unix的swap功能也成为了Unixer们认为Unix由于windows的一个论据（？）。在Unix里，swap一般被认为设置为内存的2倍大小。这个2倍大小的指标出自哪里，到目前为止我也没有找到（？如果你找到了可以留言或发私信）。</p>

<p>不过，在内存不断掉价的今天，swap的功效已经越来越弱化了——在2013年6月13日23:01，如果一个OLTP系统的swap使用超过了2G以上，基本上可以对这个系统的性能产生怀疑了。swap并不是一种优化机制，而是一种不得已而为之的手段，防止在内存紧张的时刻，操作系统性能骤降以至瞬间崩溃。swap的价值主要体现在可以把这个崩溃的时间提升至几小时到几十个小时不等。</p>

<p>本文主要关注CPU访问一个内存page时，发现该page不在内存中的情况。废话不多说了，先把swap的核心函数调用栈贴一下。</p>

<p><img src="/images/kernel/2015-09-18-11.png" alt="" /></p>

<p>当CPU检查一个页目录项/页表项的Present标志位时，如果发现该标志位为0，则表示相应的物理页面不在内存。此时，CPU会被激发“页面异常”（中断中的fault），而去执行一段代码。</p>

<p>至于到底是这个内存页面需要重新构建、还是页面的内容是存储到磁盘上去了，CPU本身是不关心的，CPU只知道中断条件发生了，要根据中断描述符跳转到另外一段代码去执行，而真正的swap或者是真的缺页的智能判断是在这段中断服务程序里做的——真正的技术是在这段中断服务程序里。（所以我在《中断——一鞭一条痕（下）》里说，作为一个初学者，不必深究中断（interrupt）、异常（exception）、陷阱（trap）这三个概念）</p>

<p>pte_present()函数会检查当前页面的描述entry的present标志位，查看该page是否在内存中。如果不在内存中，调用pte_none()判断是否建立了页目录、页表映射。如果连映射都没建立，说明是“真没在内存中”，需要从头建立映射关系。如果建立了映射关系，说明此时，该页面被暂时存储到磁盘上去了，应该到磁盘上去把该page取回来放到内存里。</p>

<p>如何去取呢？</p>

<p>如何到磁盘取一个page的数据到内存中去，这是一个多么熟悉的概念！思考一下Oracle的内存管理，一个block如何读入到SGA的buffer cache里去吧。其实这几十年来，核心的本源技术无论是在操作系统内核还是在数据库内核里，都是通用的，都是用来极大限度提升CPU任务管理能力、内存管理效率的，所有的理念、技术都是通用的——如果你站在一个系统程序猿的角度来思考，一定能明白的——不要把自己局限在一个产品里，无论这个产品是数据库、CPU、还是操作系统，这些看似绚烂神秘的技术在30年以前，已经被人反复的讨论和意淫过了。</p>

<p>接下来就到了核心部分了——do_swap_page()函数。</p>

<p>源代码如下（linux/mm/memory.c line 2022~1060）：
```
    static int do_swap_page(struct mm_struct * mm,
        struct vm_area_struct * vma, unsigned long address,
        pte_t * page_table, swp_entry_t entry, int write_access)
    { <br/>
        struct page *page = lookup_swap_cache(entry);
        pte_t pte;</p>

<pre><code>    if (!page) {
        lock_kernel();
        swapin_readahead(entry);
        page = read_swap_cache(entry);
        unlock_kernel();
        if (!page)
            return -1;

        flush_page_to_ram(page);
        flush_icache_page(vma, page);
    }

    mm-&gt;rss++;

    pte = mk_pte(page, vma-&gt;vm_page_prot);

    /*
     * Freeze the "shared"ness of the page, ie page_count + swap_count.
     * Must lock page before transferring our swap count to already
     * obtained page count.
     */
    lock_page(page);
    swap_free(entry);
    if (write_access &amp;&amp; !is_page_shared(page))
        pte = pte_mkwrite(pte_mkdirty(pte));
    UnlockPage(page);

    set_pte(page_table, pte);
    /* No need to invalidate - it was non-present before */
    update_mmu_cache(vma, address, pte);
    return 1;   /* Minor fault */
}
</code></pre>

<pre><code>
这里有2个参数需要重点关注，一个是`(pte_t *)page_table`，另外一个是`(swp_entry_t*)entry`。

当一个page在内存中，不需要swap in时，描述该page的entry是pte_t类型的；反之，是swp_entry_t类型。

swap_entry_t(include/linux/shmem_fs.h)定义如下：
</code></pre>

<pre><code>typedef struct {
    unsigned long val;
} swp_entry_t;
</code></pre>

<p>```</p>

<p>问题出来了，既然都进入do_swap_page()函数了，说明是需要swap in了，为什么还会传入一个pte_t类型的变量呢？</p>

<p>答案是，当在do_swap_page()之前，page是在磁盘上的，描述类型是swp_entry_t，而do_swap_page()之后，页面已经从磁盘交换到内存了，这个时候描述类型就是pte_t了。</p>

<p>至于lookup_swap_cache、swapin_readahead（预读——read ahead）等函数就不一一分析了，从名字就可以看出其技巧了。都是些在数据库server上的常用技巧。如果你是行家，一眼就能看出来。</p>
]]></content>
  </entry>
  
</feed>
